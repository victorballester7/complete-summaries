\documentclass[../../../main_math.tex]{subfiles}

\begin{document}
\changecolor{SC}
\begin{multicols}{2}[\section{Stochastic calculus}]
  Along the document we assume that we work in a probability space $(\Omega,\mathcal{F},\Prob)$ and that all the random variables are defined on this space.
  \subsection{Preliminaries}
  \subsubsection{Stochastic processes}
  \begin{proposition}
    A stochastic process $X={(X_t)}_{t\in \TT}$ is Gaussian if and only if $\forall n\in\NN$, $\forall t_1,\ldots,t_n\in\TT$, $\forall \lambda_1,\ldots,\lambda_n\in\RR$,
    $$
      Z:=\lambda_1 X_{t_1}+\cdots+\lambda_n X_{t_n}
    $$
    is a Gaussian random variable. In particular, we have:
    $$
      \Exp(\exp{\ii Z})=\exp{\ii \Exp(Z)-\frac{1}{2}\Var(Z)}
    $$
  \end{proposition}
  \begin{remark}
    A stochastic process $X={(X_t)}_{t\in \TT}$ can also be viewed as a single random variable taking values in $\RR^{\TT}$, equipped with the product $\sigma$-algebra $\displaystyle \bigotimes_{t\in\TT}\mathcal{B}(\RR)$.
  \end{remark}
  \begin{proposition}
    Let $m:\TT\to\RR$ be a measurable function and $\gamma:\TT^2\to\RR$ be a symmetric positive-definite function. Then, there exists a Gaussian process ${(X_t)}_{t\in \TT}$ such that $\Exp(X_t)=m(t)$ and $\cov(X_s,X_t)=\gamma(s,t)$.
  \end{proposition}
  \begin{definition}
    Let ${(X_t)}_{t\in \TT}$, ${(Y_s)}_{s\in \SS}$ be two stochastic processes. We say that they are \emph{jointly Gaussian} if the concatenated process $({(X_t)}_{t\in \TT},{(Y_s)}_{s\in \SS})$ is Gaussian.
  \end{definition}
  \begin{lemma}\label{SC:indep_joint_gauss}
    Two jointly Gaussian stochastic processes ${(X_t)}_{t\in \TT}$, ${(Y_s)}_{s\in \SS}$ are independent if and only if $\forall t\in\TT$, $\forall s\in\SS$, $\cov(X_t,Y_s)=0$.
  \end{lemma}
  \begin{proposition}
    Two stochastic processes ${(X_t)}_{t\in \TT}$, ${(Y_s)}_{s\in \SS}$ are independent if and only if $\forall n\in\NN$, $\forall t_1,\ldots,t_n\in\TT$, $\forall s_1,\ldots,s_n\in\SS$ and $\forall f,g:\RR^n\to\RR$ bounded and measurable functions, we have:
    \begin{multline*}
      \Exp(f(X_{t_1},\ldots,X_{t_n})g(Y_{s_1},\ldots,Y_{s_n}))=\\=
      \Exp(f(X_{t_1},\ldots,X_{t_n}))\Exp(g(Y_{s_1},\ldots,Y_{s_n}))
    \end{multline*}
  \end{proposition}
  \subsubsection{Brownian motion}
  \begin{definition}
    A \emph{Brownian motion} is a stochastic process ${(B_t)}_{t\geq 0}$ such that:
    \begin{enumerate}
      \item $B$ is Gaussian with $\Exp(B_t)=0$ and $\cov(B_s,B_t)=s\wedge t$.
      \item $B$ has continuous paths.
    \end{enumerate}
  \end{definition}
  \begin{proposition}
    Let $B$ be a Brownian motion. Then:
    \begin{enumerate}
      \item $B_0=0$ a.s.
      \item $B$ has independent increments.
      \item $B$ has stationary increments.
    \end{enumerate}
    Conversely, any stochastic process with these properties has the law of a Brownian motion.
  \end{proposition}
  \begin{theorem}[Strong law of large numbers for Brownian motion]
    Let ${(B_t)}_{t\geq 0}$ be a Brownian motion. Then:
    $$
      \frac{B_t}{t}\underset{t\to\infty}{\overset{\text{a.s.}}{\longrightarrow}}0
    $$
  \end{theorem}
  \begin{proof}
    We already now that the process $s\to sB_{1/s} \indi{s>0}$ is a Brownian motion. In particular, we must have continuity at $0=B_0$.
  \end{proof}
  \begin{theorem}[Markov property for Brownian motion]
    Let $B={(B_t)}_{t\geq 0}$ be a Brownian motion and $a\geq 0$ fixed. Then, the Brownian motion ${(B_{t+a}-B_a)}_{t\geq 0}$ is independent of ${(B_s)}_{s\in [0,a]}$.
  \end{theorem}
  \begin{proof}
    The processes ${(B_s)}_{s\in[0,a]}$ and ${(B_{t+a}-B_a)}_{t\geq 0}$ are jointly Gaussian, because their coordinates are linear combinations of coordinates of the same Gaussian process $B$. Thus, by \mcref{SC:indep_joint_gauss} it reduces to compute the following correlation:
    $$
      \cov(B_s,B_{t+a}-B_a)=s\wedge(t+a)-s\wedge a=0
    $$
  \end{proof}
  \begin{remark}
    Recall that $s\wedge t:=\min(s,t)$ and $s\vee t:=\max(s,t)$.
  \end{remark}
  \subsubsection{Martingales}
  \begin{definition}
    Let ${(X_t)}_{t\geq 0}$ be a stochastic process. We define the \emph{natural filtration} of $X$ as $\mathcal{F}^X:={(\mathcal{F}_t^X)}_{t\geq 0}$, where $\mathcal{F}_t^X:=\sigma(X_s:s\leq t)$.
  \end{definition}
  From now on, we will assume that we work in a filtered probability space $(\Omega,\mathcal{F},\Prob,{(\mathcal{F}_t)}_{t\geq 0})$.
  \begin{definition}[Martingale]
    A stochastic process ${(X_t)}_{t\geq 0}$ is a \emph{martingale} if:
    \begin{enumerate}
      \item it is \emph{adapted}, i.e.\ $X_t$ is $\mathcal{F}_t$-measurable for all $t\geq 0$.
      \item $\Exp(\abs{X_t})<\infty$ for all $t\geq 0$.
      \item $\Exp(X_t\mid \mathcal{F}_s)=X_s$ for all $0\leq s\leq t$.
    \end{enumerate}
    The process is called a \emph{sub-martingale} if the last condition is replaced by $\Exp(X_t\mid \mathcal{F}_s)\geq X_s$ for all $0\leq s\leq t$ and a \emph{super-martingale} if $\Exp(X_t\mid \mathcal{F}_s)\leq X_s$ for all $0\leq s\leq t$.
  \end{definition}
  \begin{proposition}
    Let $B={(B_t)}_{t\geq 0}$ be a Brownian motion. Then, the following processes are martingales ${(M_t)}_{t\geq 0}$ with respect to the natural filtration induced by $B$:
    \begin{itemize}
      \item $M_t=B_t$
      \item $M_t=B_t^2-t$
      \item $M_t=\exp{\theta B_t-\frac{1}{2}\theta^2t}$, for any fixed $\theta\in\RR$.
    \end{itemize}
  \end{proposition}
  \begin{proposition}
    Let $A\subseteq \RR$ be a closed set and $X={(X_t)}_{t\geq 0}$ be an adapted continuous process. Then, the \emph{hitting time} of $A$ by $X$, defined as:
    $$
      T_A:=\inf\{t\geq 0:X_t\in A\}
    $$
    is a stopping time.
  \end{proposition}
  \begin{proof}
    Using the continuity of $X$ and the fact that $A$ is closed, one can easily check that:
    $$
      \{T_A \leq t\}=\bigcap_{k\in\NN}\bigcup_{s\in[0,t]\cap\QQ}\left\{d(X_s,A)\leq\frac{1}{k}\right\}
    $$
    Now, $\left\{d(X_s,A)\leq\frac{1}{k}\right\}\in \mathcal{F}_s\subseteq \mathcal{F}_t$ because $X$ is adapted and $z\mapsto d(z,A)$ is measurable.
    Thus, $\{T_A \leq t\}\in \mathcal{F}_t$ because it is a countable union and intersection of events in $\mathcal{F}_t$.
  \end{proof}
  \begin{theorem}[Doob's optional sampling theorem]\label{SC:doob_sampling}
    Let ${(M_t)}_{t\geq 0}$ be a continuous martingale and $T$ be a stopping time. Then, the \emph{stopped process} $M^T:={(M_{t\wedge T})}_{t\geq 0}$ is a continuous martingale. In particular, $\forall t\geq 0$, $\Exp(M_{t\wedge T})=\Exp(M_0)$. If $M^T$ is uniformly integrable and $T\overset{\text{a.s.}}{\leq}\infty$, then taking $t\to\infty$ we have $\Exp(M_T)=\Exp(M_0)$.
  \end{theorem}
  \begin{lemma}[Orthogonality of martingales]\label{SC:orthogonality_martingales}
    Let ${(M_t)}_{t\geq 0}$ be a continuous martingale and let $0\leq s\leq t$. Then:
    $$
      \Exp({(M_t-M_s)}^2\mid \mathcal{F}_s)=\Exp({M_t}^2-{M_s}^2\mid \mathcal{F}_s)
    $$
  \end{lemma}
  \begin{proof}
    We have that:
    \begin{multline*}
      \Exp({(M_t-M_s)}^2\mid \mathcal{F}_s) =\Exp({M_t}^2-2M_tM_s+{M_s}^2\mid \mathcal{F}_s) =\\=\Exp({M_t}^2+{M_s}^2\!\mid \!\mathcal{F}_s)-2M_s\Exp(M_t\!\mid\! \mathcal{F}_s) =\Exp({M_t}^2-{M_s}^2\!\mid\! \mathcal{F}_s)
    \end{multline*}
  \end{proof}
  \begin{theorem}[Doob's maximal inequality]\label{SC:doob_maximal}
    If $M$ is a continuous square-integrable martingale, then $\forall a,t\geq 0$ we have:
    $$
      \Prob\left(\sup_{0\leq s\leq t}\abs{M_s}\geq a\right)\leq \frac{\Exp({M_t}^2)}{a^2}
    $$
  \end{theorem}
  \begin{proposition}\label{SC:limit_of_martingales}
    Let $(M^n)$ be a sequence of continuous square-integrable martingales and suppose that for each $t\geq 0$, the limit $\displaystyle M_t:=\overset{L^2}{=}\lim_{n\to\infty}M_t^n$ exists. Then, $M={(M_t)}_{t\geq 0}$ is a continuous square-integrable martingale.
  \end{proposition}
  \begin{proof}
    By \mnameref{SC:doob_maximal} applied to $M^n-M^m$ we have that for fixed $t\geq 0$ and $k\in\NN$:
    $$
      \Prob\left(\sup_{0\leq s\leq t}\abs{M_s^n-M_s^m}\geq \frac{1}{k^2}\right)\leq k^2\Exp({(M_t^n-M_t^m)}^2)\leq \frac{1}{k^2}
    $$
    where in the last inequality we have used that ${(M^n)}$ converges in $L^2$ and so we have chosen $n,m$ large enough so that the inequality holds. Thus, there is an increasing sequence $(n_k)$ such that:
    $$
      \Prob\left(\sup_{0\leq s\leq t}\abs{M_s^{n_{k+1}}-M_s^{n_k}}\geq \frac{1}{k}\right)\leq \frac{1}{k^2}
    $$
    By \mnameref{P:borel-cantelli1}, we deduce that
    $$
      \sum_{k=1}^{\infty}\sup_{0\leq s\leq t}\abs{M_s^{n_{k+1}}-M_s^{n_k}}<\infty
    $$
    which ensures that $(M^{n_k})$ is continuous in the space of continuous functions equipped with the topology of uniform convergence on every compact set. But the limit is necessarily a version of $M$, because for each $t\geq 0$ we have $M_t^n\to M_t$ in $L^2$.
  \end{proof}
  \subsubsection{Quadratic variation}
  \begin{definition}
    Let $f:\RR_{\geq 0}\to\RR$ be a function. We define the \emph{absoulte variation} of $f$ on the interval $[s,t]$ as:
    $$
      V(f,s,t):=\sup_{{(t_k)}_{0\leq k\leq n}\in \mathrm{P}([s,t])}\sum_{k=1}^{n}\abs{f(t_{k+1})-f(t_k)}
    $$
    where $\mathrm{P}([s,t])$ is the set of all partitions of $[s,t]$. A function has \emph{finite variation} if $V(f,s,t)<\infty$ for all $0\leq s\leq t$.
  \end{definition}
  \begin{lemma}\label{SC:properties_variation}
    Let $f,g:\RR_{\geq 0}\to\RR$ be a function and $0\leq s\leq t$. Then:
    \begin{itemize}
      \item $V(f,s,t)=V(f,s,u)+V(f,u,t)$, for all $s\leq u\leq t$.
      \item If $f\in C^1$, then $V(f,s,t)=\int_s^t\abs{f'(u)}\dd{u}$.
      \item If $f$ is monotone, then $V(f,s,t)=\abs{f(t)-f(s)}$.
      \item $\displaystyle V(f+g,s,t)\leq V(f,s,t)+V(g,s,t)$.
      \item Finite variation functions form a vector space.
    \end{itemize}
  \end{lemma}
  \begin{proposition}\label{SC:difference_of_increasing}
    Let $f:\RR_{\geq 0}\to\RR$. Then, $f$ has finite variation if and only if it can be written as the difference of two non-decreasing functions.
  \end{proposition}
  \begin{sproof}
    \mcref{SC:properties_variation} gives us the implication to the left. For the other one, note that the functions $f_1(t):=V(f,0,t)$ and $f_2(t):=V(f,0,t)-f(t)$ are non-decreasing.
  \end{sproof}
  \begin{theorem}[Quadratic variation]
    Let $M={(M_t)}_{t\geq 0}$ be a continuous square-integrable martingale. Then, for each $t\geq 0$ the limit
    $$
      {\langle M\rangle}_t:=\lim_{n\to\infty}\sum_{k=1}^{n}\abs{M_{t_k^n}-M_{t_{k-1}^n}}^2
    $$
    exists in $L^1$ and does not depend on the partition ${(t_k^n)}_{0\leq k\leq n}\in \mathrm{P}([0,t])$ chosen as long as the \emph{mesh} $\Delta_n:= \max_{1\leq k\leq n}(t_k^n-t_{k-1}^n)$ goes to $0$ as $n\to\infty$. Moreover, $\langle M\rangle=({\langle M\rangle}_t)_{t\geq 0}$ has the following properties:
    \begin{enumerate}
      \item\label{SC:quad_var1} ${\langle M\rangle}_0=0$
      \item\label{SC:quad_var2} $\langle M\rangle$ is non-decreasing.
      \item\label{SC:quad_var3} The function $t\mapsto {\langle M\rangle}_t$ is continuous.
      \item\label{SC:quad_var4} ${({M_t}^2-{\langle M\rangle}_t)}_{t\geq 0}$ is a martingale.
    \end{enumerate}
  \end{theorem}
  \begin{proof}
    We omit the proof of the existence and continuity. We will only prove the last property. Let $0\leq s\leq t$ and ${(t_k^n)}_{0\leq k\leq n}\in \mathrm{P}([s,t])$ be such that $\Delta_n\to 0$. Then:
    \begin{align*}
      \Exp({M_t}^2-{M_s}^2\mid \mathcal{F}_s) & =\sum_{k=1}^n \Exp({M_{t_k^n}}^2-{M_{t_{k-1}^n}}^2\mid \mathcal{F}_s)          \\
                                              & =\sum_{k=1}^n \Exp\left({(M_{t_k}^n-M_{t_{k-1}}^n)}^2\mid \mathcal{F}_s\right) \\
    \end{align*}
    by the \mnameref{SC:orthogonality_martingales}. Now since we have convergence of $\sum_{k=1}^{n}{(M_{t_k}^n-M_{t_{k-1}}^n)}^2$ to ${\langle M\rangle}_t-{\langle M\rangle}_s$ in $L^1$, we get the result:
    $$
      \Exp({M_t}^2-{M_s}^2\mid \mathcal{F}_s)=\Exp({\langle M\rangle}_t-{\langle M\rangle}_s\mid \mathcal{F}_s)
    $$
  \end{proof}
  \begin{proposition}
    Let $B$ be a Brownian motion. Then:
    $$
      \Prob(\forall s,t\geq 0, V(B,s,t)=\infty)=1
    $$
    But, ${\langle B\rangle}_t=t$ for all $t\geq 0$.
  \end{proposition}
  \begin{proof}
    Let $B={(B_t)}_{t\geq 0}$. Then:
    $$
      V(B,s,t)\!\geq\! \sum_{k=1}^{n}\abs{B_{s+k\frac{t-s}{n}}-B_{s+(k-1)\frac{t-s}{n}}}\!=\!\sqrt{\frac{t-s}{n}}\sum_{k=1}^{n}\abs{\xi_k}
    $$
    where $\xi_k$ are \iid $N(0,1)$. By the \mnameref{P:weaklaw} we get the result. The second part is similar, but we get convergence instead.
  \end{proof}
  \begin{proposition}\label{SC:prop_variation_fg}
    If a function $f$ has finite variation and $g$ is continuous, then:
    $$
      \sum_{k=1}^n(f(t_k)-f(t_{k-1}))(g(t_k)-g(t_{k-1}))\overset{n\to\infty}{\longrightarrow}0
    $$
  \end{proposition}
  \begin{proof}
    Note that:
    \begin{multline*}
      \abs{\sum_{k=1}^n(f(t_k)-f(t_{k-1}))(g(t_k)-g(t_{k-1}))}\leq\\\leq V(f,0,t)\max_{\substack{0\leq u\leq v\leq t\\\abs{u-v}\leq \Delta_n}}\abs{g(u)-g(v)}
    \end{multline*}
    which goes to zero by uniform continuity of $g$ at $[0,t]$.
  \end{proof}
  \begin{corollary}\label{SC:corollary_finite_variation}
    Let $M={(M_t)}_{t\geq 0}$ be a continuous square-integrable martingale with finite variation a.s. Fix $t\geq 0$. By \mcref{SC:prop_variation_fg} we have that ${\langle M\rangle}_t=0$
    Then:
    $$
      \Prob(\forall t\geq 0,\ M_t=M_0)=1
    $$
  \end{corollary}
  \begin{proof}
    By \mnameref{SC:orthogonality_martingales}, we have:
    $$
      \Exp({(M_t-M_0)}^2)=\Exp({M_t}^2) - \Exp({M_0}^2) = \Exp({\langle M\rangle}_t)=0
    $$
    where the penultimate equality follows from the fact that ${M_t}^2-{\langle M\rangle}_t$ is a martingale and so it has constant expectation. This shows that $\Prob (\forall t\geq 0,\ M_t=M_0)=1$. Now we can use the fact that $M$ is continuous to conclude using $t\in\QQ$.
  \end{proof}
  \begin{proposition}
    The quadratic variation is the unique process that satisfies \mcref{SC:quad_var1,SC:quad_var2,SC:quad_var3,SC:quad_var4}.
  \end{proposition}
  \begin{proof}
    Let $A$ be another process satisfying such properties. Then, ${M}^2-{\langle M\rangle}$ and ${M}^2-A$ are both martingales. Thus, $A-{\langle M\rangle}$ is also a martingale. But it is also continuous and has finite variation (by \mcref{SC:difference_of_increasing}). So by \mcref{SC:corollary_finite_variation}, $A={\langle M\rangle}$.
  \end{proof}
  \subsubsection{Local martingales}
  \begin{definition}
    A stochastic process ${(M_t)}_{t\geq 0}$ is a \emph{continuous local martingale} if there exists a sequence of stopping times ${(T_n)}_{n\in\NN}$ (called \emph{localizing sequence}) such that:
    \begin{enumerate}
      \item $T_n\nearrow \infty$ a.s.
      \item $M^{T_n}:={(M_{t\wedge T_n})}_{t\geq 0}$ is a martingale for all $n\in\NN$.
    \end{enumerate}
  \end{definition}
  \begin{remark}
    If $M$ is a martingale, then $M$ is a local martingale by taking $T_n=+\infty$ for all $n\in\NN$.
  \end{remark}
  \begin{remark}
    Any local martingale is adapted because it is the pointwise limit of $M^{T_n}$, which are adapted by definition.
  \end{remark}
  \begin{proposition}
    Let $M={(M_t)}_{t\geq 0}$ be a continuous local martingale. Then, if $\forall t\geq 0$ we have
    $$
      \Exp\left(\sup_{0\leq s\leq t}\abs{M_s}\right)<\infty
    $$
    then $M$ is a martingale.
  \end{proposition}
  \begin{proof}
    We've argued that local martingales are automatically adapted. Moreover:
    $$
      \Exp(\abs{M_t})\leq \Exp\left(\sup_{0\leq s\leq t}\abs{M_s}\right)<\infty
    $$
    Finally, fix $0\leq s\leq t$. For all $n\in\NN$ we have:
    $$
      \Exp(M_{t\wedge T_n}\mid \mathcal{F}_s)=M_{s\wedge T_n}
    $$
    And using the \mnameref{P:dominated} with $M_{t\wedge T_n}\leq \sup_{0\leq s\leq t}\abs{M_s}$ we conclude the result.
  \end{proof}
  \begin{remark}
    Note that if $M$ is a continuous local martingale with $M_0=0$, then we can always take $T_n=\inf\{t\geq 0:\abs{M_t}\geq n\}$ as a localizing sequence.
  \end{remark}
  \begin{theorem}[Doob's optional sampling theorem for local martingales]
    Let $M={(M_t)}_{t\geq 0}$ be a continuous local martingale and $T$ be a stopping time. Then, the stopped process $M^T:={(M_{t\wedge T})}_{t\geq 0}$ is a continuous local martingale.
  \end{theorem}
  \begin{proof}
    Let ${(T_n)}_{n\in\NN}$ be a localizing sequence for $M$. Since $M^{T_n}$ is a continuous martingale, by \mnameref{SC:doob_sampling} we have that $M^{T_n\wedge T}$ is a continuous martingale. Thus, $M^T$ is a local martingale with localizing sequence ${(T_n)}_{n\in\NN}$.
  \end{proof}
  \begin{proposition}
    Continuous local martingales form a vector space.
  \end{proposition}
  \begin{proof}
    Let $M$, $\tilde{M}$ be continuous local martingales with localizing sequences ${(T_n)}_{n\in\NN}$ and ${(\tilde{T}_n)}_{n\in\NN}$ respectively and $\lambda,\tilde{\lambda}\in\RR$. Then, ${(T_n\wedge \tilde{T}_n)}_{n\in\NN}$ is a localizing sequence for both $M$ and $\tilde{M}$ and so $\lambda M^{T_n\wedge \tilde{T}_n}+\tilde{\lambda}\tilde{M}^{T_n\wedge \tilde{T}_n}$ is a martingale.
  \end{proof}
  \begin{proposition}
    If $M$ is a continuous local martingale which has finite variation a.s., then:
    $$
      \Prob(\forall t\geq 0,\ M_t=M_0)=1
    $$
  \end{proposition}
  \begin{proof}
    Let ${(T_n)}_{n\in\NN}$ be a localizing sequence for $M$. Then, $M^{T_n}$ is a martingale and $V(M^{T_n},0,t)=V(M,0,t\wedge T_n)<\infty$. Thus, by \mcref{SC:corollary_finite_variation} we have that $M^{T_n}_t=M^{T_n}_0$ $\forall t\geq 0$ and $n\in\NN$. Taking $n\to\infty$ we get the result.
  \end{proof}
  \begin{proposition}
    Let $M$ be a continuous local martingale. Then, the limit
    $$
      {\langle M\rangle}_t:=\lim_{n\to\infty}\sum_{k=1}^{n}\abs{M_{t_k^n}-M_{t_{k-1}^n}}^2
    $$
    exists in probability for any $t\geq 0$ and does not depend on the partition ${(t_k^n)}_{0\leq k\leq n}\in \mathrm{P}([0,t])$ chosen as long as $\Delta_n\to 0$. Moreover, $\langle M\rangle=({\langle M\rangle}_t)_{t\geq 0}$ is the unique process (up to modification) such that:
    \begin{enumerate}
      \item ${\langle M\rangle}_0=0$
      \item $t\mapsto {\langle M\rangle}_t$ is a.s.\ continuous.
      \item $\langle M\rangle$ is a.s.\ non-decreasing.
      \item ${({M_t}^2-{\langle M\rangle}_t)}_{t\geq 0}$ is a continuous local martingale.
    \end{enumerate}
  \end{proposition}
  \begin{theorem}[Levy's characterization of Brownian motion]
    Let $M={(M_t)}_{t\geq 0}$ be a stochastic process. Then, the following are equivalent:
    \begin{enumerate}
      \item $M$ is a continuous local square-integrable martingale with $M_0=0$ and ${\langle M\rangle}_t=t$.
      \item $M$ is a ${(\mathcal{F}_t)}_{t\geq 0}$-Brownian motion.
    \end{enumerate}
  \end{theorem}
  \subsection{Stochastic integration}
  \subsubsection{Wiener isometry}
  \begin{definition}
    Let $H$, $H'$ be Hilbert. A map $I:H\to H'$ is called \emph{isometry} if it is linear and $\forall x\in H$ we have:
    $$
      \norm{I(x)}_{H'}=\norm{x}_H
    $$
    We speak of \emph{partial isometry} when $I$ is only defined on a subspace of $H$.
  \end{definition}
  \begin{theorem}
    Let $H$, $H'$ be Hilbert, $V\subseteq H$ be a dense subspace and $I:V\to H'$ be a partial isometry. Then, there exists a unique continuous isometry extension of $I$ to $H$.
  \end{theorem}
  \begin{proof}
    Let $x\in H\setminus V$. Then, $\exists(x_n)\in V$ such that $x_n\to x$. Clearly, any continuous extension must satisfy $I(x):=\lim_{n\to\infty}I(x_n)$, so we take it as a definition. Note that, first, the limit exists because $(I(x_n))$ is Cauchy and moreover this definition does not depend on the sequence $(x_n)$. From this definition, the extension is automatically linear and norm-preserving (because of the continuity).
  \end{proof}
  \begin{definition}
    Let ${(B_t)}_{t\geq 0}$ be a Brownian motion and $f\in\mathcal{S}(\RR_{\geq 0})$ be a simple function such that $f=\sum_{k=1}^na_k\indi{(t_{k-1},t_k]}$ with $0=t_0\leq t_1\leq \cdots\leq t_n$. We define the \emph{Wiener integral} of $f$ as:
    $$
      I(f):=\sum_{k=1}^na_k(B_{t_k}-B_{t_{k-1}})
    $$
  \end{definition}
  \begin{remark}
    Recall that simple functions are dense in $L^p$ (\mcref{RFA:continuousdenseLp}).
  \end{remark}
  \begin{theorem}
    Let ${(B_t)}_{t\geq 0}$ be a Brownian motion on $(\Omega, \mathcal{F}, \Prob)$. Then, there exists a unique linear and continuous map $I:L^2(\RR_{\geq 0})\to L^2((\Omega, \mathcal{F}, \Prob))$ such that for all $0\leq s\leq t$:
    $$
      I(\indi{(s,t]})=B_t-B_s
    $$
    Moreover, $I$ is an isometry. The map $I$ is called \emph{Wiener isometry} (or \emph{Wiener integral}) and denoted by $I(f)=\int_0^\infty f(u)\dd{B_u}$.
  \end{theorem}
  \begin{remark}
    Recall that the limit of Gaussian variables is Gaussian.
  \end{remark}
  \begin{proposition}
    Let ${(B_t)}_{t\geq 0}$ be a Brownian motion. Then, the following are satisfied:
    \begin{itemize}
      \item For any $f\in L^2(\RR_{\geq 0})$ we have:
            $$
              \int_0^\infty f(u)\dd{B_u}\overset{L^2}{=}\lim_{n\to\infty}\sum_{k=1}^{n^2}a_{n,k}(f)(B_{\frac{k+1}{n}}-B_{\frac{k}{n}})
            $$
            where $a_{n,k}(f):=n\int_{\frac{k}{n}}^{\frac{k+1}{n}}f(u)\dd{u}$ is an approximation of $f$ in the interval $[\frac{k}{n},\frac{k+1}{n}]$.
      \item The Wiener integral is a Gaussian variable with zero mean and variance $\int_0^\infty{f(u)}^2\dd{u}$.
      \item For any $f,g\in L^2(\RR_{\geq 0})$ we have:
            $$
              \cov\left(\int_0^\infty f(u)\dd{B_u},\int_0^\infty g(u)\dd{B_u}\right)=\int_0^\infty f(u)g(u)\dd{u}
            $$
    \end{itemize}
  \end{proposition}
  \subsubsection{The Wiener integral as a process}
  \begin{definition}
    Let $f\in L_{\text{loc}}^2(\RR_{\geq 0})$ and $0\leq s\leq t$. We define the \emph{Wiener integral} of $f$ as:
    $$
      \int_s^t f(u)\dd{B_u}:=\int_0^\infty f(u)\indi{(s,t]}(u)\dd{B_u}
    $$
  \end{definition}
  \begin{lemma}[Chasles relation]
    Let $f\in L_{\text{loc}}^2(\RR_{\geq 0})$ and $0\leq r\leq s\leq t$. Then:
    $$
      \int_r^t f(u)\dd{B_u}=\int_r^s f(u)\dd{B_u}+\int_s^t f(u)\dd{B_u}
    $$
  \end{lemma}
  \begin{proposition}
    Let ${(B_t)}_{t\geq 0}$ be a Brownian motion and $f\in L_{\text{loc}}^2(\RR_{\geq 0})$. Then, the associate process $M^f={(M_t^f)}_{t\geq 0}$ defined as:
    $$
      M_t^f:=\int_0^t f(u)\dd{B_u}
    $$
    is a centered Gaussian process with covariance function:
    $$
      \cov(M_s^f,M_t^f)=\int_0^{s\wedge t}{f(u)}^2\dd{u}
    $$
  \end{proposition}
  \begin{proof}
    We'll only proof that $M^f$ is Gaussian (the computation of the mean and covariance functions is easy). Let $n\in\NN$, $(t_1,\ldots,t_n)\in\RR^n$ and $(\lambda_1,\ldots,\lambda_n)\in\RR^n$. Then:
    $$
      \sum_{k=1}^n\lambda_k M_{t_k}^f=\int_0^\infty g(u)\dd{B_u}
    $$
    with $g(u)=\sum_{k=1}^n\lambda_k f(u)\indi{(0,t_k]}(u)\in L^2(\RR_{\geq 0})$, and the right-hand side is Gaussian because it is a Wiener integral.
  \end{proof}
  \begin{theorem}
    Let $f\in L_{\text{loc}}^2(\RR_{\geq 0})$. Then, $M^f$ is a continuous square-integrable martingale with:
    $$
      {\langle M^f\rangle}_t=\int_0^t{f(u)}^2\dd{u}
    $$
  \end{theorem}
  \begin{proof}
    The integrability and square-integrability is clear because $M^f$ is Gaussian. Note that $t\mapsto M_t^f$ is continuous when $f=\indi{(0,a]}$, because the Brownian motion is continuous. Now using \mcref{SC:limit_of_martingales} we get the result true for any $f\in L_{\text{loc}}^2(\RR_{\geq 0})$. Now let's prove that $M^f$ is a martingale. We have:
    \begin{align*}
      M_t^f & =\lim_{n\to\infty}\sum_{k=1}^{n^2}a_{n,k}(f\indi{(0,t]})(B_{\frac{k+1}{n}}-B_{\frac{k}{n}})                 \\
            & =\lim_{n\to\infty}\sum_{k=1}^{n^2}a_{n,k}(f\indi{(0,t]})(B_{\frac{k+1}{n}\wedge t}-B_{\frac{k}{n}}\wedge t)
    \end{align*}
    and the last expression is $\mathcal{F}_t$-measurable. Finally, if $0\leq s\leq t$ we have that since $M_t^f-M_s^f$ is independent of $\mathcal{F}_s$:
    $$
      \Exp(M_t^f-M_s^f\mid \mathcal{F}_s)=\Exp(M_t^f-M_s^f)=0
    $$
    Moreover, $({(M_t^f)}^2)_{t\geq 0}$ is clearly adapted and:
    \begin{multline*}
      \Exp\left({(M_t^f)}^2-{(M_s^f)}^2\mid \mathcal{F}_s\right)=\Exp\left({(M_t^f-M_s^f)}^2\mid\mathcal{F_s}\right)=\\=\Exp\left({(M_t^f-M_s^f)}^2\right)=\norm{I(f\indi{(s,t]})}_{L^2(\Omega)}^2=\\=\norm{f\indi{(s,t]}}_{L^2(\RR_{\geq 0})}^2=\int_s^t {f(u)}^2\dd{u}
    \end{multline*}
    where the first equality is due to \mnameref{SC:orthogonality_martingales} and the we used the isometry property of $I$. This implies that ${{(M_t^f)}^2}_{t\geq 0}-\int_0^t{f(u)}^2\dd{u}$ is a martingale and by the uniqueness of the quadratic variation we get the result.
  \end{proof}
  \begin{proposition}
    Let ${(B_t)}_{t\geq 0}$ be a Brownian motion. For any $f\in L_{\text{loc}}^2(\RR_{\geq 0})$, the process $Z^f={(Z_t^f)}_{t\geq 0}$ defined as:
    $$
      Z_t^f:=\exp{\int_0^t f(u)\dd{B_u}-\frac{1}{2}\int_0^t{f(u)}^2\dd{u}}
    $$
    is a continuous square-integrable martingale.
  \end{proposition}
  \begin{proof}
    The integrability and adaptedness poses no problem. Now fix $0\leq s\leq t$. We previously saw that $\int_s^t f(u)\dd{B_u}$ is independent of $\mathcal{F}_s$ and so:
    $$
      \Exp\left(Z_t^f\mid \mathcal{F}_s\right)=Z_s^f\Exp\left(\exp{\int_s^t f(u)\dd{B_u}-\frac{1}{2}\int_s^t{f(u)}^2\dd{u}}\right)=Z_s^f
    $$
    because $\int_s^t f(u)\dd{B_u}\sim N(0,\int_s^t{f(u)}^2\dd{u})$.
  \end{proof}
  \subsubsection{Progressive processes}
  \begin{definition}
    Let $(\Omega,\mathcal{F},\Prob,{(\mathcal{F}_t)}_{t\geq 0})$ be a filtered probability space and $\phi={(\phi_t)}_{t\geq 0}$ a stochastic process. We say that $\phi$ is \emph{progressive} if for fixed $t\geq 0$ the function
    $$
      \function{}{([0,t]\times \Omega,\mathcal{B}([0,t])\otimes \mathcal{F}_t)}{(\RR,\mathcal{B}(\RR))}{(u,\omega)}{\phi_u(\omega)}
    $$
    is measurable.
  \end{definition}
  \begin{lemma}
    Let $\phi={(\phi_t)}_{t\geq 0}$ be a stochastic process and
    $$
      \mathcal{P}:=\cap_{t\geq 0}\{ A\subset \RR_{\geq 0}\times\Omega:A\cap([0,t]\times\Omega)\in \mathcal{B}([0,t])\otimes \mathcal{F}_t\}
    $$
    Then, $\phi$ is progressive if and only if the map $(t,\omega)\mapsto\phi_t(\omega)$ is $\mathcal{P}$-measurable.
  \end{lemma}
  \begin{proposition}
    The following stochastic processes ${(\phi_t)}_{t\geq 0}$ are progressive:
    \begin{itemize}
      \item A deterministic process $\phi_t(\omega)=f(t)$, $f:\RR_{\geq 0}\to\RR$.
      \item $\phi_t(\omega)=X(\omega)\indi{(a,b]}(t)$ where $0\leq a<b$ and $X$ be $\mathcal{F}_a$-measurable.
      \item $\phi_t(\omega)=X(\omega)\indi{[0,T(\omega)]}(t)$ where $T$ is a stopping time.
      \item $\phi_t(\omega)=F(\phi_t^1(\omega),\ldots,\phi_t^n(\omega))$ where $F:\RR^n\to\RR$ is measurable and ${(\phi_t^i)}_{1\leq i\leq n}$ are progressive.
      \item A pointwise limit of progressive processes.
      \item A continuous adapted process.
    \end{itemize}
  \end{proposition}
  \subsubsection{Itô isometry}
  \begin{definition}
    We define the set $\MM^2(\RR_{\geq 0})$ as the set of all progressive processes $\phi={(\phi_t)}_{t\geq 0}$ such that:
    $$
      \Exp\left(\int_0^\infty{\phi_u}^2\dd{u}\right)<\infty
    $$
  \end{definition}
  \begin{remark}
    Note that $\MM^2(\RR_{\geq 0})=L^2(\RR_{\geq 0}\times\Omega,\mathcal{P},\dd{t}\otimes\Prob)$ is Hilbert with the scalar product:
    $$
      \langle \phi,\psi\rangle_{\MM^2}:=\Exp\left(\int_0^\infty\phi_u\psi_u\dd{u}\right)
    $$
  \end{remark}
  \begin{theorem}[Itô integral]
    Let ${(B_t)}_{t\geq 0}$ be a Brownian motion. Then, there exists a unique linear and continuous map $I:\MM^2(\RR_{\geq 0})\to L^2((\Omega,\mathcal{F},\Prob))$ such that $I(\phi)=X(B_t-B_s)$ whenever $\phi_u(\omega)=X(\omega)\indi{(s,t]}(u)$ for some $0\leq s\leq t$ and $X\in L^2(\Omega,\mathcal{F}_s,\Prob)$. Moreover, $I$ is an isometry, i.e.:
    $$
      \Exp\left(\int_0^\infty{\phi_u}{\psi_u} \dd{u}\right)=\Exp\left(I(\phi)I(\psi)\right)
    $$
    We call $I$ the \emph{Itô isometry} (or \emph{Itô integral}) and we denote it by $I(\phi)=\int_0^\infty\phi_u\dd{B_u}$.
  \end{theorem}
  \begin{proposition}
    Let ${(\phi_u)}, {(\psi_u)}\in \MM^2(\RR_{\geq 0})$. Then, the following are satisfied:
    \begin{itemize}
      \item $\displaystyle
              \int_0^\infty \phi_u\dd{B_u}\overset{L^2}{=}\lim_{n\to\infty}\sum_{k=1}^{n^2}\left(n\int_{\frac{k}{n}}^{\frac{k+1}{n}}\phi_u\dd{u}\right)(B_{\frac{k+1}{n}}-B_{\frac{k}{n}})
            $
      \item If $\phi_u(\omega)=f(t)$, $f\in L^2(\RR_{\geq 0})$, then we recover the Wiener integral.
      \item $\displaystyle
              \Exp\left(\int_0^\infty \phi_u\dd{B_u}\right)=0$
      \item $\displaystyle\cov\left(\int_0^\infty \phi_u\dd{B_u},\int_0^\infty \psi_u\dd{B_u}\right)=\Exp\left(\int_0^\infty \phi_u\psi_u\dd{u}\right)$
    \end{itemize}
  \end{proposition}
  \subsubsection{The Itô integral as a process}
  \begin{definition}
    Let ${(\phi_u)}$ be a progressive process and $0\leq s\leq t$. We define:
    $$
      \int_s^t \phi_u\dd{B_u}:=\int_0^\infty \phi_u\indi{(s,t]}(u)\dd{B_u}
    $$
    The set of such processes such that $\forall t\geq 0$, $\Exp\left(\int_0^t{\phi_u}^2\dd{u}\right)<\infty$ is denoted by $\MM^2$. The set of such processes such that $\forall t\geq 0$, $\int_0^t{\phi_u}^2\dd{u}<\infty$ is denoted by $\MM^2_{\text{loc}}$.
  \end{definition}
  \begin{remark}
    Note that $\MM^2(\RR_{\geq 0})\subsetneq \MM^2\subsetneq \MM^2_{\text{loc}}$.
  \end{remark}
  \begin{theorem}
    Let $(\phi_u)\in\MM^2$. Then, the associate process $M^\phi={(M_t^\phi)}_{t\geq 0}$ defined as:
    $$
      M_t^\phi:=\int_0^t \phi_u\dd{B_u}
    $$
    is a continuous square-integrable martingale with:
    $$
      {\langle M^\phi\rangle}_t=\int_0^t{\phi_u}^2\dd{u}
    $$
  \end{theorem}
  \begin{remark}
    Note that the by \mnameref{RFA:polarization} we have that:
    $$
      {\langle M^\phi,M^\psi\rangle}_t=\int_0^t \phi_u\psi_u\dd{u}
    $$
  \end{remark}
  \subsubsection{Generalized Itô integral}
  \begin{proposition}
    Let $(\phi_u)\in \MM^2_{\text{loc}}$. Consider the stopping time
    $$
      T_n:=\inf\{t\geq 0:\int_0^t{\phi_u}^2\dd{u}\geq n\}
    $$
    and the truncated progressive process $\phi^n_t(\omega):=\phi_t(\omega)\indi{[0,T_n(\omega)]}(t)$. Then, $\phi^n\in\MM^2(\RR_{\geq 0})$.
  \end{proposition}
  \begin{definition}
    Let $(\phi_u)\in \MM^2_{\text{loc}}$. We define the \emph{generalized Itô integral} of $\phi$ as:
    $$
      \int_0^\infty \phi_u\dd{B_u}:=\lim_{n\to\infty}\int_0^\infty \phi_u \indi{[0,T_n]}(u)\dd{B_u}
    $$
    which is well-defined.
  \end{definition}
  \begin{theorem}
    Let $(\phi_u)\in \MM^2_{\text{loc}}$. Then, the associate process $M^\phi={(M_t^\phi)}_{t\geq 0}$ defined as:
    $$
      M_t^\phi:=\int_0^t \phi_u\dd{B_u}
    $$
    is a continuous local martingale with:
    $$
      {\langle M^\phi\rangle}_t=\int_0^t{\phi_u}^2\dd{u}
    $$
  \end{theorem}
  \begin{theorem}[Stochastic dominated convergence theorem]\label{SC:stochastic_dominated}
    Let $t\geq 0$ and $(\phi_u^n)\in \MM^2_{\text{loc}}$ be a sequence of progressive processes such that $\phi_u^n\overset{\Prob}{\underset{n\to\infty}{\longrightarrow}} \phi_u$ for all a.e.\ $u\in[0,t]$. Suppose that $\forall u\in [0,t]$ and $\forall n\in\NN$ we have $\abs{\phi_u^n}\almoste{\leq} \Psi_u$, with $\Psi\in\MM^2_{\text{loc}}$. Then:
    $$
      \int_0^t \phi_u^n\dd{B_u}\overset{\Prob}{\underset{n\to\infty}{\longrightarrow}} \int_0^t \phi_u\dd{B_u}
    $$
  \end{theorem}
  \begin{corollary}
    If $(\phi_u)$ is a continuous and adapted process, then $\forall t\geq 0$ and any subdivision $(t_k^n)\in \mathrm{P}([0,t])$ such that $\Delta_n\to 0$ we have:
    $$
      \sum_{k=0}^{n-1}\phi_{t_{k+1}^n}(B_{t_{k+1}^n}-B_{t_k^n})\overset{\Prob}{\underset{n\to\infty}{\longrightarrow}} \int_0^t \phi_u\dd{B_u}
    $$
  \end{corollary}
  \subsection{Stochastic differentiation}
  \subsubsection{Itô processes}
  \begin{proposition}
    Let $\psi={(\psi_t)}_{t\geq 0}$ be a stochastic process such that $\forall t\geq 0$ we have
    $$
      \int_0^t \abs{\psi_u}\dd{u}<\infty
    $$
    In this case we say that $\psi\in \MM^1_{\text{loc}}$. Then, the process $$
      t\mapsto \int_0^t \psi_u\dd{B_u}
    $$
    is an adapted continuous process.
  \end{proposition}
  \begin{definition}
    An \emph{Itô process} is a stochastic process ${(X_t)}_{t\geq 0}$ of the form:
    \begin{equation}\label{SC:ito_process}
      X_t=X_0+\int_0^t \phi_u\dd{B_u}+\int_0^t \psi_u\dd{u}
    \end{equation}
    with $\phi\in\MM^2_{\text{loc}}$ and $\psi\in\MM^1_{\text{loc}}$. The two integrals are called \emph{martingale term} and \emph{drift term} respectively. Instead of \mcref{SC:ito_process} we usually write:
    $$
      \dd{X_t}=\phi_t\dd{B_t}+\psi_t\dd{t}
    $$
    This expression is called \emph{stochastic differential}.
  \end{definition}
  \begin{remark}
    Itô processes form a vector space. That is, if $X$ and $Y$ are Itô processes and $\lambda,\mu\in\RR$, then $Z=\lambda X+\mu Y$ is an Itô process and:
    $$
      \dd{Z_t}=\lambda\dd{X_t}+\mu\dd{Y_t}
    $$
    Moreover they are always continuous adapted processes.
  \end{remark}
  \begin{proposition}
    Let $X={(X_t)}_{t\geq 0}$ be an Itô process such that $\forall t\geq 0$ we have:
    $$
      \dd{X_t}=\phi_t\dd{B_t}+\psi_t\dd{t}=\tilde{\phi}_t\dd{B_t}+\tilde{\psi}_t\dd{t}
    $$
    for some $\phi,\tilde{\phi}\in\MM^2_{\text{loc}}$ and $\psi,\tilde{\psi}\in\MM^1_{\text{loc}}$. Then, $\phi$, $\tilde{\phi}$ are indistinguishable and so are $\psi$, $\tilde{\psi}$.
  \end{proposition}
  \begin{proof}
    By assumption, we have that a.e.\ $\forall t\geq 0$:
    $$
      \int_0^t{( \phi_u-\tilde{\phi}_u)}\dd{B_u}=\int_0^t{(\psi_u-\tilde{\psi}_u)}\dd{u}
    $$
    But since the right-hand side of the equation is a local martingale and the left-hand side has finite variation, we have that both sides must be 0 a.e.\ in $t$. Moreover, by the uniqueness of the quadratic variation we have that:
    $$
      \int_0^t{(\phi_u-\tilde{\phi}_u)}^2\dd{u}=0
    $$
    Letting $t\to \infty$ we get that $\phi$, $\tilde{\phi}$ are indistinguishable. Finally, from the Lebesgue integral, we have that $\psi$, $\tilde{\psi}$ are indistinguishable.
  \end{proof}
  \begin{proposition}\label{SC:ito_process_martingale}
    Let $X={(X_t)}_{t\geq 0}$ be an Itô process such that $\dd{X_t}=\phi_t\dd{B_t}+\psi_t\dd{t}$. Then:
    \begin{itemize}
      \item $X$ is a local martingale if and only if $X_0\in L^1$ and $\psi=0$.
      \item $X$ is a square-integrable martingale if and only if $X_0\in L^2$, $\phi\in\MM^2$ and $\psi=0$.
    \end{itemize}
  \end{proposition}
  \begin{definition}
    Let $X={(X_t)}_{t\geq 0}$ be an Itô process such that $\dd{X_t}=\phi_t\dd{B_t}+\psi_t\dd{t}$, and $Y={(Y_t)}_{t\geq 0}$ be a continuous adapted process. Then, $Y\phi\in \MM^2_{\text{loc}}$ and $Y\psi\in \MM^1_{\text{loc}}$ and we define:
    $$
      \int_0^tY_u \dd{X_u}:=\int_0^t Y_u\phi_u\dd{B_u}+\int_0^t Y_u\psi_u\dd{u}
    $$
  \end{definition}
  \begin{remark}
    Note that using \mnameref{RFA:dominated;SC:stochastic_dominated} we also have:
    $$
      \int_0^tY_u \dd{X_u}\overset{\Prob}{=}\lim_{n\to\infty} \sum_{k=0}^{n-1}Y_{t_k^n}(X_{t_{k+1}^n}-X_{t_k^n})
    $$
    along any subdivision ${(t_k^n)}_{0\leq k\leq n}\in \mathrm{P}([0,t])$ such that $\Delta_n\to 0$.
  \end{remark}
  \subsubsection{Quadratic variation of Itô processes}
  \begin{lemma}\label{SC:ito_quadratic_variation}
    Let $X={(X_t)}_{t\geq 0}$, $\tilde{X}=({\tilde{X}_t})_{t\geq 0}$ be two Itô processes with differentials:
    $$
      \dd{X_t}=\phi_t\dd{B_t}+\psi_t\dd{t}\qquad \dd{\tilde{X}_t}=\tilde{\phi}_t\dd{B_t}+\tilde{\psi}_t\dd{t}
    $$
    Then, for any ${(t_k^n)}_{0\leq k\leq n}\in \mathrm{P}([0,t])$ such that $\Delta_n\to 0$ we have:
    \begin{multline*}
      \sum_{k=0}^{n-1}{(X_{t_{k+1}^n}-X_{t_k^n})}{(\tilde{X}_{t_{k+1}^n}-\tilde{X}_{t_k^n})}\overset{\Prob}{\underset{n\to\infty}{\longrightarrow}} \int_0^t{\phi_u}{\tilde{\phi}_u}\dd{u}=:\\=:{\langle X,\tilde{X}\rangle}_t
    \end{multline*}
    In particular:
    $$
      {\langle X\rangle}_t:= {\langle X,X\rangle}_t=\int_0^t{\phi_u}^2\dd{u}
    $$
    and we call it the \emph{quadratic variation} of $X$.
  \end{lemma}
  \begin{proof}
    We saw it for $X=\tilde{X}$, and the general formula follows from \mnameref{RFA:polarization}. Now, if $\psi =0$, $X$ is a continuous local martingale with quadratic variation $t\mapsto \int_0^t{\phi_u}^2\dd{u}$. Now if $\phi=0$, we know it because $t\mapsto \int_0^t \psi_u\dd{u}$ has finite variation, and therefore, null quadratic variation. Finally in the general case we have:
    \begin{multline*}
      \sum_{k=0}^{n-1}{(X_{t_{k+1}^n}-X_{t_k^n})}^2=\sum_{k=0}^{n-1}{\left(\int_{t_k^n}^{t_{k+1}^n}\phi_u\dd{B_u}\right)}^2+\\+\sum_{k=0}^{n-1}{\left(\int_{t_k^n}^{t_{k+1}^n}\psi_u\dd{B_u}\right)}^2+2\sum_{k=0}^{n-1}\int_{t_k^n}^{t_{k+1}^n}\phi_u\dd{B_u}\int_{t_k^n}^{t_{k+1}^n}\psi_u\dd{u}
    \end{multline*}
    The first part tends to $\int_0^t{\phi_u}^2\dd{u}$, the second part tends to 0 and for the last part use \mcref{SC:prop_variation_fg}.
  \end{proof}
  \begin{theorem}[Stochastic integration by parts]
    Let $X={(X_t)}_{t\geq 0}$ and $Y={(Y_t)}_{t\geq 0}$ be two Itô processes. Then, ${(X_tY_t)}_{t\geq 0}$ is an Itô process and:
    $$
      \dd{(X_tY_t)}=X_t\dd{Y_t}+Y_t\dd{X_t}+\dd{{\langle X,Y\rangle}_t}
    $$
    The last term $\dd{{\langle X,Y\rangle}_t}$ is called \emph{Itô term}.
  \end{theorem}
  \begin{proof}
    Let ${(t_k^n)}_{0\leq k\leq n}\in \mathrm{P}([0,t])$ such that $\Delta_n\to 0$. Then:
    \begin{multline*}
      X_tY_t-X_0Y_0=\sum_{k=0}^{n-1}(X_{t_{k+1}^n}Y_{t_{k+1}^n}-X_{t_k^n}Y_{t_k^n})=\\=\sum_{k=0}^{n-1}(X_{t_{k+1}^n}-X_{t_k^n})Y_{t_{k+1}^n}+\sum_{k=0}^{n-1}X_{t_k^n}(Y_{t_{k+1}^n}-Y_{t_k^n})+\\+\sum_{k=0}^{n-1}(X_{t_{k+1}^n}-X_{t_k^n})(Y_{t_{k+1}^n}-Y_{t_k^n})
    \end{multline*}
    Letting $n\to\infty$ and using \mcref{SC:ito_quadratic_variation} and a previous remark we get the result.
  \end{proof}
  \begin{corollary}
    Let $X={(X_t)}_{t\geq 0}$ be an Itô process. Then, ${(X_t^2)}_{t\geq 0}$ is an Itô process and:
    $$
      \dd{X_t^2}=2X_t\dd{X_t}+\dd{{\langle X\rangle}_t}
    $$
  \end{corollary}
  \subsubsection{Itô's formula}
  \begin{theorem}[Itô's formula]\label{SC:ito_formula}
    Let $X={(X_t)}_{t\geq 0}$ be an Itô process and $f\in C^2(\RR)$. Then, ${(f(X_t))}_{t\geq 0}$ is an Itô process and:
    $$
      \dd{f(X_t)}=f'(X_t)\dd{X_t}+\frac{1}{2}f''(X_t)\dd{{\langle X\rangle}_t}
    $$
  \end{theorem}
  \begin{proof}
    Let $t\geq 0$ and ${(t_k^n)}_{0\leq k\leq n}\in \mathrm{P}([0,t])$ such that $\Delta_n\to 0$. Then, using the Taylor expansion of $f$ we have:
    \begin{align*}
      f(X_t)-f(X_0) & =\sum_{k=0}^{n-1}f(X_{t_{k+1}^n})-f(X_{t_k^n})=                                    \\&=\sum_{k=0}^{n-1}f'(X_{t_k^n})(X_{t_{k+1}^n}-X_{t_k^n})+\\
                    & \qquad\quad+\frac{1}{2}\sum_{k=0}^{n-1}f''(X_{u_k^n}){(X_{t_{k+1}^n}-X_{t_k^n})}^2
    \end{align*}
    with $u_k^n\in [t_k^n,t_{k+1}^n]$. By a previous remark we have that:
    $$
      \sum_{k=0}^{n-1}f'(X_{t_k^n})(X_{t_{k+1}^n}-X_{t_k^n})\overset{\Prob}{\longrightarrow} \int_0^t f'(X_u)\dd{X_u}
    $$
    Now, by \mcref{SC:ito_quadratic_variation} we have that:
    $$
      \sum_{k=0}^{n-1}Y_{t_k^n}{(X_{t_{k+1}^n}-X_{t_k^n})}^2 \overset{\Prob}{\longrightarrow} \int_0^t Y_u\dd{{\langle X\rangle}_u}
    $$
    in the elementary case where $Y_u=\indi{(0,s]}(u)$, $s\geq 0$. By linearity, this immediately extends to the case where $Y$ is a random step function. By density, it further extends to the case where $Y$ is any continuous and adapted process. In particular, we may take
    $Y_u=f''(X_u)$, and the formula holds by uniform continuity:
    $$
      \max_{0\leq k\leq n}\abs{f''(X_{t_k^n})-f''(X_{u_k^n})}\almoste{\longrightarrow} 0
    $$
  \end{proof}
  \begin{theorem}
    Let $X^1,\ldots,X^d$ be Itô processes and $f\in C^2(\RR^d)$. Then, ${(f(X^1_t,\ldots,X^d_t))}_{t\geq 0}$ is an Itô process and:
    $$
      \dd{f(\vf{X})} =\sum_{i=1}^d\pdv{f}{x_i}(\vf{X})\dd{X^i_t}+\frac{1}{2}\sum_{i,j=1}^d\frac{\partial^2 f}{\partial x_i\partial x_j}(\vf{X})\dd{{\langle X^i,X^j\rangle}_t}
    $$
    where $\vf{X}:=(X^1,\ldots,X^d)$.
  \end{theorem}
  \subsubsection{Exponential martingales}
  \begin{lemma}[Doléans-Dade exponential]
    For any $\phi\in\MM^2_{\text{loc}}$, the process $Z^\phi={(Z_t^\phi)}_{t\geq 0}$ defined as
    $$
      Z_t^\phi:=\exp{\int_0^t \phi_u\dd{B_u}-\frac{1}{2}\int_0^t{\phi_u}^2\dd{u}}
    $$
    is a continuous local martingale.
  \end{lemma}
  \begin{proof}
    Applying Itô's formula to $f(x)=\exp{x}$ and $X_t=\int_0^t \phi_u\dd{B_u}-\frac{1}{2}\int_0^t{\phi_u}^2\dd{u}$ we get:
    \begin{equation*}
      \dd{Z^\phi_t} = \exp{X_t}\left( \phi_t\dd{B_t}-\frac{1}{2}{\phi_t}^2\dd{t}\right)+\frac{1}{2}\exp{X_t}{\phi_t}^2\dd{t}= \exp{X_t}\phi_t\dd{B_t}
    \end{equation*}
    Since, $Z^\phi_0=1$, we obtain that $\forall t \geq 0$:
    \begin{equation*}
      Z^\phi_t=1+\int_0^t Z^\phi_u\phi_u\dd{B_u}
    \end{equation*}
    and the result follows from \mcref{SC:ito_process_martingale}.
  \end{proof}
  \begin{lemma}\label{SC:preNovikov}
    If $M$ is a non-negative local martingale, then $M$ is a super-martingale. Moreover, for $T\in \RR_{\geq 0}$, ${(M_t)}_{t\in[0,T]}$ is a martingale if and only if $\Exp(M_T)\geq \Exp(M_0)$.
  \end{lemma}
  \begin{proof}
    Since $M$ is a local martingale with localizing sequence $(T_n)$, then $\forall t\geq 0$ we have that $\Exp(M_{t\wedge T_n}\mid \mathcal{F}_s )= M_{s\wedge T_n}$ and so:
    $$
      \begin{cases}
        M_{s\wedge T_n} \overset{\text{a.s.}}{\underset{n\to\infty}{\longrightarrow}} M_s \\
        M_{t \wedge T_n} \overset{\text{a.s.}}{\underset{n\to\infty}{\longrightarrow}} M_t
      \end{cases}
    $$
    Now, by \mnameref{P:fatou} we have:
    $$
      \Exp(M_t\mid \mathcal{F}_s)\leq \liminf_{n\to\infty}\Exp(M_{t\wedge T_n}\mid \mathcal{F}_s)=\liminf_{n\to\infty}M_{s\wedge T_n}\!=M_s
    $$
    which shows that $M$ is a super-martingale. Now, fix $T\geq 0$, and suppose that $\Exp(M_T)\geq \Exp(M_0)$. This forces the non-increasing map $t\mapsto \Exp(M_t)$ to be constant on $[0,T]$. In particular, for any $0\leq s\leq t\leq T$, the non-negative variable $M_s-\Exp(M_t\mid \mathcal{F}_s)$ has zero mean, hence is null a.s.
  \end{proof}
  \begin{theorem}[Novikov's condition]
    Let $t\geq 0$ be fixed and assume that:
    $$
      \Exp\left(\exp{\frac{1}{2}\int_0^t{\phi_u}^2\dd{u}}\right)<\infty
    $$
    Then, ${(Z^\phi_s)}_{s\in[0,t]}$ is a martingale.
  \end{theorem}
  \begin{proof}
    Fix $0<\varepsilon<1$. We have that for all $s\in[0,t]$:
    $$
      {\left( Z_s^{(1-\varepsilon)\phi}\right)}^{\frac{1}{1-\varepsilon^2}}={\left( Z_s^\phi\right)}^{\frac{1}{1+\varepsilon}}{\left(\exp{\frac{1}{2}\int_0^s \phi_u^2\dd{u}}\right)}^{\frac{\varepsilon}{1+\varepsilon}}
    $$
    Now, choosing $s=t\wedge T_n$, where $T_n$ is a localizing sequence for $Z^{(1-\varepsilon)\phi}$, taking expectation and using \mnameref{RFA:holder} we get:
    \begin{align*}
      \Exp\left[
        {\left( Z_{t\wedge T_n}^{(1-\varepsilon)\phi}\right)}^{\frac{1}{1-\varepsilon^2}}
      \right] & \leq \Exp{\left[
          { Z_{t\wedge T_n}^\phi}\right]}^{\frac{1}{1+\varepsilon}}\Exp{\left[
      {\exp{\frac{1}{2}\int_0^{t\wedge T_n} \phi_u^2\dd{u}}}\right]}^{\frac{\varepsilon}{1+\varepsilon}} \\
              & \leq \Exp{\left[
          {\exp{\frac{1}{2}\int_0^{t} \phi_u^2\dd{u}}}\right]}^{\frac{\varepsilon}{1+\varepsilon}}
    \end{align*}
    because $\Exp{\left[
          { Z_{t\wedge T_n}^\phi}\right]}=1$.  This implies that ${(Z_{t\wedge T_n}^{(1-\varepsilon)\phi})}$ is bounded in $L^p$ for $p=\frac{1}{\varepsilon^2}>1$. Thus:
    $$
      \Exp(Z_{t}^{(1-\varepsilon)\phi})=\lim_{n\to\infty}\Exp(Z_{t\wedge T_n}^{(1-\varepsilon)\phi})=1
    $$
    In particular, $\Exp\left[{\left( Z_{t}^{(1-\varepsilon)\phi}\right)}^{p}\right]\geq 1$ and so:
    $$
      1\leq \Exp{\left[
          { Z_{t}^\phi}\right]}^{\frac{1}{1+\varepsilon}}\Exp{\left[
          {\exp{\frac{1}{2}\int_0^{t} \phi_u^2\dd{u}}}\right]}^{\frac{\varepsilon}{1+\varepsilon}}
    $$
    Taking $\varepsilon\to 0$, yields $\Exp(Z_t^\phi)\geq 1$, which suffices to conclude by \mcref{SC:preNovikov}.
  \end{proof}
  \subsubsection{Girsanov's theorem}
  \begin{theorem}[Giranov's theorem]
    Let $\phi\in\MM^2_{\text{loc}}$ and suppose its associated exponential local martingale ${(Z_t^\phi)}_{t\geq 0}$ is a martingale. Then, the formula
    $$
      \QQ(A):=\Exp(Z_t^\phi\indi{A})\qquad\forall A\in \mathcal{F}_t
    $$
    defines a probability measure on $(\Omega, \mathcal{F}_t)$, under which the process $X={(X_s)}_{s\in[0,t]}$ defined as
    $$
      X_s:=B_s-\int_0^s \phi_u\dd{u}
    $$
    is a ${(\mathcal{F}_s)}_{s\in[0,t]}$-Brownian motion.
  \end{theorem}
  \begin{remark}
    Note that, by linearity we have that for ant $\mathcal{F}_t$-measurable non-negative random variable $Y$:
    $$
      \Exp_\QQ(Y)=\Exp(YZ_t^\phi)\qquad
      \Exp(Y)=\Exp_\QQ\left(\frac{Y}{Z_t^\phi}\right)
    $$
    where $\Exp_\QQ$ denotes the expectation with respect to $\QQ$.  This is useful for transferring computations between $\Prob$ and $\QQ$.
  \end{remark}
  \subsection{Stochastic differential equations}
  \subsubsection{Introduction}
  \begin{definition}
    Let $X={(X_t)}_{t\geq 0}$ be a stochastic process and $b:\RR_{\geq 0}\times\RR\to \RR$ and $\sigma:\RR_{\geq 0}\times \RR$ be deterministic functions called \emph{drift} and \emph{diffusion}, respectively. A \emph{stochastic differential equation} (\emph{SDE}) is an equation of the form:
    \begin{equation}\label{SC:SDE_eq}
      \dd{X_t}=b(t,X_t)\dd{t}+\sigma(t,X_t)\dd{B_t}
    \end{equation}
  \end{definition}
  \begin{definition}
    Consider the SDE of \mcref{SC:SDE_eq}. We say that a progressive process $X={(X_t)}_{t\geq 0}$ defined on $(\Omega, \mathcal{F}, {(\mathcal{F}_t)}_{t\geq 0}, \Prob)$ is a \emph{solution of the SDE} if ${(b(t,X_t))}_{t\geq 0}\in\MM^1_{\text{loc}}$ and ${(\sigma(t,X_t))}_{t\geq 0}\in\MM^2_{\text{loc}}$ and $\forall t\geq 0$:
    $$
      X_t=X_0+\int_0^t b(s,X_s)\dd{s}+\int_0^t \sigma(s,X_s)\dd{B_s}
    $$
  \end{definition}
  \subsubsection{Existence and uniqueness of solutions}
  \begin{lemma}[Grönwall's lemma]\label{SC:gronwall}
    Let ${(x_t)}_{t\in[0,T]}$ be a non-negative function in $L^1([0,T])$ satisfying that $\forall t\in[0,T]$:
    $$
      x_t\leq \alpha+\beta\int_0^t x_s\dd{s}
    $$
    for some constants $\alpha,\beta\geq 0$. Then, $x_t\leq \alpha\exp{\beta t}$ for all $t\in[0,T]$.
  \end{lemma}
  \begin{theorem}[Existence and uniqueness of solutions of SDEs]\label{SC:existence_uniqueness_SDE}
    Let $b,\sigma:\RR_{\geq 0}\times\RR\to \RR$ be a measurable function satisfying:
    \begin{itemize}
      \item Uniform spatial Lipschitz continuity: $\exists C>0$ such that $\forall t\geq 0$ and $\forall x,y\in\RR$ we have:
            \begin{align*}
              \abs{b(t,x)-b(t,y)}           & \leq C\abs{x-y} \\
              \abs{\sigma(t,x)-\sigma(t,y)} & \leq C\abs{x-y}
            \end{align*}
      \item Local square-integrability in time: $\forall t \geq 0$ we have: $$\int_0^t \abs{b(s,0)}^2\dd{s}<\infty
              \qquad \int_0^t \abs{\sigma(s,0)}^2\dd{s}<\infty
            $$
    \end{itemize}
    Then, for each initial condition $\zeta\in L^(\Omega,\mathcal{F}_0,\Prob)$, there exists a unique (up to indistinguishability) solution $X={(X_t)}_{t\geq 0}$ to the SDE of \mcref{SC:SDE_eq} with $X_0=\zeta$. Moreover, $X\in \MM^2$.
  \end{theorem}
  \begin{remark}
    In the proof of the above theorem, which we omit here, it can be shown that
    \begin{equation}\label{SC:expression_x_psi}
      X_t=\Psi_t\left(\zeta,{(B_s)}_{s\in[0,t]}\right)
    \end{equation}
    for some measurable function $\Psi_t:\RR\times C([0,t],\RR)\to \RR$.
  \end{remark}
  \subsubsection{Practical examples}
  \begin{proposition}[Langevin equation]
    Consider the following SDE:
    $$
      \dd{X_t}=-b X_t\dd{t}+\sigma\dd{B_t}
    $$ with $b,\sigma>0$ and $X_0=\zeta\in L^2(\Omega,\mathcal{F}_0,\Prob)$. Then, the solution is given by:
    $$
      X_t= \zeta \exp{-bt}+\sigma\int_0^t \exp{-b(t-s)}\dd{B_s}
    $$
  \end{proposition}
  \begin{remark}
    This SDE was proposed by Paul Langevin in 1908 to describe the random motion of a small particle in a fluid, due to collisions with the surrounding molecules. Note that the long-term behavior of $X_t$ has law of $N(0,\frac{\sigma^2}{2b})$ (because the second term has law $N(0,\frac{\sigma^2}{2b}(1-\exp{-2bt}))$), independently of the initial condition $\zeta$.
  \end{remark}
  \begin{proposition}[Geometric Brownian motion]
    Consider the following SDE:
    $$
      \dd{X_t}=X_t(b \dd{t}+\sigma\dd{B_t})
    $$
    with $X_0=\zeta\in L^2(\Omega,\mathcal{F}_0,\Prob)$. Then, the solution is given by:
    $$
      X_t=\zeta\exp{\left(b-\frac{\sigma^2}{2}\right) t+\sigma B_t}
    $$
  \end{proposition}
  \begin{proof}
    This equation has a unique solution and it's natural to expect it is of the form $X_t=\zeta \exp{Y_t}$, where $Y_t$ is an Itô process. Identifying $\dd{Y_t}=\psi_t\dd{t}+\phi_t\dd{B_t}$ and using the \mnameref{SC:ito_formula} we get:
    \begin{align*}
      \dd{(\zeta \exp{Y_t})} & =\zeta \exp{Y_t}\left(\dd{Y_t}+\frac{1}{2}\dd{{\langle Y\rangle}_t}\right)         \\
                             & =\zeta \exp{Y_t}\left(\psi_t\dd{t}+\phi_t\dd{B_t}+\frac{1}{2}\phi_t^2\dd{t}\right)
    \end{align*}
    and so $\phi_t=\sigma$ and $\psi_t=b-\frac{\sigma^2}{2}$.
  \end{proof}
  \begin{proposition}[Black-Scholes process]
    Consider the following SDE:
    $$
      \dd{X_t}=X_t(b_t \dd{t}+\sigma_t\dd{B_t})
    $$
    with $X_0=\zeta\in L^2(\Omega,\mathcal{F}_0,\Prob)$ and ${(b_t)}_{t\geq 0}$, ${(b_t)}_{t\geq 0}$ deterministic measurable bounded functions. Then, the solution is given by:
    $$
      X_t=\zeta\exp{\int_0^t \left(b_s-\frac{\sigma_s^2}{2}\right)\dd{s}+\int_0^t \sigma_s\dd{B_s}}
    $$
  \end{proposition}
  \begin{proof}
    This equation has a unique solution and as in the previous example, we expect $X_t=\zeta \exp{Y_t}$, where $Y_t$ is an Itô process. Identifying $\dd{Y_t}=\psi_t\dd{t}+\phi_t\dd{B_t}$ and using the \mnameref{SC:ito_formula} we get:
    \begin{equation*}
      \dd{(\zeta \exp{Y_t})} =\zeta \exp{Y_t}\left(\psi_t\dd{t}+\phi_t\dd{B_t}+\frac{1}{2}\phi_t^2\dd{t}\right)
    \end{equation*}
    and so $\phi_t=\sigma_t$ and $\psi_t=b_t-\frac{\sigma_t^2}{2}$.
  \end{proof}
  \subsubsection{Markov property for diffusions}
  \begin{definition}
    Let $b.\sigma:\RR\to\RR$ be two Lipschitz functions and consider the following \emph{homogeneous SDE}:
    \begin{equation}\label{SC:homogeneous_SDE}
      \begin{cases}
        \dd{X_t}=b(X_t)\dd{t}+\sigma(X_t)\dd{B_t} \\
        X_0=\zeta\in L^2(\Omega,\mathcal{F}_0,\Prob)
      \end{cases}
    \end{equation}
    These kinds of problems are called \emph{diffusions}.
  \end{definition}
  \begin{theorem}[Invariance under time shift]
    Let $X={(X_t)}_{t\geq 0}$ be a solution to the SDE of \mcref{SC:homogeneous_SDE} and assume we write $X_t$ as in \mcref{SC:expression_x_psi}. Then, for any $s,t\geq 0$ we have:
    $$
      X_{t+s}=\Psi_{t}(X_s,{(B_{u+s}-B_s)}_{u\in[0,t]})
    $$
  \end{theorem}
  \begin{definition}
    Let $f\in L^\infty(\RR)$ and $t\geq 0$. We define the function $P_tf$ as:
    $$
      \function{P_tf}{\RR}{\RR}{x}{\Exp(f(X_t^x))}
    $$
    where $X_t^x$ is the solution to the SDE of \mcref{SC:homogeneous_SDE} with $X_0=x$.
  \end{definition}
  \begin{corollary}
    For any $s,t\geq 0$ and any $f\in L^\infty(\RR)$ we have:
    $$
      \Exp(f(X_{t+s}))=(P_tf)(X_s)
    $$
  \end{corollary}
  \begin{proposition}
    The family ${(P_t)}_{t\geq 0}$ has the following properties:
    \begin{enumerate}
      \item $P_t$ is a bounded linear operator from $L^\infty(\RR)$ to itself for each $t\geq 0$.
      \item $P_0=\id$ and $P_{t+s}=P_t\circ P_s$ for all $s,t\geq 0$.
      \item If $f$ is continuous, then so is $t\mapsto P_tf(x)$ for each fixed $x\in\RR$.
      \item If $f$ is monotone, then so is $P_tf$ for each $t\geq 0$.
      \item If $f$ is Lipschitz, then so is $P_tf$ for each $t\geq 0$.
      \item If $\sigma, b,f\in\mathcal{C}_\text{b}^k(\RR)$ for some $k\geq 1$, then so is $P_tf$ for each $t\geq 0$.
    \end{enumerate}
  \end{proposition}
  \subsubsection{Generator of a diffusion}
  \begin{definition}[Generator]
    The \emph{generator} of the semigroup ${(P_t)}_{t\geq 0}$ is the linear operator $L$ defined by:
    $$
      (Lf)(x):=\lim_{t\to 0}\frac{P_tf(x)-f(x)}{t}
    $$
    for all $f\in L^\infty(\RR)$ and $x\in\RR$ such that the limit exists. Those functions form a vector space denoted by $\text{Dom}(L)$.
  \end{definition}
  \begin{theorem}
    Let $f\in \mathcal{C}_\text{b}^2(\RR)$. Then:
    \begin{enumerate}
      \item $Lf$ is well-defined and it is given $\forall x\in\RR$ by:
            $$
              Lf(x)=\frac{1}{2}\sigma^2(x)f''(x)+b(x)f'(x)
            $$
      \item For all $t\geq 0$, we have $P_tf\in \text{Dom}(L)$ and it satisfies the \emph{Kolmogorov's equation}:
            $$
              \dv{}{t}P_tf=P_t(Lf)=L(P_tf)
            $$
      \item The process ${(M_t)}_{t\geq 0}$ defined as
            $$
              M_t:=f(X_t)-f(X_0)-\int_0^t Lf(X_s)\dd{s}
            $$
            is a continuous square-integrable martingale.
    \end{enumerate}
  \end{theorem}
  \subsubsection{Connection with PDEs}
  For this section recall the diffusion equation:
  \begin{equation}\label{SC:sde_pde}
    \begin{cases}
      \dd{X_t^x}=b(X_t^x)\dd{t}+\sigma(X_t^x)\dd{B_t} \\
      X_0^x=x
    \end{cases}
  \end{equation}
  where $b,\sigma:\RR\to\RR$ are Lipschitz functions. Now, fix $f\in L^\infty(\RR)$ and consider the PDE:
  \begin{equation}\label{SC:pde_sde}
    \begin{cases}
      \pdv{v}{t}(t,x)=b(x)\pdv{v}{x}(t,x)+\frac{1}{2}\sigma^2(x)\pdv[2]{v}{x}(t,x) \\
      v(0,x)=f(x)
    \end{cases}
  \end{equation}
  where $v\in \mathcal{C}^{1,2}([0,\infty)\times\RR)$.
  \begin{theorem}\hfill
    \begin{enumerate}
      \item If $v$ is a bounded solution to the PDE of \mcref{SC:pde_sde}, then we must have $\forall (t,x)\in [0,\infty)\times\RR$:
            \begin{equation}\label{SC:sol_v}
              v(t,x)=\Exp(f(X_t^x))
            \end{equation}
      \item If $b,\sigma, f\in \mathcal{C}_\text{b}^2(\RR)$, then conversely, the function $v$ defined in \mcref{SC:sol_v} is a bounded solution of \mcref{SC:pde_sde}.
    \end{enumerate}
  \end{theorem}
  \begin{remark}
    The interest of this connection between SDEs and PDEs is two-fold: on the one hand, one can use tools from PDE theory to understand the distribution of $X_t^x$. Conversely, the probabilistic representation of \mcref{SC:sol_v} offers a practical way to numerically solve the PDE of \mcref{SC:pde_sde}, by simulation.
  \end{remark}
  \begin{theorem}[Feynman-Kac's formula]\label{SC:feynman_kac}
    Let $v\in \mathcal{C}^{1,2}([0,\infty)\times\RR)$ be a bounded solution to the PDE
    $$
      \begin{cases}
        \pdv{v}{t}(t,x)=-h(x)v(t,x)+b(x) \pdv{v}{x}(t,x)+\frac{1}{2}\sigma^2(x)\pdv[2]{v}{x}(t,x) \\
        v(0,x)=f(x)
      \end{cases}
    $$
    where $f,h:\RR\to\RR$ are measurable, with $h$ non-negative. Then, we have the representation
    $$
      v(t,x)=\Exp\left(f(X_t^x)\exp{-\int_0^t h(X_s^x)\dd{s}}\right)
    $$
    for all $(t,x)\in [0,\infty)\times\RR$.
  \end{theorem}
\end{multicols}
\end{document}