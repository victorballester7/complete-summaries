\documentclass[../../../main_math.tex]{subfiles}
% break in parametric statistical model
% break in Fisher theorem

\begin{document}
\changecolor{S}
\begin{multicols}{2}[\section{Statistics}]
  \subsection{Point estimation}
  \subsubsection{Statistical models}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space\footnote{From now on we will assume that the random variables are defined always in the same probability space $(\Omega,\mathcal{A},\Prob)$, so we will omit to say that.}, $\Theta$ be a set, $n\in\NN$ and $x_1,\ldots,x_n$ be a collection of data that we may assume that they are the outcomes of a random vector $\vf{X}_n=(X_1,\ldots,X_n)$ defined on $(\Omega,\mathcal{A},\Prob)$. Suppose, moreover, that the outcomes of $\vf{X}_n$ are in a set $\mathcal{X}\subseteq\RR^n$, the law $\vf{X}_n$ is one in the set $\mathcal{P}=\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\}$ and $\mathcal{F}$ is a $\sigma$-algebra over $\mathcal{X}$\footnote{That is, $\mathcal{P}$ denotes a family of probability distributions of $\vf{X}_n$ in $(\mathcal{X},\mathcal{F})$, indexed by $\theta\in\Theta$. Note that we denote that distribution of $\vf{X}_n$ by $\Prob^{\vf{X}_n}$ to distinguish it from the probability distribution $\Prob_{\vf{X}_n}$ in $(\Omega,\mathcal{A},\Prob)$.}. We define a \emph{statistical model} as the triplet $(\mathcal{X},\mathcal{F},\mathcal{P})$\footnote{Often we will take $\mathcal{F}=\mathcal{B}(\mathcal{X})$.}. The set $\mathcal{X}$ is called \emph{sample space}, and the set $\Theta$, \emph{parameter space}. The random vector $\vf{X}_n$ is called \emph{random sample}. If, moreover, $X_1,\ldots,X_n$ are \iid random variables, $\vf{X}_n$ is called a \emph{simple random sample}. The value $(x_1,\ldots,x_n)\in\mathcal{X}$ is called a \emph{realization} of $(X_1,\ldots,X_n)$.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We say $\mathcal{P}=\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\}$ is \emph{identifiable} if the function $$\function{}{\Theta}{\mathcal{P}}{\theta}{\Prob_\theta^{\vf{X}_n}}$$ is injective\footnote{From now on, we will suppose that all the sets $\mathcal{P}$ are always identifiable.}.
  \end{definition}
  \begin{definition}
    A statistical model $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ is said to be \emph{parametric} if $\Theta\subseteq \RR^d$ for some $d\in\NN$\footnote{There are cases where $\Theta$ is not a subset of $\RR^d$. For example, we could have $\Theta=\{f:\RR\rightarrow\RR_{\geq 0} : \int_{-\infty}^{+\infty}f(x)\dd{x}=1\}$.}.
  \end{definition}
  \subsubsection{Statistics and estimators}
  \begin{definition}[Statistic]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We define a \emph{statistic} $\vf{T}$ as a Borel measurable function. That is, $\vf{T}$ can be written as $\vf{T}=\vf{h}(X_1,\ldots,X_n)$, where $\vf{h}:\mathcal{X}\rightarrow\RR^m$ is a Borel measurable function. Hence, $\vf{T}$ is a random vector. The value $m$ is the \emph{dimension} of the statistic.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We define the \emph{sample mean} as the statistic: $$T(X_1,\ldots,X_n)=\frac{1}{n}\sum_{i=1}^nX_i=:\overline{X}_n$$
    Given a realization $(x_1,\ldots,x_n)\in\mathcal{X}$, we will denote $\overline{x}_n:=\overline{X}_n(x_1,\ldots,x_n)$\footnote{Some times, and if the context is clear, we will denote $\overline{x}_n$ simply as $\overline{x}$.}.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We define the \emph{sample variance} as the statistic: $$T(X_1,\ldots,X_n)=\frac{1}{n}\sum_{i=1}^n{(X_i-\overline{X}_n)}^2=:{S_n}^2$$ We define the \emph{corrected sample variance} as the statistic:
    $$T(X_1,\ldots,X_n)=\frac{1}{n-1}\sum_{i=1}^n{(X_i-\overline{X}_n)}^2=:\tilde{S}_n{}^2$$
    Given a realization $(x_1,\ldots,x_n)\in\mathcal{X}$, we will denote ${s_n}^2:={S_n}^2(x_1,\ldots,x_n)$ and $\tilde{s}_n{}^2:=\tilde{S}_n{}^2(x_1,\ldots,x_n)$\footnote{Some times, and if the context is clear, we will denote ${s_n}^2$ and $\tilde{s}_n{}^2$ simply as $s^2$ and $\tilde{s}^2$, respectively.}.
  \end{definition}
  \begin{proposition}
    Let $X_1,\ldots,X_n$ be random variables. Then: $${S_n}^2=\frac{1}{n}\sum_{i=1}^n{X_i}^2-{\overline{X}_n}^2$$
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, ${\theta} \in\Theta$ and $g:\Theta\rightarrow\Theta$ be a function. An \emph{estimator} of $g({\theta})$ is a statistic ${\hat\theta}$ whose outcomes are in $\Theta$ and does not depend on any unknown parameter. It is used to give an estimation of the (supposedly unknown) parameter $g({\theta})$.
  \end{definition}
  \subsubsection{Properties of estimators}
  \begin{definition}[Bias]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be an integrable estimator of $g({\theta})\in\Theta$. We define the \emph{bias} of ${\hat\theta}$ with respect to ${\theta}$ as: $$\bias({\hat\theta}):=\Exp({\hat\theta})-g({\theta})$$ We say that ${\hat\theta}$ is an \emph{unbiased estimator} of $g({\theta})$ if $\bias({\hat\theta})=0$ $\forall\theta\in\Theta$. Otherwise we say that it is a \emph{biased estimator} of $g({\theta})$.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be an integrable estimator of $g({\theta})\in\Theta$. Suppose that $\bias(\hat\theta)=cg(\theta)$ for some $c\in\RR$. Then, $\frac{\hat\theta}{c+1}$ is an unbiased estimator for $g(\theta)$
  \end{proposition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model such that $X_1,\ldots,X_n$ are square-integrable\footnote{That is, with finite 2nd moments.} \iid random variables with mean $\mu$ and variance $\sigma^2$. Then: $$\Exp(\overline{X}_n)=\mu\quad\text{and}\quad\Var{(\overline{X}_n)}=\frac{\sigma^2}{n}$$
    Hence, the estimator $\overline{X}_n$ of $\mu$ is unbiased.
  \end{proposition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model such that $X_1,\ldots,X_n$ are square-integrable \iid random variables with mean $\mu$ and variance $\sigma^2$. Then: $$\Exp({S_n}^2)=\frac{n-1}{n}\sigma^2\quad\text{and}\quad\Exp(\tilde{S}_n{}^2)=\sigma^2$$
    Hence, the estimator $\tilde{S}_n{}^2$ of $\sigma^2$ is unbiased whereas the estimator ${S_n}^2$ of $\sigma^2$ is biased.
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be a square-integrable integrable estimator of $g({\theta})\in\Theta$. The \emph{mean squared error} (\emph{MSE}) of ${\hat\theta}$ is the function: $$\MSE({\hat\theta}):=\Exp\left({\left({\hat\theta}-g({\theta})\right)}^2\right)$$
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be a square-integrable integrable estimator of $g({\theta})\in\Theta$. Then: $$\MSE({\hat\theta})=\Var({\hat\theta})+{(\bias({\hat\theta}))}^2$$
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$, ${\tilde\theta}$ be estimators of $g({\theta})\in\Theta$. We say that ${\hat\theta}$ is \emph{more efficient than} ${\tilde\theta}$ if $$\Var{({\hat\theta})}<\Var{({\tilde\theta})}\quad\forall{\theta}\in\Theta$$
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model and ${\hat\theta}$ be a square integrable estimator of ${\theta}\in\Theta$. We say that ${\hat\theta}$ is a \emph{minimum-variance unbiased estimator} (\emph{MVUE}) if it is an unbiased estimator that has lower variance than any other unbiased estimator $\forall {\theta}\in\Theta$.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model. Then, the MVUE is unique almost surely.
  \end{proposition}
  \subsubsection{Sufficient statistics}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $\vf{T}$ be a statistic. We say that $\vf{T}$ is \emph{sufficient} for $\theta\in\Theta$ if the joint conditional distribution of $(X_1,\ldots,X_n)$ given $\vf{T}(X_1,\ldots,X_n)=\vf{t}$ does not depend on $\theta$.
  \end{definition}
  \begin{theorem}[Fisher-Neyman factorization theorem]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $T$ be a statistic. Then, $T$ is sufficient if and only if $\forall \vf{x}_n\in\mathcal{X}$ we have:
    \begin{enumerate}
      \item For the discrete case: $$p_{\vf{X}_n}(\vf{x}_n;\theta)=g(T(\vf{x}_n);\theta)h(\vf{x}_n)$$
      \item For the continuous case:  $$f_{\vf{X}_n}(\vf{x}_n;\theta)=g(T(\vf{x}_n);\theta)h(\vf{x}_n)$$
    \end{enumerate}
    for certain functions $g$ and $h$. Here we have denoted by $p_{\vf{X}_n}(\vf{x}_n;\theta)$ the joint pmf of $\vf{X}_n$ (in the discrete case) and by $f_{\vf{X}_n}(\vf{x}_n;\theta)$ the joint pdf of $\vf{X}_n$ (in the continuous case).
  \end{theorem}
  \subsubsection{Asymptotic properties}
  \begin{definition}
    For each $n\in\NN$, let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a parametric statistical model with $X_1,\ldots,X_n$ being i.i.d., $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}_n$ be an estimator of $g({\theta})\in\Theta$. We say that the sequence $({{\hat\theta}}_n)$ is a \emph{weakly consistent estimator} of $g(\theta)$ if ${{\hat\theta}}_n\overset{\Prob}{\longrightarrow}g(\theta)$.
  \end{definition}
  \begin{definition}
    For each $n\in\NN$, let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a parametric statistical model with $X_1,\ldots,X_n$ being i.i.d., $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}_n$ be an estimator of $g({\theta})\in\Theta$. We say that the sequence $({{\hat\theta}}_n)$ is a \emph{strongly consistent estimator} of $g(\theta)$ if ${{\hat\theta}}_n\overset{\text{a.s.}}{\longrightarrow}g(\theta)$.
  \end{definition}
  \begin{definition}
    For each $n\in\NN$, let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a parametric statistical model with $X_1,\ldots,X_n$ being i.i.d., $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}_n$ be an estimator of $g({\theta})\in\Theta$. We say that the sequence $({{\hat\theta}}_n)$ is a \emph{consistent estimator in $L^2$} of $g(\theta)$ if $$\lim_{n\to\infty}\Exp\left({\left({\hat\theta}_n-g({\theta})\right)}^2\right)=\lim_{n\to\infty}\MSE({\hat\theta}_n)=0$$
  \end{definition}
  \begin{proposition}
    For each $n\in\NN$, let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a parametric statistical model with $X_1,\ldots,X_n$ being i.i.d., $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}_n$ be a consistent estimator in $L^2$ of $g({\theta})\in\Theta$. Then, ${{\hat\theta}}_n$ is a weakly consistent estimator of $g(\theta)$.
  \end{proposition}
  \begin{definition}
    For each $n\in\NN$, let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a parametric statistical model with $X_1,\ldots,X_n$ being i.i.d., $g:\Theta\rightarrow\Theta$ be a function and ${{\hat\theta}}_n$ be an estimator of $g(\theta)\in\Theta$. We say that the sequence $({{\hat\theta}}_n)$ is an \emph{asymptotically unbiased estimator} of $g(\theta)$ if $$\Exp({{\hat\theta}}_n)\longrightarrow g(\theta)$$
  \end{definition}
  \begin{definition}
    For each $n\in\NN$, let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a parametric statistical model with $X_1,\ldots,X_n$ being \iid whose variance is $\sigma^2$, $g:\Theta\rightarrow\Theta$ be a function and ${{\hat\theta}}_n$ be an estimator of $g(\theta)\in\Theta$. We say that the sequence $({\hat\theta}_n)$ is \emph{an asymptotically normal estimator} of $g(\theta)$ with asymptotically variance $\frac{\sigma^2}{n}$ if $$\sqrt{n}({\hat\theta}_n-g(\theta))\overset{\text{d}}{\longrightarrow}N(0,\sigma^2)\qquad\forall \theta\in\Theta$$
    In that case, we denote it by ${\hat\theta}_n\overset{\text{a}}{\sim}N\left(g(\theta),\frac{\sigma^2}{n}\right)$.
  \end{definition}
  \subsubsection{Methods of estimation}
  \begin{definition}[Method of moments]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model such that $X_1,\ldots,X_n$ are \iid random variables, and $\mu_k$ be $k$-th moment of each of them. Suppose $\vf{\theta}=(\theta_1,\ldots,\theta_d)$. Then, given a realization $\vf{x}_n=(x_1,\ldots,x_n)\in\mathcal{X}$ of $\vf{X}_n$, an estimator $\vf{\tilde{\theta}}(\vf{x}_n)=(\tilde{\theta}_1(\vf{x}_n),\ldots,\tilde{\theta}_d(\vf{x}_n))$ of $\vf{\theta}$ is given by the solution of the following system:
    $$
      \left\{
      \begin{aligned}
        \frac{1}{n}\sum_{i=1}^nx_i     & =\mu_1(\theta_1,\ldots,\theta_d) \\
        \frac{1}{n}\sum_{i=1}^n{x_i}^2 & =\mu_2(\theta_1,\ldots,\theta_d) \\
                                       & \;\;\vdots                       \\
        \frac{1}{n}\sum_{i=1}^n{x_i}^d & =\mu_d(\theta_1,\ldots,\theta_d)
      \end{aligned}
      \right.
    $$
  \end{definition}
  \begin{proposition}
    The estimators obtained by the method of moments are strongly consistent and consistent in $L^2$.
  \end{proposition}
  \begin{definition}[Likelihood]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$.
    \begin{enumerate}
      \item For the discrete case, let $p_{\vf{X}_n}(\vf{x}_n;\theta)$ be the pmf of $\Prob_{\theta}^{\vf{X}_n}$. In this case, we define the \emph{likelihood function} as the function:
            $$\function{L(\cdot;\vf{x}_n)}{\Theta}{\RR}{\theta}{p_{\vf{X}_n}(\vf{x}_n;\theta)}$$
      \item For the continuous case, let $f_{\vf{X}_n}(\vf{x}_n;\theta)$ be the pdf of $\Prob_{\theta}^{\vf{X}_n}$. In this case, we define the \emph{likelihood function} as the function:
            $$\function{L(\cdot;\vf{x}_n)}{\Theta}{\RR}{\theta}{f_{\vf{X}_n}(\vf{x}_n;\theta)}$$
    \end{enumerate}
  \end{definition}
  \begin{definition}[Maximum likelihood method]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$. A \emph{maximum likelihood estimator} (\emph{MLE}) of $\theta\in\Theta$ is the estimator $\hat{\theta}$ such that: $$L(\hat{\theta};\vf{x}_n)=\max\{L(\theta;\vf{x}_n):\theta\in\Theta\}\footnote{Note that sometimes this estimator is not unique or may not even exist.}$$
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf\theta\in\Theta\subseteq \RR^d\})$ be a parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$. We define the \emph{log-likelihood function} as: $$\ell(\vf\theta;\vf{x}_n):=\ln L(\vf\theta;\vf{x}_n)$$
    We define the \emph{score function} as: $$\vf{S}(\vf\theta;\vf{x}_n):=\pdv{\ell}{\vf\theta}(\vf\theta,\vf{x}_n)$$
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$ Then, a MLE $\vf{\hat\theta}$ of $\vf\theta$ is the one that satisfies:
    $$\pdv{L}{\vf\theta}(\vf{\hat\theta};\vf{x}_n)=\vf{0}$$
    Or equivalently, $\pdv{\ell}{\vf\theta}(\vf{\hat\theta};\vf{x}_n)=\vf{0}$.
  \end{proposition}
  \begin{proposition}[Invariance of the MLE]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $g:\Theta\rightarrow\Theta$ be a measurable function. Suppose $\hat\theta$ is a MLE of $\theta$. Then, $g(\hat\theta)$ is a MLE of $g(\theta)$.
  \end{proposition}
  \subsubsection{Regular statistical models}
  \begin{definition}
    A statistical model $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\})$ is said to be \emph{regular} if it satisfies the following conditions:
    \begin{enumerate}
      \item $\Theta$ is open.
      \item The support of $\Prob_{\theta}^{\vf{X}_n}$ does not depend on $\theta$.
      \item The function $L(\theta;\vf{x}_n)$ is two times differentiable with respect to $\theta$ $\forall \vf{x}_n\in\mathcal{X}$ (except in a set of probability zero) and moreover:
            \begin{enumerate}
              \item For the discrete case: $$\pdv[2]{}{\theta}\sum_{\vf{x}_n\in\mathcal{X}}L(\theta;\vf{x}_n)=\sum_{\vf{x}_n\in\mathcal{X}}\pdv[2]{L}{\theta}(\theta;\vf{x}_n)$$
              \item For the continuous case: $$\pdv[2]{}{\theta}\int_{\mathcal{X}}L(\theta;\vf{x}_n)\dd{\vf{x}_n}=\int_{\mathcal{X}}\pdv[2]{L}{\theta}(\theta;\vf{x}_n)\dd{\vf{x}_n}$$
            \end{enumerate}
      \item For all $\theta\in\Theta$, we have: $$0<\int_\mathcal{X}{\left(\pdv[2]{\ell}{\theta}(\theta;\vf{x}_n)\right)}^2 f_{\vf{X}_n}(\vf{x}_n;\vf\theta)\dd{\vf{x}_n}<\infty$$
    \end{enumerate}
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a regular parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$. We define the \emph{observed information} of the model as:
    $$\vf{J}(\vf\theta;\vf{x}_n)=-\pdv[2]{\ell}{\vf\theta}(\vf\theta;\vf{x}_n)$$
    We define the \emph{Fisher information} of the model as: $$\vf{I}(\vf\theta)=\Exp(\vf{J}(\vf\theta;\vf{X}_n))=-\Exp\left(\pdv[2]{\ell}{\vf\theta}(\vf\theta;\vf{X}_n)\right)\footnote{Since generally $\vf{J}(\vf\theta;\vf{X}_n)$ will be a matrix, the expectation of $\vf{J}(\vf\theta;\vf{X}_n)$ is taken component by component.}$$
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a regular parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$. Then, $\Exp(\vf{S}(\vf\theta;\vf{X}_n))=0$ and $$\vf{I}(\vf\theta)=\Var(\vf{S}(\vf\theta;\vf{X}_n))=\Exp\left[{\left(\pdv{\ell}{\vf\theta}(\vf\theta;\vf{X}_n)\right)}^2\right]$$
    for all $\vf\theta\in\Theta$.
  \end{proposition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{X_1}:\vf\theta\in\Theta\})$ be a regular parametric statistical model of one observation $x_1\in \mathcal{X}$. Then, the model corresponding to $n$ \iid observations $x_1,\ldots,x_n$ is regular and $$\vf{I}(\vf\theta)=n \vf{I}_1(\vf\theta)$$
    where $\vf{I}_1(\vf\theta)$ denotes the Fisher information in the model with one observation.
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $T$ be a statistic. We say that $T$ is \emph{regular} if
    \begin{enumerate}
      \item for the discrete case: $$\pdv{}{\theta}\sum_{\vf{x}_n\in\mathcal{X}}T(\vf{x}_n)L(\theta;\vf{x}_n)=\sum_{\vf{x}_n\in\mathcal{X}}T(\vf{x}_n)\pdv{L}{\theta}(\theta;\vf{x}_n)$$
      \item for the continuous case: $$\pdv{}{\theta}\int_{\mathcal{X}}T(\vf{x}_n)L(\theta;\vf{x}_n)\dd{\vf{x}_n}=\int_{\mathcal{X}}T(\vf{x}_n)\pdv{L}{\theta}(\theta;\vf{x}_n)\dd{\vf{x}_n}$$
    \end{enumerate}
    for all $\theta\in\Theta$.
  \end{definition}
  \begin{theorem}[CramÃ©r-Rao bound]
    Let $(\mathcal{X}, \mathcal{F}, \{\Prob_{\theta}^{\vf{X}_n}: \theta\in\Theta\subseteq\RR\})$ be a regular parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$, $g:\Theta\rightarrow\Theta$ be a differentiable function and ${\hat\theta}$ be a regular estimator of $g({\theta})\in\Theta$. Then: $$\Var(\hat\theta)\geq \frac{{g'(\theta)}^2}{I(\theta) }\left[1+{\left(\bias'(\hat\theta)\right)}^2\right]$$ Moreover if the estimator $\hat\theta$ is unbiased we have: $$\Var(\hat\theta)\geq \frac{{g'(\theta)}^2}{I(\theta)}$$
  \end{theorem}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a regular statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$, $g:\Theta\rightarrow\Theta$ be a differentiable function and ${\hat\theta}$ be a regular and unbiased estimator of $g({\theta})\in\Theta$. We say that $\hat\theta$ is an \emph{efficient estimator} of $g(\theta)$ if $$\Var(\hat\theta)=\frac{{g'(\theta)}^2}{I(\theta)}$$
    We say that $\hat\theta$ is an \emph{asymptotic efficient estimator} of $g(\theta)$ if the asymptotic variance of $\hat\theta$ is $\frac{{g'(\theta)}^2}{I(\theta)}$.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a regular statistical model, $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be a regular, unbiased and efficient estimator of $g({\theta})\in\Theta$. Then, $\hat\theta$ is a MVUE in the class of regular estimators.
  \end{proposition}
  \begin{theorem}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a regular statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$ and ${\hat\theta}$ be a MLE of ${\theta}\in\Theta$. Suppose that $\pdv[2]{\ell}{\theta}$ is a continuous function of $\theta$ and that $$\left|\pdv[2]{\ell}{\theta}(\tilde{\theta};\vf{x}_n)\right|<h(\vf{x}_n;\theta)$$
    for all $\tilde{\theta}$ in a neighbourhood of $\theta$ with $\int_\mathcal{X}h(\vf{x}_n;\theta)L(\theta;\vf{x}_n)\dd{\vf{x}_n}<\infty$. Then:
    $$\hat\theta\overset{\text{d}}{\longrightarrow}N\left(\theta,\frac{1}{I(\theta)}\right)$$
    Thus, ${\hat\theta}$ is an asymptotically efficient estimator of $\theta$. Hence, an asymptotic confidence interval for $\theta$ of confidence $1-\alpha$ is:
    $$\theta\in\left(\hat\theta-\frac{z_{1-\frac{\alpha}{2}}}{\sqrt{I(\hat\theta)}},\hat\theta+\frac{z_{1-\frac{\alpha}{2}}}{\sqrt{I(\hat\theta)}}\right)$$
    where $z_{1-\frac{\alpha}{2}}$ denote the $1-\frac{\alpha}{2}$ quantile of the standard normal distribution (see \mcref{S:quantile}).
  \end{theorem}
  \begin{theorem}[Delta method]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a statistical model, $g:\Theta\rightarrow\Theta$ be a two-times differentiable function such that $g'(\theta)\ne 0$ and $\hat\theta$ be an estimator of $\theta\in\Theta$. Then:
    $$g(\hat\theta)\overset{\text{d}}{\longrightarrow}N\left(g(\theta),{g'(\theta)}^2\Var(\hat\theta)\right)$$
  \end{theorem}
  \subsubsection{Order statistics}
  \begin{definition}
    Let $X_1,\ldots,X_n$ be random variables. We define the $k$-th \emph{order statistic}, denoted by $X_{(k)}$ of the sample $X_1,\ldots,X_n$ as the $k$-th smallest value of it. In particular:
    $$X_{(1)}:=\min\{X_1,\ldots,X_n\}\quad X_{(n)}:=\max\{X_1,\ldots,X_n\}$$
    The sample $X_{(1)},\ldots,X_{(n)}$ is usually called \emph{order statistics}.
  \end{definition}
  \subsection{Distributions relating \texorpdfstring{$N(\mu,\sigma^2)$}{N(mu,sigma2)}}
  \subsubsection{Standard normal distribution}
  \begin{definition}
    We denote by $\Phi(t)$ the cdf of a standard normal distribution $N(0,1)$.
  \end{definition}
  \begin{definition}[Quantile]\label{S:quantile}
    We define quantile function $Q_X(p)$ of a distribution of a random variable $X$ as the inverse function of the cdf. That is, $Q_X(p)$ satisfies:
    $$\Prob(X\leq Q_X(p))=p$$
    In particular, we denote the quantile of a standard normal distribution as $z_p:=Q_X(p)=\Phi^{-1}(p)$.
  \end{definition}
  \subsubsection{Multivariate normal distribution}
  \begin{definition}
    Let $\vf{b}\in\RR^n$, $\vf{\Sigma}\in\mathcal{M}_n(\RR)$ be a symmetric positive-definite matrix and $\vf{X}$ be a random vector. We say that $\vf{X}$ has \emph{multivariate normal distribution}, and we denote it by $\vf{X}\sim N(\vf{b},\vf\Sigma)$ if has density function:
    $$f_{\vf{X}}(\vf{x})={(2\pi)}^{-\frac{n}{2}}{(\det\vf\Sigma)}^{-\frac{1}{2}}\exp{-\frac{\transpose{(\vf{x}-\vf{b})}{\vf\Sigma}^{-1}(\vf{x}-\vf{b})}{2}}$$
    The vector $\vf{b}$ is called \emph{mean vector} and the matrix $\vf\Sigma$, \emph{covariance matrix}.
  \end{definition}
  \begin{proposition}
    Let $\vf{\Sigma}\in\mathcal{M}_n(\RR)$ be a symmetric positive-definite matrix. Then, $\exists\vf{A}\in\GL_n(\RR)$ such that $\vf\Sigma=\vf{A}\transpose{\vf{A}}$\footnote{When $\vf\Sigma$ is the covariance matrix, the matrix $\vf{A}$ such that $\vf\Sigma=\vf{A}\transpose{\vf{A}}$ plays the role of the \emph{multivariate standard deviation}.}.
  \end{proposition}
  \begin{proposition}
    Let $\vf{b}\in\RR^n$, $\vf\Sigma=\vf{A}\transpose{\vf{A}}\in\mathcal{M}_n(\RR)$ be a symmetric positive-definite matrix with $\vf{A}\in\GL_n(\RR)$ and $\vf{X}$, $\vf{Z}$ be random vectors.
    \begin{itemize}
      \item If $\vf{Z}\sim N(\vf{0},\vf{I}_n)$, then $\vf{AZ}+\vf{b}\sim N(\vf{b},\vf\Sigma)$.
      \item If $\vf{X}\sim N(\vf{b},\vf{I}_n)$, then ${\vf{A}}^{-1}(\vf{X}-\vf{b})\sim N(\vf{0},\vf\Sigma)$.
    \end{itemize}
  \end{proposition}
  \begin{proposition}
    Let $\vf{b}\in\RR^n$, $\vf{\Sigma}\in\mathcal{M}_n(\RR)$ be a symmetric positive-definite matrix and $\vf{X}=(X_1,\ldots,X_n)\sim N(\vf{b},\vf\Sigma)$. Then, $\Exp(\vf{X})=\vf{b}$ and $\Var(\vf{X})=\vf\Sigma$ and moreover:
    \begin{gather*}
      \vf{b}=\transpose{(\Exp(X_1),\ldots,\Exp(X_2))}\\
      \vf\Sigma=
      \begin{pmatrix}
        \Var(X_1)     & \cov(X_1,X_2) & \cdots & \cov(X_1,X_n) \\
        \cov(X_2,X_1) & \Var(X_2)     & \cdots & \cov(X_2,X_n) \\
        \vdots        & \vdots        & \ddots & \vdots        \\
        \cov(X_n,X_1) & \cov(X_n,X_2) & \cdots & \Var(X_n)     \\
      \end{pmatrix}
    \end{gather*}
  \end{proposition}
  \begin{proposition}
    Let $\vf{b},\vf{c}\in\RR^n$, $\vf{\Sigma}\in\mathcal{M}_n(\RR)$ be a symmetric positive-definite matrix, $\vf{B}\in\GL_n(\RR)$, $\vf{X}\sim N(\vf{b},\vf\Sigma)$ and $\vf{Y}:=\vf{BX}+\vf{c}$. Then: $$\vf{Y}\sim N(\vf{Bb}+\vf{c},\vf{B\Sigma}\transpose{\vf{B}})$$
  \end{proposition}
  \subsubsection{\texorpdfstring{$\chi^2$}{chi2}-distribution}
  \begin{proposition}
    Let $n\in\NN$ and $X_1,\ldots,X_n$ be independent random variables such that $X_i\sim \text{Gamma}(\alpha_i,\beta)$ for $i=1,\ldots,n$. Then: $$\sum_{i=1}^nX_i\sim\text{Gamma}\left(\sum_{i=1}^n\alpha_i,\beta\right)$$
  \end{proposition}
  \begin{corollary}
    Let $n\in\NN$ and $Z_1,\ldots,Z_n$ be \iid random variable with standard normal distribution. Then: $${Z_1}^2+\cdots+{Z_n}^2\sim\text{Gamma}\left(\frac{n}{2},\frac{1}{2}\right)$$
  \end{corollary}
  \begin{definition}
    We define the \emph{chi-squared distribution with $n$ degrees of freedom}, denoted as ${\chi_n}^2$, as the distribution $${\chi_n}^2:=\text{Gamma}\left(\frac{n}{2},\frac{1}{2}\right)$$ which is the distribution of ${Z_1}^2+\cdots+{Z_n}^2$, where $Z_1,\ldots,Z_n\sim N(0,1)$ are \iid random variables. Its pdf is:
    $$f_{{\chi_n}^2}(x)=\frac{1}{2^\frac{n}{2}\Gamma\left(\frac{n}{2}\right)}x^{\frac{n}{2}-1}\exp{-\frac{x}{2}}\indi{(0,\infty)}(x)$$
    We will denote by ${\chi_{n;p}}^2:=Q_{{\chi_n}^2}(p)$ the quantile of the ${\chi_n}^2$.
  \end{definition}
  \begin{proposition}
    Let $X\sim{\chi_a}^2$ and $Y\sim{\chi_b}^2$ be \iid random variables. Then: $$X+Y\sim{\chi_{a+b}}^2$$
  \end{proposition}
  \begin{proposition}
    Let $X\sim\text{Gamma}(\alpha,\beta)$ and $c\in\RR_{>0}$. Then, $cX\sim\text{Gamma}(\alpha,\beta/c)$. In particular, if $X\sim\text{Gamma}(n,1)$, then $2X\sim {\chi_{2n}}^2$.
  \end{proposition}
  \subsubsection{Student's \texorpdfstring{$t$}{t}-distribution}
  \begin{definition}
    Let $n\in\NN$ and $Z\sim N(0,1)$ and $Y\sim{\chi_n}^2$ be independent random variables. We define the \emph{Student's $t$-distribution with $n$ degrees of freedom} as the distribution of: $$\frac{Z}{\sqrt{Y/n}}$$
    We will denote by $t_{n;p}:=Q_{t_n}(p)$ the quantile of the $t_n$.
  \end{definition}
  \begin{proposition}
    Let $n\in\NN$. Then, the pdf of $t_n$ is: $$f_{t_n}(x)=\frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{\pi n}\Gamma\left(\frac{n}{2}\right)}{\left(1+\frac{x^2}{n}\right)}^{-\frac{n+1}{2}}\footnote{It makes sense if we replace the value $n\in\NN$ for a value $\nu\in\RR_{>0}$. However the original definition of $t_n$ from the ${\chi_n}^2$ fails.}$$
  \end{proposition}
  \subsubsection{Fisher's theorem}
  \begin{theorem}[Fisher's theorem]
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots, \\X_n\sim N(\mu,\sigma^2)\text{ i.i.d.}:(\mu,\sigma^2)\in\RR\times\RR_{\geq 0}\})$ be a parametric statistical model. Then:
    \begin{enumerate}
      \item $\overline{X}_n\sim N\left(\mu,\frac{\sigma^2}{n}\right)$
      \item $\tilde{S}_n{}^2\sim\frac{\sigma^2}{n-1}{\chi_{n-1}}^2$
      \item $\overline{X}_n$ and $\tilde{S}_n{}^2$ are independent.
    \end{enumerate}
  \end{theorem}
  \begin{corollary}
    Let $n\in\NN$ and $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ be \iid random variables. Then: $$\frac{\overline{X}_n-\mu}{\frac{\tilde{S}_n}{\sqrt{n}}}\sim t_{n-1}$$
  \end{corollary}
  \begin{corollary}
    Let $n\in\NN$ and $X\sim t_n$ be a random variable. Then: $$X\overset{\text{d}}{\longrightarrow }N\left(0,1\right)$$
    Hence, $N(0,1)=t_\infty$.
  \end{corollary}
  \begin{corollary}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and suppose $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ are \iid random variables. Then, the estimators $\overline{X}_n$ of $\mu$ and $\tilde{S}_n{}^2$ of $\sigma^2$ are unbiased and consistent.
  \end{corollary}
  \begin{center}
    \begin{minipage}{\linewidth}
      \centering
      \includestandalone[mode=image|tex,width=\linewidth]{Images/student-normal}
      \captionof{figure}{Probability density function of 4 Student's $t$-distribution together with a standard normal $N(0,1)=t_{\infty}$.}
    \end{minipage}
  \end{center}
  \subsection{Confidence intervals}
  \subsubsection{Confidence regions}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a parametric statistical model and $\vf{g}:\Theta\rightarrow\RR^m$ be a function with $m\leq d$. A \emph{confidence region} for $\vf{g}(\vf\theta)$ with \emph{confidence level} $\gamma\in[0,1]$ is a random region $C(\vf{X}_n)$ such that: $$\Prob(\vf{g}(\vf\theta)\in C(\vf{X}_n))\geq \gamma\quad\forall \theta\in\Theta$$ If $d=1$, we talk about \emph{confidence intervals}. The value $\alpha:=1-\gamma$ is called \emph{significance level}.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a parametric statistical model and $\vf{g}:\Theta\rightarrow\RR^m$ be a function with $m\leq d$. A \emph{pivot} for $\vf{g}(\vf\theta)$ is a measurable function
    $$\function{\pi}{\mathcal{X}\times \vf{g}(\Theta)}{\RR^m}{(\vf{x}_n,\vf{g}(\vf\theta))}{\vf\pi(\vf{x}_n,\vf{g}(\vf\theta))}$$
    such that the distribution of $\vf\pi(\vf{x}_n,\vf{g}(\vf\theta))$ does not depend on $\vf\theta$.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a parametric statistical model, $\gamma\in[0,1]$, $\vf{g}:\Theta\rightarrow\RR^m$ be a function with $m\leq d$, $\vf\pi(\vf{x}_n,\vf{g}(\vf\theta))$ be a pivot for $\vf{g}(\vf\theta)$ and $B\in\mathcal{B}(\RR^m)$ such that: $$\Prob(\vf\pi(\vf{X}_n,\vf{g}(\vf\theta))\in B)\geq \gamma\quad\forall \vf\theta\in\Theta$$
    Then: $$C(\vf{X})=\{\vf{g}(\vf\theta):\vf\pi(\vf{X}_n,\vf{g}(\vf\theta))\in B\}\subseteq \vf{g}(\vf\Theta)$$
    is a confidence region with confidence level $\gamma$.
  \end{proposition}
  \subsubsection{Confidence intervals for the relative frequency}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_n\sim\text{Ber}(p)\ \text{i.i.d.}:p\in(0,1)\})$ be a parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_n)$ and $\alpha\in[0,1]$. Let $\hat{p}=\overline{x}_n$. Then, an asymptotic confidence interval for $p$ of confidence level $1-\alpha$ is:
    $$p\in\left(\hat{p}-z_{1-\frac{\alpha}{2}}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}},\hat{p}+z_{1-\frac{\alpha}{2}}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\right)$$
  \end{proposition}
  \subsubsection{Confidence intervals for \texorpdfstring{$N(\mu,\sigma^2)$}{N(mu,sigma2)}}
  \begin{proposition}[Interval for $\mu$ with $\sigma$ known]
    Let $\sigma\in\RR_{\geq 0}$ be a known parameter, $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_n\sim N(\mu,\sigma^2)\text{ i.i.d.}:\mu\in\RR\})$ be a parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_n)$ and $\alpha\in[0,1]$. Then, a confidence interval for $\mu$ of confidence level $1-\alpha$ is:
    $$\mu\in\left(\overline{x}_n-z_{1-\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}},\overline{x}_n+z_{1-\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}\right)$$
  \end{proposition}
  \begin{proposition}[Intervals for $\mu$ and $\sigma^2$]
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_n\sim N(\mu,\sigma^2)\text{ i.i.d.}:(\mu,\sigma^2)\in\RR\times\RR_{\geq 0}\})$ be a parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_n)$ and $\alpha\in[0,1]$. Then, a confidence interval for $\mu$ of confidence level $1-\alpha$ is:
    $$\mu\in\left(\overline{x}_n-t_{n-1;1-\frac{\alpha}{2}}\frac{\tilde{s}_n}{\sqrt{n}},\overline{x}_n+t_{n-1;1-\frac{\alpha}{2}}\frac{\tilde{s}_n}{\sqrt{n}}\right)$$
    A confidence interval for $\sigma^2$ of confidence level $1-\alpha$ is:
    $$\sigma^2\in\left(\frac{(n-1)\tilde{s}_n{}^2}{{\chi_{n;1-\frac{\alpha}{2}}}},\frac{(n-1)\tilde{s}_n{}^2}{{\chi_{n;\frac{\alpha}{2}}}}\right)$$
  \end{proposition}
  \subsubsection{Confidence intervals for two-samples problems}
  \begin{proposition}[Independent samples with known variances]
    Let $\sigma_x,\sigma_y\in\RR_{\geq 0}$ be known parameters, $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_{n_x}\sim N(\mu_x,{\sigma_x}^2)\text{ i.i.d.},Y_1,\ldots,Y_{n_y}\sim N(\mu_y,{\sigma_y}^2)\text{ i.i.d.}:(\mu_x,\mu_y,)\in\RR^2\})$ be a parametric statistical model such that each $X_i$ is independent of $Y_j$ $\forall (i,j)\in\{1,\ldots,n_x\}\times\{1,\ldots,n_y\}$, $\vf{x}_{n_x}\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_{n_x})$, $\vf{y}_{n_y}\in\mathcal{X}$ be a realization of $(Y_1,\ldots,Y_{n_y})$ and $\alpha\in[0,1]$. Then, an asymptotic confidence interval for $\mu_x-\mu_y$ of confidence level $1-\alpha$ is:
    $$\mu_x-\mu_y\in\left(\overline{x}_{n_x}-\overline{y}_{n_y}-z_{1-\frac{\alpha}{2}}s,\overline{x}_{n_x}-\overline{y}_{n_y}+z_{1-\frac{\alpha}{2}}s\right)$$ where $s=\sqrt{\frac{{\sigma_x}^2}{n_x}+\frac{{\sigma_y}^2}{n_y}}$.
  \end{proposition}
  \begin{proposition}[Independent samples with unknown equal variances]
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_{n_x}\sim N(\mu_x,\sigma^2)\text{ i.i.d.},Y_1,\ldots,Y_{n_y}\sim N(\mu_y,\sigma^2)\text{ i.i.d.}:(\mu_x,\mu_y,)\in\RR^2\times{\RR_{\geq 0}}^2\})$ be a parametric statistical model such that each $X_i$ is independent of $Y_j$ $\forall (i,j)\in\{1,\ldots,n_x\}\times\{1,\ldots,n_y\}$, $(x_1,\ldots,x_n)\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_{n})$, $(y_1,\ldots,y_n)\in\mathcal{X}$ be a realization of $(Y_1,\ldots,Y_{n})$ and $\alpha\in[0,1]$. Let $\tilde{s}_{n_x}{}^2:=\frac{1}{n_x-1}\sum_{i=1}^n{(x_i-\overline{x})}^2$ and $\tilde{s}_{n_y}{}^2:=\frac{1}{n_y-1}\sum_{i=1}^n{(y_i-\overline{y})}^2$. Then, an asymptotic confidence interval for $\mu_x-\mu_y$ of confidence level $1-\alpha$ is:
    \begin{multline*}
      \mu_x-\mu_y\in\left(\overline{x}_{n_x}-\overline{y}_{n_y}-t_{\nu;1-\frac{\alpha}{2}}s_p\sqrt{\frac{1}{n_x}+\frac{1}{n_y}},\right.\\\left.\overline{x}_{n_x}-\overline{y}_{n_y}+t_{\nu;1-\frac{\alpha}{2}}s_p\sqrt{\frac{1}{n_x}+\frac{1}{n_y}}\right)
    \end{multline*}
    where ${s_p}^2=\frac{(n_x-1)\tilde{s}_{n_x}{}^2+(n_y-1)\tilde{s}_{n_y}{}^2}{n_x+n_y-2}$ and $\nu=n_x+n_y-2$.
  \end{proposition}
  \begin{proposition}[Independent samples with unknown variances]
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_{n_x}\sim N(\mu_x,{\sigma_x}^2)\text{ i.i.d.},Y_1,\ldots,Y_{n_y}\sim N(\mu_y,{\sigma_y}^2)\text{ i.i.d.}:(\mu_x,\mu_y,)\in\RR^2\times{\RR_{\geq 0}}^2\})$ be a parametric statistical model such that each $X_i$ is independent of $Y_j$ $\forall (i,j)\in\{1,\ldots,n_x\}\times\{1,\ldots,n_y\}$, $\vf{x}_{n_x}\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_{n_x})$, $\vf{y}_{n_y}\in\mathcal{X}$ be a realization of $(Y_1,\ldots,Y_{n_y})$ and $\alpha\in[0,1]$. Let $\tilde{s}_{n_x}{}^2:=\frac{1}{n_x-1}\sum_{i=1}^n{(x_i-\overline{x})}^2$ and $\tilde{s}_{n_y}{}^2:=\frac{1}{n_y-1}\sum_{i=1}^n{(y_i-\overline{y})}^2$. Then, an asymptotic confidence interval for $\mu_x-\mu_y$ of confidence level $1-\alpha$ is:
    \begin{multline*}
      \mu_x-\mu_y\in\left(\overline{x}_{n_x}-\overline{y}_{n_y}-t_{\nu;1-\frac{\alpha}{2}}\sqrt{\frac{\tilde{s}_{n_x}{}^2}{n_x}+\frac{\tilde{s}_{n_y}{}^2}{n_y}},\right.\\\left.\overline{x}_{n_x}-\overline{y}_{n_y}+t_{\nu;1-\frac{\alpha}{2}}\sqrt{\frac{\tilde{s}_{n_x}{}^2}{n_x}+\frac{\tilde{s}_{n_y}{}^2}{n_y}}\right)
    \end{multline*}
    where $$\nu=\frac{\displaystyle{\left(\frac{\tilde{s}_{n_x}{}^2}{n_x}+\frac{\tilde{s}_{n_y}{}^2}{n_y}\right)}^2}{\displaystyle\frac{{\left(\frac{\tilde{s}_{n_x}{}^2}{n_x}\right)}^2}{n_x-1}+\frac{{\left(\frac{\tilde{s}_{n_y}{}^2}{n_y}\right)}^2}{n_y-1}}$$
  \end{proposition}
  \begin{proposition}[Related samples with unknown variances]
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_{n}\sim N(\mu_x,{\sigma_x}^2)\text{ i.i.d.},Y_1,\ldots,Y_{n}\sim N(\mu_y,{\sigma_y}^2)\text{ i.i.d.}:(\mu_x,{\sigma_x}^2,\mu_y,{\sigma_y}^2)\in\RR^2\times{\RR_{\geq 0}}^2\})$ be a parametric statistical model such that each $W_i:=X_i-Y_i\sim N(\mu_x-\mu_y,{\sigma_x}^2-{\sigma_y}^2)$ are i.i.d., $(x_1,\ldots,x_n)\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_{n})$, $(y_1,\ldots,y_n)\in\mathcal{X}$ be a realization of $(Y_1,\ldots,Y_{n})$ and $\alpha\in[0,1]$. Then, we can proceed as if we only had the sample $(W_1,\ldots,W_n)$. In particular, a confidence interval for $\mu_x-\mu_y$ of confidence level $1-\alpha$ is:
    \begin{multline*}
      \mu_x-\mu_y\in\left(\overline{x}_{n}-\overline{y}_{n}-t_{n-1;1-\frac{\alpha}{2}}\frac{\hat{s}_n}{\sqrt{n}},\right.\\\left.\overline{x}_{n}-\overline{y}_{n}+t_{n-1;1-\frac{\alpha}{2}}\frac{\hat{s}_n}{\sqrt{n}}\right)
    \end{multline*}
    where ${\hat{s}_n}^2=\frac{1}{n-1}\sum_{i=1}^n{(x_i-y_i-(\overline{x}-\overline{y}))}^2$.
  \end{proposition}
  \subsection{Hypothesis testing}
  \subsubsection{Hypothesis test}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $\Theta_0,\Theta_1\subset\Theta$ be disjoint subsets. Our goal is to know whether $\theta\in\Theta_0$ or $\theta\in\Theta_1$ (even if it isn't neither of them) and we will use a sample $\vf{x}_n\in\mathcal{X}$ to conclude our objective. We define the following two propositions which we will call \emph{hypothesis}:
    $$\mathcal{H}_0:\theta\in\Theta_0\qquad\mathcal{H}_1:\theta\in\Theta_1$$
    $\mathcal{H}_0$ is called \emph{null hypothesis} and $\mathcal{H}_1$ is called \emph{alternative hypothesis}. We say that the hypothesis $\mathcal{H}_i$ is \emph{simple} if $\Theta_i=\{\theta_0\}$ for some $\theta_0\in\Theta$. Otherwise we say that the hypothesis $\mathcal{H}_i$ is compound.
  \end{definition}
  \begin{definition}[Hypothesis test]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. A \emph{hypothesis test} is a function $$\function{\delta}{\mathcal{X}}{\{\mathcal{H}_0,\mathcal{H}_1\}}{\vf{x}_n}{\delta(\vf{x}_n)}$$
    The set $A_0:=\delta^{-1}(\mathcal{H}_0)\subseteq \mathcal{X}$, which is the set of samples that will lead us to accept\footnote{Some authors prefer to say that they don't reject $\mathcal{H}_0$ instead of saying that they accept $\mathcal{H}_0$.} $\mathcal{H}_0$, is called \emph{acceptation region}. The set $A_1:=\delta^{-1}(\mathcal{H}_1)\subseteq \mathcal{X}$, which is the set of samples that will lead us to accept $\mathcal{H}_1$ (and therefore reject $\mathcal{H}_0$), is called \emph{critical region}\footnote{In order to denote these concepts more compactly, we will write $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ to denote the hypothesis test whose acceptation and critical regions are $A_0$ and $A_1$, respectively.}.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test. An \emph{error of type I} is the rejection of $\mathcal{H}_0$ when it is true. An \emph{error of type II} is the acceptation of $\mathcal{H}_0$ when it is false. We define the probabilities $\alpha$ and $\beta$ as:
    \begin{align*}
      \alpha & :=\Prob(\text{Error of type I})=\Prob(\text{Reject $\mathcal{H}_0$}\mid\text{$\mathcal{H}_0$ is true})   \\
      \beta  & :=\Prob(\text{Error of type II})=\Prob(\text{Accept $\mathcal{H}_0$}\mid\text{$\mathcal{H}_0$ is false})
    \end{align*}
    More precisely, if $\vf{x}_n\in\mathcal{X}$ is a realization of $\vf{X}_n$, then: $$\alpha:=\sup\{\Prob(\vf{x}_n\in A_1\mid \theta):\theta\in\Theta_0\}$$
    The value $1-\beta$ is called \emph{power} of the test and the value $\alpha$, \emph{size} of the test. Moreover, we say that the test has \emph{significance level} $\alpha\in[0,1]$ if its size is less than or equal to $\alpha$. In many cases the size of the test and the significance level are equal, hence the use of the same letter\footnote{In particular, for simple hypothesis they are the same thing.}\footnote{In practice, we fix a significance level $\alpha\in[0,1]$ small enough ($\approx 0.05$ but may vary depending on the problem) with which we accept making mistakes and from here we try to minimize the $\beta$ (or maximize the power $1-\beta$). Moreover, having fixed $\alpha$, we obtain a confidence level $1-\alpha$. And if we impose $\Prob(\vf{x}_n\in A_0\mid \mathcal{H}_0)=1-\alpha$, we are able to determine $A_0$.}.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model, $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test and $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$. We define the \emph{power function} as: $$\Pi(\theta)=\Prob(\text{Reject $\mathcal{H}_0$})=\Prob(\vf{x}_n\in A_1)$$
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$ and $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test. Then:
    $$
      \Pi(\theta)=
      \begin{cases}
        \alpha  & \text{if }\theta\in\Theta_0 \\
        1-\beta & \text{if }\theta\in\Theta_1
      \end{cases}
    $$
  \end{proposition}
  \subsubsection{Test statistic and \texorpdfstring{$p$}{p}-value}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$ and $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test. A statistic $T$ used to decide whether or not reject the null hypothesis is called a \emph{test statistic}.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$, $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test such that $\Theta_0=\{\theta_0\}$, and $T$ be a test statistic. Suppose that we have observed the value $t:=T(\vf{x}_n)$. We define the \emph{$p$-value} as the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$, $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test such that $\Theta_0=\{\theta_0\}$. We say that the test is a
    \begin{itemize}
      \item \emph{one-sided right tail test} if $\Theta_1=(\theta_0,\infty)$.
      \item \emph{one-sided left tail test} if $\Theta_1=(-\infty,\theta_0)$.
      \item \emph{two-sided test} if $\Theta_1=\RR\setminus\{\theta_0\}$.
    \end{itemize}
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$, $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test such that $\Theta_0=\{\theta_0\}$, and $T$ be a test statistic. Suppose that we have observed the value $t:=T(\vf{x}_n)$ and let $p$ be the $p$-value of the test. Then:
    \begin{enumerate}
      \item One-sided right tail test: $$p=\Prob(T\geq t\mid \mathcal{H}_0)$$
      \item One-sided left tail test: $$p=\Prob(T\leq t\mid \mathcal{H}_0)$$
      \item Two-sided test: $$p=2\min\{\Prob(T\geq t\mid \mathcal{H}_0),\Prob(T\leq t\mid \mathcal{H}_0)\}\footnote{If the statistic $T$ is symmetric with respect to the origin, then $p=\Prob(|T|\geq |t|\mid \mathcal{H}_0)$.}$$
    \end{enumerate}
    And given a significance level $\alpha\in(0,1)$ we will reject $\mathcal{H}_0$ if $p<\alpha$ and accept $\mathcal{H}_0$ if $p\geq\alpha$.
  \end{proposition}
  \begin{center}
    \begin{minipage}{\linewidth}
      \centering
      \includestandalone[mode=image|tex,width=\linewidth]{Images/p-value}
      \captionof{figure}{Probability density function of a test statistic (assuming the null hypothesis) together with an observed value and the $p$-value of the one-sided right tail test.}
    \end{minipage}
  \end{center}
  \subsubsection{Neymann-Pearson test}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We say that a test $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ of significance level $\alpha$ is a \emph{uniformly most powerful} (\emph{UMP}) \emph{test} if it has the greatest power among all the tests with significance level $\alpha$.
  \end{definition}
  \begin{lemma}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We say that a test $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ of significance level $\alpha$. If $A_1$ does not depend on the parameter $\theta\in\Theta_1$, then $\delta$ is a UMP test.
  \end{lemma}
  \begin{definition}[Neymann-Pearson test]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$ and $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test such that $\Theta_0=\{\theta_0\}$ and $\Theta_1=\{\theta_1\}$. We say that $\delta$ is a \emph{Neyman-Pearson test} of significance level $\alpha\in[0,1]$ if $\exists C>0$ such that:
    \begin{gather*}
      \left\{\vf{x}_n\in\mathcal{X}:\frac{L(\theta_0;\vf{x}_n)}{L(\theta_1;\vf{x}_n)}> C\right\}= A_0\\
      \left\{\vf{x}_n\in\mathcal{X}:\frac{L(\theta_0;\vf{x}_n)}{L(\theta_1;\vf{x}_n)}\leq C\right\}= A_1
    \end{gather*}
    and $\Prob(\vf{x}_n\in A_1\mid \mathcal{H}_0)=\alpha$.
  \end{definition}
  \begin{lemma}[Neyman-Pearson lemma]
    Any Neyman-Pearson test is a UMP test.
  \end{lemma}
  \begin{theorem}
    Any UMP test is a Neyman-Pearson test.
  \end{theorem}
  \subsubsection{Likelihood-ratio test}
  \begin{definition}[Likelihood-ratio test]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:{\vf\theta}\in\Theta\subseteq\RR^d\})$ be a statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$, $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test of compound hypothesis $\mathcal{H}_0:{\vf\theta}\in\Theta_0$ and $\mathcal{H}_1:{\vf\theta}\in\Theta_1$.
    Then, the \emph{likelihood ratio test} (\emph{LRT}) is given by the critical region:
    $$\left\{\vf{x}_n\in\mathcal{X}:\frac{\sup\{L({\vf\theta};\vf{x}_n):{\vf\theta}\in\Theta_0\}}{\sup\{L({\vf\theta};\vf{x}_n):{\vf\theta}\in\Theta\}}\leq C\right\}= A_1$$
    for some constant $C>0$\footnote{Note that $L({\vf{\hat\theta}},\vf{x}_n)=\sup\{L({\vf\theta};\vf{x}_n):{\vf\theta}\in\Theta\}$, where ${\vf{\hat\theta}}$ is the MLE. Similarly $L({\vf{\hat\theta}}_0,\vf{x}_n)=\sup\{L({\vf\theta};\vf{x}_n):{\vf\theta}\in\Theta\}$, where ${\vf{\hat\theta}}_0$ is the MLE restricted to $\Theta_0$.}.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:{\vf\theta}\in\Theta\subseteq\RR^d\})$ be a statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$, $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test of simple hypothesis $\mathcal{H}_0$ and $\mathcal{H}_1$. Then, the LRT is a Neyman-Pearson test.
  \end{proposition}
  \begin{theorem}[Asymptotic behaviour of the LRT]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:{\vf\theta}\in\Theta\subseteq\RR^d\})$ be a parametric regular statistical model and consider the test $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ of compound hypothesis $\mathcal{H}_0:{\vf\theta}\in\Theta_0$ and $\mathcal{H}_1:{\vf\theta}\in\Theta_1$. Let $$\Lambda(\vf{x}_n):=-2\ln \frac{\sup\{L({\vf\theta};\vf{x}_n):{\vf\theta}\in\Theta_0\}}{\sup\{L({\vf\theta};\vf{x}_n):{\vf\theta}\in\Theta\}}$$ which is called \emph{LRT test statistic}. Then if the model is regular, we have:
    $$\Lambda(\vf{x}_n)\overset{\text{d}}{\longrightarrow}{\chi_r}^2$$ where $r=\dim\Theta-\dim\Theta_0$.
  \end{theorem}
  \begin{definition}[Goodness of fit]
    Suppose we have a random variable $X$ whose outcomes are $x_1,\ldots,x_n$ and that we classify these outcomes in $k$ classes. Thus, we obtain a table of the form:
    \begin{center}
      \begin{minipage}{\linewidth}
        \centering
        \begin{tabular}{l||ccccc|c}
          Class     & $a_1$   & $\cdots$ & $a_j$   & $\cdots$ & $a_k$   & $\mathbf{Total}$ \\
          \hline
          Frequency & $n_{1}$ & $\cdots$ & $n_{j}$ & $\cdots$ & $n_{k}$ & $n$              \\
        \end{tabular}
      \end{minipage}
    \end{center}
    We want a test for:
    $$
      \begin{cases}
        \mathcal{H}_0: & X\sim f_{\vf\theta}                     \\
        \mathcal{H}_1: & P(X\in a_i)=\frac{n_i}{n}\quad\forall i
      \end{cases}
    $$
    If $\pi_i$ denotes the probability of being in the cell $i$ under $\mathcal{H}_0$, we can approximate $\pi_i$ by $\hat\pi_i=\Prob(X\in a_i\mid {\vf\theta}={\vf{\hat\theta}})$, where ${\vf{\hat\theta}}$ is the MLE of ${\vf\theta}$, for $i=1,\ldots,k-1$ and let $\hat\pi_k:=1-\sum_{i=1}^{k-1}\hat\pi_i$. Hence since the distribution of the data in the table follows a multinomial distribution, we have\footnote{In order to have the expected asymptotic behaviour we need to check that the expectations of each $\hat\pi_i$ are greater than or equal to 5 (heuristic criterion), i.e. $n_i\hat\pi_i\geq 5$ $\forall i$. If this is not the case, we should reduce the number of classes by groupping some of them together.}:
    $$\Lambda=2\sum_{i=1}^kn_i\log\left(\frac{n_i}{n\hat\pi_i}\right)\overset{\text{d}}{\longrightarrow}{\chi_{k-2}}^2$$
  \end{definition}
  \begin{definition}[Test of homogenity]
    Consider $r$ \iid random variables $X_1,\ldots,X_r$ whose outcomes can be classified in the classes $a_1,\ldots,a_s$ each with probability $\Prob(X_i=a_j)=p_{ij}$ $\forall i,j$. Suppose we have $n_{ij}$ observations of the variable $X_i$ taking the value $a_j$ and denote $n_{i\cdot}:=\sum_{j=1}^sn_{ij}$, $n_{\cdot j}:=\sum_{i=1}^rn_{ij}$ and $n:=\sum_{i=1}^r\sum_{j=1}^sn_{ij}$. That is, we have the following table:
    \begin{center}
      \begin{minipage}{\linewidth}
        \centering
        \begin{tabular}{c||ccccc|c}
                           & $a_1$         & $\cdots$ & $a_j$         & $\cdots$ & $a_s$         & $\mathbf{Total}$ \\
          \hline\hline
          $X_1$            & $n_{11}$      & $\cdots$ & $n_{1j}$      & $\cdots$ & $n_{1s}$      & $n_{1\cdot}$     \\
          $\vdots$         & $\vdots$      & $\ddots$ & $\vdots$      & $\ddots$ & $\vdots$      & $\vdots$         \\
          $X_i$            & $n_{i1}$      & $\cdots$ & $n_{ij}$      & $\cdots$ & $n_{is}$      & $n_{i\cdot}$     \\
          $\vdots$         & $\vdots$      & $\ddots$ & $\vdots$      & $\ddots$ & $\vdots$      & $\vdots$         \\
          $X_r$            & $n_{r1}$      & $\cdots$ & $n_{rj}$      & $\cdots$ & $n_{rs}$      & $n_{r\cdot}$     \\
          \hline
          $\mathbf{Total}$ & $n_{\cdot 1}$ & $\cdots$ & $n_{\cdot j}$ & $\cdots$ & $n_{\cdot s}$ & $n$
        \end{tabular}
        \captionof{table}{}
        \label{S:table}
      \end{minipage}
    \end{center}
    We want a test for:
    $$
      \begin{cases}
        \mathcal{H}_0: & p_j:=p_{1j}=\cdots=p_{rj}\ \forall j \\
        \mathcal{H}_1: & \text{otherwise}
      \end{cases}
    $$
    Again, the distribution of the data in the table follows a multinomial distribution, so under $\mathcal{H}_0$ we get the following MLEs (with the constraint that $\sum_{j=1}^sp_{j}=1$):
    $$\hat{p}_j=\frac{n_{\cdot j}}{n}\qquad\forall j$$
    And in general, using the constraint $\sum_{i=1}^r\sum_{j=1}^sp_{ij}=1$, we have: $$\hat{p}_{ij}=\frac{n_{ij}}{n}\qquad\forall i,j$$
    Finally we have: $$\Lambda=2\sum_{i=1}^r\sum_{j=1}^sn_{ij}\log\left(\frac{n_{ij}n}{n_{i\cdot}n_{\cdot j}}\right)\overset{\text{d}}{\longrightarrow}{\chi_{(r-1)(s-1)}}^2$$
  \end{definition}
  \begin{definition}[Test of independence]
    Consider $r$ \iid random variables $X_1,\ldots,X_r$ whose outcomes can be classified in the classes $a_1,\ldots,a_s$ each with probability $\Prob(X_i=a_j)=p_{ij}$ $\forall i,j$. Suppose we have $n_{ij}$ observations of the variable $X_i$ taking the value $a_j$ and denote $n_{i\cdot}:=\sum_{j=1}^sn_{ij}$, $n_{\cdot j}:=\sum_{i=1}^rn_{ij}$ and $n:=\sum_{i=1}^r\sum_{j=1}^sn_{ij}$. That is, we have again the \mcref{S:table}.
    We want a test for:
    $$
      \begin{cases}
        \mathcal{H}_0: & p_{ij}=\theta_i\phi_j\ \forall i,j \\
        \mathcal{H}_1: & \text{otherwise}
      \end{cases}
    $$
    Again, the distribution of the data in the table follows a multinomial distribution, so under $\mathcal{H}_0$ we get the following MLEs for $\theta_i$ and $\phi_j$ (with the constraints that $\sum_{i=1}^r\theta_i=\sum_{j=1}^s\phi_j=1$):
    $$\hat\theta_i=\frac{n_{i\cdot}}{n}\quad\hat\phi_j=\frac{n_{\cdot j}}{n}\qquad\forall i,j$$
    And in general, using the constraint $\sum_{i=1}^r\sum_{j=1}^sp_{ij}=1$, we have: $$\hat{p}_{ij}=\frac{n_{ij}}{n}\qquad\forall i,j$$
    Finally we have: $$\Lambda=2\sum_{i=1}^r\sum_{j=1}^sn_{ij}\log\left(\frac{n_{ij}n}{n_{i\cdot}n_{\cdot j}}\right)\overset{\text{d}}{\longrightarrow}{\chi_{(r-1)(s-1)}}^2$$
  \end{definition}
  \subsubsection{\texorpdfstring{$t$}{t}-test}
  \begin{definition}[$t$-test]
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_n\sim N(\mu,\sigma^2)\ \text{\iid} : \mu\in\RR\})$ be a statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_n)$. The \emph{$t$-test} is the test $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$, where $\mathcal{H}_0:\mu=\mu_0$ for some $\mu_0\in\RR$ and $\mathcal{H}_1$ can be either of $\{\mu>\mu_0,\mu<\mu_0,\mu\ne\mu_0\}$. In this case the test statistic that is taken is: $$\frac{\overline{X}_n-\mu}{\frac{\tilde{s}_n}{\sqrt{n}}}\sim t_{n-1}$$
  \end{definition}
  \subsubsection{Wald and score tests}
  \begin{definition}[Wald test]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a parametric regular statistical model and consider the test $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ of simple hypothesis $\mathcal{H}_0:\vf\theta=\vf\theta_0$ and $\mathcal{H}_1:\vf\theta\ne\vf\theta_0$. The \emph{Wald test} is the test whose statistic is: $$\transpose{(\vf{\hat\theta}-\vf\theta_0)}\vf{I}(\vf{\hat\theta})(\vf{\hat\theta}-\vf\theta_0)\overset{\text{a}}{\sim}{\chi_d}^2$$ where $\vf{\hat\theta}$ is the MLE of $\vf\theta$ and $d=\dim\Theta$. If $\mathcal{H}_0:\vf\theta\in\Theta_0$, we shall replace $\vf\theta_0$ by the MLE under $\mathcal{H}_0$, $\vf{\hat\theta_0}$, in the test statistic. For the 1-dimensional case, we have: $$I(\hat\theta){(\hat\theta-\theta_0)}^2\overset{\text{a}}{\sim}{\chi_1}^2$$
  \end{definition}
  \begin{corollary}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a parametric regular statistical model and consider the test $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ of simple hypothesis $\mathcal{H}_0:\vf{R}\vf\theta=\vf{r}$ and $\mathcal{H}_1:\vf{R}\vf\theta\ne\vf{r}$, where $\vf{R}\in\mathcal{M}_{k\times d}(\RR)$, $\vf\theta\in\RR^d$ and $\vf{r}\in\RR^k$. The test statistic of Wald test is: $$\transpose{(\vf{R\hat\theta}-\vf{r})}{\left[\vf{R}{\vf{I}(\vf{\hat\theta})}^{-1}\transpose{\vf{R}}\right]}^{-1}(\vf{R\hat\theta}-\vf{r})\overset{\text{a}}{\sim}{\chi_k}^2$$ where $\vf{\hat\theta}$ is the MLE of $\vf\theta$. The matrix $\vf{R}$ is called \emph{contrast matrix}.
  \end{corollary}
  \begin{definition}[Score test]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a parametric regular statistical model and consider the test $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ of simple hypothesis $\mathcal{H}_0:\vf\theta=\vf\theta_0$ and $\mathcal{H}_1:\vf\theta\ne\vf\theta_0$. The \emph{score test} is the test whose statistic is: $$\transpose{\vf{S}(\vf\theta_0)}\vf{I}^{-1}(\vf{\theta}_0)\vf{S}(\vf\theta_0)\overset{\text{a}}{\sim}{\chi_d}^2$$ where $d=\dim\Theta$. If $\mathcal{H}_0:\vf\theta\in\Theta_0$, we shall replace $\vf\theta_0$ by the MLE under $\mathcal{H}_0$, $\vf{\hat\theta_0}$, in the test statistic. For the 1-dimensional case, we have: $$\frac{{S(\theta_0)}^2}{I(\theta_0)}\overset{\text{a}}{\sim}{\chi_1}^2$$
  \end{definition}
  \subsection{Bootstrapping}
  \subsubsection{Parametric and non-parametric bootstrap}
  \begin{definition}[Non-parametric bootstrap]
    Consider a sample $x_1,\ldots,x_n$ from \iid random variables $X_1,\ldots,X_n\sim F$ of an unknown distribution $F$. Our goal is to get an estimator of a parameter $\theta=T(X_1,\ldots,X_n)$. To do so, we define the \emph{empirical distribution} $F_n$ as follows: if $X\sim F_n$ then: $$\Prob(X=x_i)=\frac{1}{n}\quad\forall i\in\{1,\ldots,n\}$$ First we compute ${\hat\theta}:=T(x_1,\ldots,x_n)$ from the data given.
    Now we generate $B$ samples of data $\{{x_1^b},\ldots,{x_n^b}\}$, $b=1,\ldots,B$, taken from the distribution $F_n$ (with replacement) and for each sample we compute the respective estimator ${\hat\theta}_b=T({x_1^b},\ldots,{x_n^b})$. This gives us the \emph{bootstrap distribution} of ${\hat\theta}$ and so the bias and variance of ${\hat\theta}$ is:
    $$\bias_B({\hat\theta})=\overline{\theta}_B-{\hat\theta}\quad\Var_B({\hat\theta})=\frac{1}{B-1}\sum_{b=1}^B{({\hat\theta}_b-\overline{\theta}_B)}^2$$
    where $\overline{\theta}_B:=\frac{1}{B}\sum_{b=1}^B{\hat\theta}_b$. And hence, the bootstrap estimate of the standard error is:
    $$\tilde{s}_B(\hat\theta)=\sqrt{\frac{\sum_{b=1}^B{({\hat\theta}_b-\overline{\theta}_B)}^2}{B-1}}$$
    If we want an unbiased estimator ${\hat\theta}$ of $\theta$ we can replace ${\hat\theta}$ by $2{\hat\theta}-\overline{\theta}_B$.
  \end{definition}
  \begin{definition}[Parametric bootstrap]
    Consider a sample $x_1,\ldots,x_n$ from \iid random variables $X_1,\ldots,X_n\sim F_{\theta}$ and we are interested in estimating $\theta=T(X_1,\ldots,X_n)$. First we compute ${\hat\theta}:=T(x_1,\ldots,x_n)$ from the data given.
    Now we generate $B$ samples of data $\{{x_1^b},\ldots,{x_n^b}\}$, $b=1,\ldots,B$, taken from the distribution $F_{{\hat\theta}}$ (with replacement) and for each sample we compute the respective estimator ${\hat\theta}_b=T({x_1^b},\ldots,{x_n^b})$. This gives us the \emph{parametric bootstrap distribution} of ${\hat\theta}$\footnote{Note that in order for bootstrapping to work we need some regular conditions of the function $T$ (Hadamard differentiability). Statistics like minimum or maximum doesn't satisfy these restrictions.}.
  \end{definition}
  \subsubsection{Bootstrap confidence intervals}
  \begin{definition}[Normal confidence interval]
    Consider a sample $x_1,\ldots,x_n$ from \iid random variables $X_1,\ldots,X_n\sim F$ of an unknown distribution $F$. The \emph{normal confidence interval} for $\theta=T(X_1,\ldots,X_n)$ of level $\alpha$ is: $$\theta\in(\hat\theta-z_{1-\alpha/2}\tilde{s}_B(\hat\theta),\hat\theta+z_{1-\alpha/2}\tilde{s}_B(\hat\theta))$$ To use a bias-corrected bootstrap estimator, replace ${\hat\theta}$ by $2{\hat\theta}-\overline{\theta}_B$.
  \end{definition}
  \begin{definition}[Basic bootstrap confidence interval]
    Consider a sample $x_1,\ldots,x_n$ from \iid random variables $X_1,\ldots,X_n\sim F$ of an unknown distribution $F$. The \emph{basic bootstrap confidence interval} for $\theta=T(X_1,\ldots,X_n)$ of level $\alpha$ is: $$\theta\in(2\hat\theta-\hat\theta_{1-\alpha/2},2\hat\theta-\hat\theta_{\alpha/2})$$ where $\hat\theta_\alpha$ is the sample quantiles of the bootstrap relicates.
  \end{definition}
  \begin{definition}[Bootstrap-t confidence interval]
    Consider a sample $x_1,\ldots,x_n$ from \iid random variables $X_1,\ldots,X_n\sim F$ of an unknown distribution $F$. The \emph{bootstrap-t confidence interval} for $\theta=T(X_1,\ldots,X_n)$ of level $\alpha$ is: $$\theta\in(\hat\theta-t_{1-\alpha/2}\tilde{s}_B(\hat\theta),\hat\theta+t_{1-\alpha/2}\tilde{s}_B(\hat\theta))$$ To use a bias-corrected bootstrap estimator, replace ${\hat\theta}$ by $2{\hat\theta}-\overline{\theta}_B$.
  \end{definition}
  \begin{definition}[Percentile confidence interval]
    Consider a sample $x_1,\ldots,x_n$ from \iid random variables $X_1,\ldots,X_n\sim F$ of an unknown distribution $F$. Once we have computed $B$ estimates of $\hat\theta$ we order them:
    $${\hat\theta}_{(1)}\leq\cdots\leq{\hat\theta}_{(B)}$$
    The \emph{percentile confidence interval} for $\theta=T(X_1,\ldots,X_n)$ of level $\alpha$ is: $$\theta\in(\hat\theta_{\alpha/2},\hat\theta_{1-\alpha/2})$$
  \end{definition}
  \subsection{Bayesian inference}
  \subsubsection{Prior and posterior distributions}
  \begin{definition}
    Let $\vf{X}$ be a random vector with pdf $f(\vf{x}\mid \theta)$. As always we would like to estimate the unknown parameter $\theta\in\Theta$. Bayesian approach to statistical inference treats the parameter $\theta$ as a random variable with an appropriate \emph{prior distribution} (or simply \emph{prior}) $f(\theta)$.
  \end{definition}
  \begin{definition}
    Let $\vf{X}$ be a random vector with pdf $f(\vf{x}\mid \theta)$, $f(\theta)$ be the prior of $\theta$ and $\vf{x}$ be a realization of $\vf{X}$. We define the \emph{posterior distribution} (or simply \emph{posterior}) of $\theta$ as the pdf $f(\theta\mid \vf{x})$ given by Bayes' theorem: $$f(\theta\mid\vf{x})=\frac{f(\vf{x}\mid\theta)f(\theta)}{f(\vf{x})}=\frac{f(\vf{x}\mid\theta)f(\theta)}{\int f(\vf{x}\mid\theta)f(\theta)\dd{\theta}}\footnote{For practical purposes it is sometimes sufficient to study only $f(\vf{x}\mid\theta)f(\theta)$, since $f(\theta\mid \vf{x})\propto f(\vf{x}\mid\theta)f(\theta)$.}$$
    If the prior and the posterior are of the same distribution type, prior and observation model are called \emph{conjugate}.
  \end{definition}
  \begin{definition}[Bayesian point estimates]
    Let $\vf{X}$ be a random vector with pdf $f(\vf{x}\mid \theta)$, $f(\theta)$ be the prior of $\theta\in\Theta$ and $\vf{x}$ be a realization of $\vf{X}$.
    \begin{itemize}
      \item The \emph{posterior mean} $\Exp(\theta\mid\vf{x})$ is: $$\Exp(\theta\mid\vf{x})=\int_\Theta\theta f(\theta\mid\vf{x})\dd{\theta}$$
      \item The \emph{posterior mode} $\Mod(\theta\mid\vf{x})$ is: $$\Mod(\theta\mid\vf{x})=\argmax\{f(\theta\mid\vf{x}) : \theta\in\Theta\}$$
      \item The \emph{posterior median} $\Med(\theta\mid\vf{x})$ is any number $a$ such that: $$\int_{-\infty}^af(\theta\mid\vf{x})\dd{\theta}\quad\text{and}\quad\int_a^{+\infty}f(\theta\mid\vf{x})\dd{\theta}$$
    \end{itemize}
  \end{definition}
  \subsubsection{Choice of the prior}
  \begin{definition}
    Let $\theta\in\Theta$ be the parameter of interest of our model. A prior distribution with pdf $f(\theta)$ is called \emph{flat} if $$f(\theta)\propto\const\qquad\theta\in\Theta$$
  \end{definition}
  \begin{definition}
    Let $\theta\in\Theta$ be the parameter of interest of our model. A prior distribution with pdf $f(\theta)\geq 0$ is called \emph{improper} if $$\int_\Theta f(\theta)\dd{\theta}=\infty\quad\text{or}\quad\sum_{\theta\in\Theta}f(\theta)=\infty$$ for continuous or discrete parameters $\theta$, respectively.
  \end{definition}
  \begin{definition}[Jeffrey's prior]
    Let $\vf{X}$ be a random vector with pdf $f(\vf{x}\mid \theta)$ where $\theta\in\Theta$ is the parameter of interest. \emph{Jeffrey's prior} is defined as: $$f(\theta)\propto\sqrt{I(\theta)}$$
    If $\vf\theta$ is a vector valued parameter, Jeffrey's prior is defined as: $$f(\vf\theta)\propto\sqrt{\det\vf{I}(\vf\theta)}$$
  \end{definition}
  \begin{proposition}
    Let $\vf{X}$ be a random vector with pdf $f(\vf{x}\mid \theta)$ where $\theta\in\Theta$ is the parameter of interest and $\eta=h(\theta)$ where $h$ is an injective function. If the prior $f_\theta(\theta)$ of $\theta$ is flat, then: $$f_\eta(\eta)\propto\abs{\dv{h^{-1}(\eta)}{\eta}}$$ where $f_\eta(\eta)$ is the prior of $\eta$. And so, $f_\eta(\eta)$ is flat if and only if $h$ is a linear transformation.
  \end{proposition}
  \begin{proposition}
    Let $\vf{X}$ be a random vector with pdf $f(\vf{x}\mid \theta)$ where $\theta\in\Theta$ is the parameter of interest and $\eta=h(\theta)$ where $h$ is an injective function. Suppose $f_\theta(\theta)$ is the prior of $\theta$ and $f_\eta(\eta)$ is the prior of $\eta$. If $f_\theta(\theta)\propto\sqrt{I(\theta)}$, then $f_\eta(\eta)\propto\sqrt{I(\eta)}$
  \end{proposition}
  \subsubsection{Properties of Bayesian point and interval estimates}
  \begin{definition}
    Let $\theta\in\Theta$ be the parameter of interest of our model. A \emph{loss function} $\ell(\hat\theta,\theta)\in\RR$ quantifies the loss encountered when estimating the true parameter $\theta$ by $\hat\theta$. Commonly used loss functions are the following ones:
    \begin{itemize}
      \item \emph{Quadratic loss function}: $\ell(\hat\theta,\theta)={(\hat\theta-\theta)}^2$
      \item \emph{Linear loss function}: $\ell(\hat\theta,\theta)=\abs{\hat\theta-\theta}$
      \item \emph{Zero-one loss function}: $$\ell_\varepsilon(\hat\theta,\theta)=
              \begin{cases}
                0 & \text{if }\hat\theta=\theta   \\
                1 & \text{if }\hat\theta\ne\theta
              \end{cases}$$
    \end{itemize}
  \end{definition}
  \begin{definition}
    Let $\vf{X}$ be a random vector with pdf $f(\vf{x}\mid \theta)$, $f(\theta)$ be the prior of $\theta\in\Theta$ and $\vf{x}$ be a realization of $\vf{X}$. A \emph{Bayes estimate} of $\theta$ with respect to a loss function $\ell(\hat\theta,\theta)$ minimizes the expected loss with respect to the posterior distribution $f(\theta\mid\vf{x})$, i.e. it minimizes: $$\Exp(\ell(\hat\theta,\theta)\mid\vf{x})=\int_\Theta\ell(\hat\theta,\theta)f(\theta\mid\vf{x})\dd{\theta}$$
  \end{definition}
  \begin{proposition}
    \hfill
    \begin{enumerate}
      \item The posterior mean is the Bayes estimate with respect to quadratic loss.
      \item The posterior median is the Bayes estimate with respect to linear loss.
      \item The posterior mode is the Bayes estimate with respect to zero-one loss.
    \end{enumerate}
  \end{proposition}
  \begin{theorem}
    Let $\vf{X}$ be a random vector (of length $n$) with pdf $f(\vf{x}\mid \theta)$ where $\theta\in\Theta$ is the parameter of interest which has a prior $f(\theta)$. Suppose the MLE of $\theta$ is $\hat\theta_n$. We define: $$m_0:=\argmax\{f(\theta):\theta\in\Theta\},\quad J_0=-\pdv[2]{(\log f(\theta))}{\theta}\bigg|_{\theta=m_0}$$ Then: $$\theta\mid\vf{X}\overset{\text{a}}{\sim} N(m_n,{J_n}^{-1})$$
    where: $$J_n=J_0+J(\hat\theta_n)\quad\text{and}\quad m_n=\frac{J_0m_0+J(\hat\theta_n)\hat\theta_n}{J_n}$$
    If $n$ is large enough, we will have: $$\theta\mid\vf{X}\overset{\text{a}}{\sim} N\left(\hat\theta_n,{I(\hat\theta_n)}^{-1}\right)$$
  \end{theorem}
  \begin{definition}
    Let $\vf{X}$ be a random vector with pdf $f(\vf{x}\mid \theta)$ where $\theta\in\Theta$ is the parameter of interest. A subset $C\subseteq \Theta$ is called a \emph{credible region} for $\theta$ of confidence $\gamma$ if: $$\int_Cf(\theta\mid\vf{x})\dd{\theta}=\gamma$$ If $C$ is a real interval, $C$ is also called \emph{credible interval}. If the parameter is discrete, we will define a credible region of confidence $\gamma$ as: $$\sum_{\theta\in C\cap \Theta}f(\theta\mid\vf{x})\geq \gamma$$
  \end{definition}
  \begin{definition}
    Let $\vf{X}$ be a random vector with pdf $f(\vf{x}\mid \theta)$ where $\theta\in\Theta$ is the parameter of interest. A $\gamma$ credible region $C$ is called a \emph{highest posterior density} (HPC) \emph{region} if $f(\theta\mid\vf{x})\geq f(\tilde{\theta}\mid \vf{x})$ $\forall \theta\in C$ and all $\tilde{\theta}\notin C$.
  \end{definition}
  \begin{proposition}
    Let $\vf{X}$ be a random vector with pdf $f(\vf{x}\mid \theta)$ where $\theta\in\Theta$ is the parameter of interest whose posterior is $f(\theta\mid\vf{x})$. Then, among all $\gamma$ credible region $C$, the HPC minimizes the expected loss for the function $\ell(C,\theta)=\abs{C}-\indi{C}(\theta)$, where $\abs{C}$ is the size of the region.
  \end{proposition}
  % \subsubsection{Numerical methods for Bayesian inference}
  % \begin{definition}[Laplace approximation]

  % \end{definition}
  \subsection{Analyzing data}
  \subsubsection{Comparizing distributions}
  \begin{definition}[Q-Q plots]
    Consider a sample $x_1,\ldots,x_n$ of data and the ordered sample $x_{(1)},\ldots,x_{(n)}$. We would like to know whether the data come from a distribution $F$ or not. To do conclude something, we define: $$y_{(k)}=y_k:=F^{-1}\left(\frac{k}{n+1}\right)\quad k=1,\ldots,n$$
    Then, we plot the pairs $(y_{(k)},x_{(k)})$ (or equivalently the pairs $(F(y_{(k)}),F(x_{(k)}))$). The more similar is the plot to a line, the more possible is for the data to come from the distribution $F$. This plot is called \emph{Quantile-Quantile plot} (or \emph{Q-Q plots}), $y_{(k)}$ are called the \emph{theoretical quantiles} and $x_{(k)}$ the \emph{sample quantiles}, that is, the quantile function of the discrete distribution induced by the sample (assigning probability $1/n$ to each data of the sample)\footnote{Sometimes the theoretical quantiles taken are $F^{-1}\left(\frac{k-0.5}{n}\right)$ instead of $F^{-1}\left(\frac{k}{n+1}\right)$.}.
  \end{definition}
  \begin{proposition}[Normal Q-Q plots]
    Consider a sample $x_1,\ldots,x_n$ of data and the ordered sample $x_{(1)},\ldots,x_{(n)}$. We would like to know whether the data come from a normal distribution $N(\mu,\sigma^2)$ or not. We define $$z_{(k)}=z_{\frac{k}{n+1}}=\Phi^{-1}\left(\frac{k}{n+1}\right)\quad k=1,\ldots,n$$
    If the data are reasonably normal, the Q-Q plot of the pairs $(z_{(k)},x_{(k)})$ is approximately a line of slope $\sigma$ and $y$-intercept $\mu$.
  \end{proposition}
  \begin{center}
    \begin{minipage}{\linewidth}
      \centering
      \includestandalone[mode=image|tex,width=\linewidth]{Images/qq-plot}
      \captionof{figure}{Two normal Q-Q plots of samples of size 100. On the right-hand side the sample comes from an exponential distribution and so we can see that the data doesn't fit quite well in a line. On the left-hand side the data come from a normal distribution and so the data is approximately fitted in a line whose slope is $2.369\approx \sqrt{4}$ and whose $y$-intersect is $2.956\approx 3$.}
    \end{minipage}
  \end{center}
\end{multicols}
\end{document}