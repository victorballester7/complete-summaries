\documentclass[../../../main.tex]{subfiles}
% break in parametric statistical model

\begin{document}
\renewcommand{\col}{\sta}
\begin{multicols}{2}[\section{Statistics}]
  \subsection{Point estimation}
  \subsubsection{Statistical models}
  \begin{definition}
    Let $(\Omega,\mathcal{A},\Prob)$ be a probability space\footnote{From now on we will assume that the random variables are defined always in the same probability space $(\Omega,\mathcal{A},\Prob)$, so we will omit to say that.}, $\Theta$ be a set, $n\in\NN$ and $x_1,\ldots,x_n$ be a collection of data that we may assume that they are the outcomes of a random vector $\vf{X}_n=(X_1,\ldots,X_n)$ defined on $(\Omega,\mathcal{A},\Prob)$. Suppose, moreover, that the outcomes of $\vf{X}_n$ are in a set $\mathcal{X}\subseteq\RR^n$, the law $\vf{X}_n$ is one in the set $\mathcal{P}=\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\}$ and $\mathcal{F}$ is a $\sigma$-algebra over $\mathcal{X}$\footnote{That is, $\mathcal{P}$ denotes a family of probability distributions of $\vf{X}_n$ in $(\mathcal{X},\mathcal{F})$, indeXed by $\theta\in\Theta$. Note that we denote that distribution of $\vf{X}_n$ by $\Prob^{\vf{X}_n}$ to distinguish it from the probability distribution $\Prob_{\vf{X}_n}$ in $(\Omega,\mathcal{A},\Prob)$.}. We define a \emph{statistical model} as the triplet $(\mathcal{X},\mathcal{F},\mathcal{P})$\footnote{Often we will take $\mathcal{F}=\mathcal{B}(\mathcal{X})$.}. The set $\mathcal{X}$ is called \emph{sample space}, and the set $\Theta$, \emph{parameter space}. The random vector $\vf{X}_n$ is called \emph{random sample}. If, moreover, $X_1,\ldots,X_n$ are \iid random variables, $\vf{X}_n$ is called a \emph{simple random sample}. The value $(x_1,\ldots,x_n)\in\mathcal{X}$ is called a \emph{realization} of $(X_1,\ldots,X_n)$.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We say $\mathcal{P}=\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ is \emph{identifiable} if the function $$\function{}{\Theta}{\mathcal{P}}{\theta}{\Prob_\theta^{\vf{X}_n}}$$ is injective\footnote{From now on, we will suppose that all the sets $\mathcal{P}$ are always identifiable.}.
  \end{definition}
  \begin{definition}
    A statistical model $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ is said to be \emph{parametric} if $\Theta\subseteq \RR^d$ for some $d\in\NN$\footnote{There are cases where $\Theta$ is not a subset of $\RR^d$. For example, we could have $\Theta=\{f:\RR\rightarrow\RR_{\geq 0} : \int_{-\infty}^{+\infty}f(x)\dd{x}=1\}$.}.
  \end{definition}
  \subsubsection{Statistics and estimators}
  \begin{definition}[Statistic]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We define a \emph{statistic} as a Borel measurable function. That is, $\vf{T}$ can be written as $\vf{T}=\vf{h}(X_1,\ldots,X_n)$, where $\vf{h}:\mathcal{X}\rightarrow\RR^m$ is a Borel measurable function. Hence, $\vf{T}$ is a random vector. The value $m$ is the \emph{dimension} of the statistic.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We define the \emph{sample mean} as the statistic: $$T(X_1,\ldots,X_n)=\frac{1}{n}\sum_{i=1}^nX_i=:\overline{X}_n$$
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We define the \emph{sample variance} as the statistic: $$T(X_1,\ldots,X_n)=\frac{1}{n}\sum_{i=1}^n{(X_i-\overline{X}_n)}^2=:{s_n}^2$$ We define the \emph{corrected sample variance} as the statistic:
    $$T(X_1,\ldots,X_n)=\frac{1}{n-1}\sum_{i=1}^n{(X_i-\overline{X}_n)}^2=:\tilde{s}_n{}^2$$
  \end{definition}
  \begin{proposition}
    Let $X_1,\ldots,X_n$ be random variables. Then: $${s_n}^2=\frac{1}{n}\sum_{i=1}^n{X_i}^2-{\overline{X}_n}^2$$
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a statistical model, ${\theta} \in\Theta$ and $g:\Theta\rightarrow\Theta$ be a function. An \emph{estimator} of $g({\theta})$ is a statistic ${\hat\theta}$ whose outcomes are in $\Theta$ and does not depend on any unknown parameter. It is used to give an estimation of the (supposedly unknown) parameter $g({\theta})$.
  \end{definition}
  \subsubsection{Properties of estimators}
  \begin{definition}[Bias]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be an integrable estimator of $g({\theta})\in\Theta$. We define the \emph{bias} of ${\hat\theta}$ with respect to ${\theta}$ as: $$\bias({\hat\theta}):=\Exp({\hat\theta})-g({\theta})$$ We say that ${\hat\theta}$ is an \emph{unbiased estimator} of $g({\theta})$ if $\bias({\hat\theta})=0$ $\forall\theta\in\Theta$. Otherwise, we say that it is a \emph{biased estimator} of $g({\theta})$.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be an integrable estimator of $g({\theta})\in\Theta$. Suppose that $\bias(\hat\theta)=cg(\theta)$ for some $c\in\RR$. Then, $\frac{\hat\theta}{c+1}$ is an unbiased estimator for $g(\theta)$
  \end{proposition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model such that $X_1,\ldots,X_n$ are square-integrable\footnote{That is, with finite 2nd moments.} \iid random variables with mean $\mu$ and variance $\sigma^2$. Then: $$\Exp(\overline{X}_n)=\mu\quad\text{and}\quad\Var{(\overline{X}_n)}=\frac{\sigma^2}{n}$$
    Hence, the estimator $\overline{X}_n$ of $\mu$ is unbiased.
  \end{proposition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model such that $X_1,\ldots,X_n$ are square-integrable \iid random variables with mean $\mu$ and variance $\sigma^2$. Then: $$\Exp({s_n}^2)=\frac{n-1}{n}\sigma^2\quad\text{and}\quad\Exp(\tilde{s}_n{}^2)=\sigma^2$$
    Hence, the estimator $\tilde{s}_n{}^2$ of $\sigma^2$ is unbiased whereas the estimator ${s_n}^2$ of $\sigma^2$ is biased.
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be an integrable estimator of $g({\theta})\in\Theta$. The \emph{mean squared error} (\emph{MSE}) of ${\hat\theta}$ is the function: $$\MSE({\hat\theta}):=\Exp\left({({\hat\theta}-g({\theta}))}^2\right)$$
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be an integrable estimator of $g({\theta})\in\Theta$. Then: $$\MSE({\hat\theta})=\Var({\hat\theta})+{(\bias({\hat\theta}))}^2$$
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$, ${\tilde\theta}$ be estimators of $g({\theta})\in\Theta$. We say that ${\hat\theta}$ is \emph{more efficient than} ${\tilde\theta}$ if $$\Var{({\hat\theta})}<\Var{({\tilde\theta})}\quad\forall{\theta}\in\Theta$$
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model and ${\hat\theta}$ be a square integrable estimator of ${\theta}\in\Theta$. We say that ${\hat\theta}$ is a \emph{minimum-variance unbiased estimator} (\emph{MVUE}) if it is an unbiased estimator that has lower variance than any other unbiased estimator $\forall {\theta}\in\Theta$.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model. Then, the MVUE is unique almost surely.
  \end{proposition}
  \subsubsection{Sufficient statistics}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $\vf{T}$ be a statistic. We say that $\vf{T}$ is \emph{sufficient} for $\theta\in\Theta$ if the joint conditional distribution of $(X_1,\ldots,X_n)$ given $\vf{T}(X_1,\ldots,X_n)=\vf{t}$ does not depend on $\theta$.
  \end{definition}
  \begin{theorem}[Fisher-Neyman factorization theorem]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $\vf{T}$ be a statistic. Then, $T$ is sufficient if and only if $\forall \vf{x}\in\mathcal{X}$ we have:
    \begin{enumerate}
      \item for the discrete case: $$p_{\vf{X}_n}(\vf{x};\theta)=g(T(\vf{x});\theta)h(\vf{x})$$
      \item for the continuous case:  $$f_{\vf{X}_n}(\vf{x};\theta)=g(T(\vf{x});\theta)h(\vf{x})$$
    \end{enumerate}
    for certain functions $g$ and $h$. Here we have denoted by $p_{\vf{X}_n}(\vf{x};\theta)$ the joint pmf of $\vf{X}_n$ (in the discrete case) and by $f_{\vf{X}_n}(\vf{x};\theta)$ the joint pdf of $\vf{X}_n$ (in the continuous case).
  \end{theorem}
  \subsubsection{Asymptotic properties}
  \begin{definition}
    For each $n\in\NN$, let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a parametric statistical model with $X_1,\ldots,X_n$ being i.i.d., $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be an estimator of $g({\theta})\in\Theta$. We say that the sequence $({{\hat\theta}}_n)$ is a \emph{weakly consistent estimator} of $g(\theta)$ if $${{\hat\theta}}_n\overset{\Prob}{\longrightarrow}g(\theta)$$
  \end{definition}
  \begin{definition}
    For each $n\in\NN$, let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a parametric statistical model with $X_1,\ldots,X_n$ being i.i.d., $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be an estimator of $g({\theta})\in\Theta$. We say that the sequence $({{\hat\theta}}_n)$ is a \emph{strongly consistent estimator} of $g(\theta)$ if $${{\hat\theta}}_n\overset{\text{a.s.}}{\longrightarrow}g(\theta)$$
  \end{definition}
  \begin{definition}
    For each $n\in\NN$, let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a parametric statistical model with $X_1,\ldots,X_n$ being i.i.d., $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be an estimator of $g({\theta})\in\Theta$. We say that the sequence $({{\hat\theta}}_n)$ is a \emph{consistent estimator in $L^2$} of $g(\theta)$ if $$\lim_{n\to\infty}\Exp\left({({\hat\theta}_n-g({\theta}))}^2\right)=\lim_{n\to\infty}\MSE({\hat\theta}_n)=0$$
  \end{definition}
  \begin{proposition}
    For each $n\in\NN$, let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a parametric statistical model with $X_1,\ldots,X_n$ being i.i.d., $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be a consistent estimator in $L^2$ of $g({\theta})\in\Theta$. Then, ${{\hat\theta}}_n$ is a weakly consistent estimator of $g(\theta)$.
  \end{proposition}
  \begin{definition}
    For each $n\in\NN$, let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a parametric statistical model with $X_1,\ldots,X_n$ being i.i.d., $g:\Theta\rightarrow\Theta$ be a function and ${{\hat\theta}}_n$ be an estimator of $g(\theta)\in\Theta$. We say that the sequence $({{\hat\theta}}_n)$ is an \emph{asymptotically unbiased estimator} of $g(\theta)$ if $$\Exp({{\hat\theta}}_n)\longrightarrow g(\theta)$$
  \end{definition}
  \begin{definition}
    For each $n\in\NN$, let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a parametric statistical model with $X_1,\ldots,X_n$ being \iid whose variance is $\sigma^2$, $g:\Theta\rightarrow\Theta$ be a function and ${{\hat\theta}}_n$ be an estimator of $g(\theta)\in\Theta$. We say that the sequence $({\hat\theta}_n)$ is \emph{an asymptotically normal estimator} of $g(\theta)$ with asymptotically variance $\frac{\sigma^2}{n}$ if $$\sqrt{n}({\hat\theta}_n-g(\theta))\overset{\text{d}}{\longrightarrow}N(0,\sigma^2)$$
    for all $\theta\in\Theta$. In that case, we denote it by ${\hat\theta}_n\overset{\text{a}}{\sim}N\left(g(\theta),\frac{\sigma^2}{n}\right)$.
  \end{definition}
  \subsubsection{Methods of estimation}
  \begin{definition}[Method of moments]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model such that $X_1,\ldots,X_n$ are \iid random variables, and $\mu_k$ be $k$-th moment of each of them. Suppose $\vf{\theta}=(\theta_1,\ldots,\theta_d)$. Then, given a realization $\vf{x}_n=(x_1,\ldots,x_n)\in\mathcal{X}$ of $\vf{X}_n$, an estimator $\vf{\tilde{\theta}}(\vf{x}_n)=(\tilde{\theta}_1(\vf{x}_n),\ldots,\tilde{\theta}_d(\vf{x}_n))$ of $\vf{\theta}$ is given by the solution of the following system:
    $$
      \left\{
      \begin{aligned}
        \frac{1}{n}\sum_{i=1}^nx_i     & =\mu_1(\theta_1,\ldots,\theta_d) \\
        \frac{1}{n}\sum_{i=1}^n{x_i}^2 & =\mu_2(\theta_1,\ldots,\theta_d) \\
                                       & \;\;\vdots                       \\
        \frac{1}{n}\sum_{i=1}^n{x_i}^d & =\mu_d(\theta_1,\ldots,\theta_d)
      \end{aligned}
      \right.
    $$
  \end{definition}
  \begin{proposition}
    The estimators obtained by the method of moments are strongly consistent (and in $L^2$).
  \end{proposition}
  \begin{definition}[Likelihood]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:{\theta}\in\Theta\subseteq \RR\})$ be a parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$.
    \begin{enumerate}
      \item For the discrete case, let $p_{\vf{X}_n}(\vf{x};\theta)$ be the pmf of $\Prob_{\theta}^{\vf{X}_n}$. In this case, we define the \emph{likelihood function} as the function:
            $$\function{L(\cdot;\vf{x}_n)}{\Theta}{\RR}{\theta}{p_{\vf{X}_n}(\vf{x}_n;\theta)}$$
      \item For the continuous case, let $f_{\vf{X}_n}(\vf{x};\theta)$ be the pdf of $\Prob_{\theta}^{\vf{X}_n}$. In this case, we define the \emph{likelihood function} as the function:
            $$\function{L(\cdot;\vf{x}_n)}{\Theta}{\RR}{\theta}{f_{\vf{X}_n}(\vf{x}_n;\theta)}$$
    \end{enumerate}
  \end{definition}
  \begin{definition}[Maximum likelihood method]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$. A \emph{maximum likelihood estimator} (\emph{MLE}) of $\theta\in\Theta$ is the estimator $\hat{\theta}$ such that: $$L(\hat{\theta};\vf{x}_n)=\max\{L(\theta;\vf{x}_n):\theta\in\Theta\}\footnote{Note that sometimes this estimator is not unique or may not even exist.}$$
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf\theta\in\Theta\subseteq \RR^d\})$ be a parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$. We define the \emph{log-likelihood function} as: $$\ell(\vf\theta;\vf{x}_n):=\ln L(\vf\theta;\vf{x}_n)$$
    We define the \emph{score function} as: $$\vf{S}(\vf\theta;\vf{x}_n):=\pdv{\ell}{\vf\theta}(\vf\theta,\vf{x}_n)$$
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf{\theta}\in\Theta\subseteq \RR^d\})$ be a parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$ Then, a MLE $\vf{\hat\theta}$ of $\vf\theta$ is the one that satisfies:
    $$\pdv{L}{\vf\theta}(\vf{\hat\theta};\vf{x}_n)=\vf{0}$$
    Or equivalently, $\pdv{\ell}{\vf\theta}(\vf{\hat\theta};\vf{x}_n)=\vf{0}$.
  \end{proposition}
  \begin{proposition}[Invariance of the MLE]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $g:\Theta\rightarrow\Theta$ be a measurable function. Suppose $\hat\theta$ is a MLE of $\theta$. Then, $g(\hat\theta)$ is a MLE of $g(\theta)$.
  \end{proposition}
  \subsubsection{Regular statistical models}
  \begin{definition}
    A statistical model $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\})$ is said to be \emph{regular} if it satisfies the following conditions:
    \begin{enumerate}
      \item $\Theta$ is open.
      \item The support of $\Prob_{\theta}^{\vf{X}_n}$ does not depend on $\theta$.
      \item The function $L(\theta;\vf{x}_n)$ is two times differentiable with respect to $\theta$ $\forall \vf{x}_n\in\mathcal{X}$ (except in a set of probability zero) and moreover:
            \begin{enumerate}
              \item For the discrete case: $$\pdv[2]{}{\theta}\sum_{\vf{x}_n\in\mathcal{X}}L(\theta;\vf{x}_n)=\sum_{\vf{x}_n\in\mathcal{X}}\pdv[2]{L}{\theta}(\theta;\vf{x}_n)$$
                    \item\label{EST_regular} For the continuous case: $$\pdv[2]{}{\theta}\int_{\mathcal{X}}L(\theta;\vf{x}_n)\dd{\vf{x}_n}=\int_{\mathcal{X}}\pdv[2]{L}{\theta}(\theta;\vf{x}_n)\dd{\vf{x}_n}$$
            \end{enumerate}
      \item For all $\theta\in\Theta$, we have: $$0<\int_\mathcal{X}{\left(\pdv[2]{\ell}{\theta}(\theta;\vf{x}_n)\right)}^2 f_{\vf{X}_n}(\vf{x};\vf\theta)\dd{\vf{x}}<\infty$$
    \end{enumerate}
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a regular parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$. We define the \emph{observed information} of the model as:
    $$\vf{J}(\vf\theta;\vf{x}_n)=-\pdv[2]{\ell}{\vf\theta}(\vf\theta;\vf{x}_n)$$
    We define the \emph{Fisher information} of the model as: $$\vf{I}(\vf\theta)=\Exp(\vf{J}(\vf\theta;\vf{X}_n))=-\Exp\left(\pdv[2]{\ell}{\vf\theta}(\vf\theta;\vf{X}_n)\right)\footnote{Since generally $\vf{J}(\vf\theta;\vf{X}_n)$ will be a matrix, the expectation of $\vf{J}(\vf\theta;\vf{X}_n)$ is taken component by component.}$$
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a regular parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$. Then, $\Exp(\vf{S}(\vf\theta;\vf{X}_n))=0$ and $$\vf{I}(\vf\theta)=\Var(\vf{S}(\vf\theta;\vf{X}_n))=\Exp\left[{\left(\pdv{\ell}{\vf\theta}(\vf\theta;\vf{X}_n)\right)}^2\right]$$
    for all $\vf\theta\in\Theta$.
  \end{proposition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{X_1}:\vf\theta\in\Theta\})$ be a regular parametric statistical model of one observation $x_1\in \mathcal{X}$. Then, the model corresponding to $n$ \iid observations $x_1,\ldots,x_n$ is regular and $$\vf{I}(\vf\theta)=n \vf{I}_1(\vf\theta)$$
    where $\vf{I}_1(\vf\theta)$ denotes the Fisher information in the model with one observation.
  \end{proposition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $T$ be a statistic. We say that $T$ is \emph{regular} if
    \begin{enumerate}
      \item for the discrete case: $$\pdv{}{\theta}\sum_{\vf{x}_n\in\mathcal{X}}T(\vf{x}_n)L(\theta;\vf{x}_n)=\sum_{\vf{x}_n\in\mathcal{X}}T(\vf{x}_n)\pdv{L}{\theta}(\theta;\vf{x}_n)$$
      \item for the continuous case: $$\pdv{}{\theta}\int_{\mathcal{X}}T(\vf{x}_n)L(\theta;\vf{x}_n)\dd{\vf{x}_n}=\int_{\mathcal{X}}T(\vf{x}_n)\pdv{L}{\theta}(\theta;\vf{x}_n)\dd{\vf{x}_n}$$
    \end{enumerate}
    for all $\theta\in\Theta$.
  \end{definition}
  \begin{theorem}[CramÃ©r-Rao bound]
    Let $(\mathcal{X}, \mathcal{F}, \{\Prob_{\theta}^{\vf{X}_n}: \theta\in\Theta\subseteq\RR\})$ be a regular parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$, $g:\Theta\rightarrow\Theta$ be a differentiable function and ${\hat\theta}$ be a regular and unbiased estimator of $g({\theta})\in\Theta$. Then: $$\Var(\hat\theta)\geq \frac{{g'(\theta)}^2}{I(\theta)}$$
  \end{theorem}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a regular statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$, $g:\Theta\rightarrow\Theta$ be a differentiable function and ${\hat\theta}$ be a regular and unbiased estimator of $g({\theta})\in\Theta$. We say that $\hat\theta$ is an \emph{efficient estimator} of $g(\theta)$ if $$\Var(\hat\theta)=\frac{{g'(\theta)}^2}{I(\theta)}$$
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a regular statistical model, $g:\Theta\rightarrow\Theta$ be a function and ${\hat\theta}$ be a regular, unbiased and efficient estimator of $g({\theta})\in\Theta$. Then, $\hat\theta$ is a MVUE in the class of regular estimators.
  \end{proposition}
  \begin{theorem}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq \RR\})$ be a regular statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $\vf{X}_n$ and ${\hat\theta}$ be a MLE of ${\theta}\in\Theta$. Suppose that $\pdv[2]{\ell}{\theta}$ is a continuos function of $\theta$ and that $$\left|\pdv[2]{\ell}{\theta}(\tilde{\theta};\vf{x}_n)\right|<h(\vf{x};\theta)$$
    with $\int_\mathcal{X}h(\vf{x};\theta)L(\theta;\vf{x})\dd{\vf{x}}<\infty$, for all $\tilde{\theta}$ in a neighbourhood of $\theta$. Then:
    $$\hat\theta\overset{\text{d}}{\longrightarrow}N\left(\theta,\frac{1}{I(\theta)}\right)$$
    Thus, ${\hat\theta}$ is an asymptotically efficient estimator of $\theta$.
  \end{theorem}
  \begin{theorem}[Delta method]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\theta}^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a statistical model, $g:\Theta\rightarrow\Theta$ be a two-times differentiable function such that $g'(\theta)\ne 0$ and $\hat\theta$ be an estimator of $\theta\in\Theta$. Then:
    $$g(\hat\theta)\overset{\text{d}}{\longrightarrow}N\left(g(\theta),{g'(\theta)}^2\Var(\hat\theta)\right)$$
  \end{theorem}
  \subsubsection{Order statistics}
  \begin{definition}
    Let $X_1,\ldots,X_n$ be random variables. We define the $k$-th \emph{order statistic}, denoted by $X_{(k)}$ of the sample $X_1,\ldots,X_n$ as the $k$-th smallest value of it. In particular:
    $$X_{(1)}:=\min\{X_1,\ldots,X_n\}\quad X_{(n)}:=\max\{X_1,\ldots,X_n\}$$
    The sample $X_{(1)},\ldots,X_{(n)}$ is usually called \emph{order statistics}.
  \end{definition}
  \subsection{Distributions relating \texorpdfstring{$N(\mu,\sigma^2)$}{N(mu,sigma2)}}
  \subsubsection{Standard normal distribution}
  \begin{definition}
    We denote by $\Phi(t)$ the cdf of a standard normal distribution $N(0,1)$.
  \end{definition}
  \begin{definition}[Quantile]
    We define quantile function $Q_X(p)$ of a distribution of a random variable $X$ as the inverse of the cmf. That is, $Q_X(p)$ satisfies:
    $$\Prob(X\leq Q_X(p))=p$$
    In particular, we denote the quantile of a standard normal distribution as $z_p:=Q_X(p)$.
  \end{definition}
  \subsubsection{Multivariate normal distribution}
  \begin{definition}
    Let $\vf{b}\in\RR^n$, $\vf{\Sigma}\in\mathcal{M}_n(\RR)$ be a symmetric positive-definite matrix and $\vf{X}$ be a random vector. We say that $\vf{X}$ has \emph{multivariate normal distribution}, and we denote it by $\vf{X}\sim N(\vf{b},\vf\Sigma)$ if has density function:
    $$f_{\vf{X}}(\vf{x})={(2\pi)}^{-\frac{n}{2}}{(\det\vf\Sigma)}^{-\frac{1}{2}}\exp{-\frac{\transpose{(\vf{x}-\vf{b})}{\vf\Sigma}^{-1}(\vf{x}-\vf{b})}{2}}$$
    The vector $\vf{b}$ is called \emph{mean vector} and the matrix $\vf\Sigma$, \emph{covariance matrix}.
  \end{definition}
  \begin{proposition}
    Let $\vf{\Sigma}\in\mathcal{M}_n(\RR)$ be a symmetric positive-definite matrix. Then, $\exists\vf{A}\in\GL_n(\RR)$ such that $\vf\Sigma=\vf{A}\transpose{\vf{A}}$\footnote{When $\vf\Sigma$ is the covariance matrix, the matrix $\vf{A}$ such that $\vf\Sigma=\vf{A}\transpose{\vf{A}}$ plays the role of the \emph{multivariate standard deviation}.}.
  \end{proposition}
  \begin{proposition}
    Let $\vf{b}\in\RR^n$, $\vf\Sigma=\vf{A}\transpose{\vf{A}}\in\mathcal{M}_n(\RR)$ be a symmetric positive-definite matrix with $\vf{A}\in\GL_n(\RR)$ and $\vf{X}$, $\vf{Z}$ be random vectors.
    \begin{itemize}
      \item If $\vf{Z}\sim N(\vf{0},\vf{I}_n)$, then $\vf{AZ}+\vf{b}\sim N(\vf{b},\vf\Sigma)$.
      \item If $\vf{X}\sim N(\vf{b},\vf{I}_n)$, then ${\vf{A}}^{-1}(\vf{X}-\vf{b})\sim N(\vf{0},\vf\Sigma)$.
    \end{itemize}
  \end{proposition}
  \begin{proposition}
    Let $\vf{b}\in\RR^n$, $\vf{\Sigma}\in\mathcal{M}_n(\RR)$ be a symmetric positive-definite matrix and $\vf{X}=(X_1,\ldots,X_n)\sim N(\vf{b},\vf\Sigma)$. Then, $\Exp(\vf{X})=\vf{b}$ and $\Var(\vf{X})=\vf\Sigma$ and moreover:
    \begin{gather*}
      \vf{b}=\transpose{(\Exp(X_1),\ldots,\Exp(X_2))}\\
      \vf\Sigma=
      \begin{pmatrix}
        \Var(X_1)     & \cov(X_1,X_2) & \cdots & \cov(X_1,X_n) \\
        \cov(X_2,X_1) & \Var(X_2)     & \cdots & \cov(X_2,X_n) \\
        \vdots        & \vdots        & \ddots & \vdots        \\
        \cov(X_n,X_1) & \cov(X_n,X_2) & \cdots & \Var(X_n)     \\
      \end{pmatrix}
    \end{gather*}
  \end{proposition}
  \begin{proposition}
    Let $\vf{b},\vf{c}\in\RR^n$, $\vf{\Sigma}\in\mathcal{M}_n(\RR)$ be a symmetric positive-definite matrix, $\vf{B}\in\GL_n(\RR)$, $\vf{X}\sim N(\vf{b},\vf\Sigma)$ and $\vf{Y}:=\vf{BX}+\vf{c}$. Then: $$\vf{Y}\sim N(\vf{Bb}+\vf{c},\vf{B\Sigma}\transpose{\vf{B}})$$
  \end{proposition}
  \subsubsection{\texorpdfstring{$\chi^2$}{chi2}-distribution}
  \begin{proposition}
    Let $n\in\NN$ and $X_1,\ldots,X_n$ be independent random variables such that $X_i\sim \text{Gamma}(\alpha_i,\beta)$ for $i=1,\ldots,n$. Then: $$\sum_{i=1}^nX_i\sim\text{Gamma}\left(\sum_{i=1}^n\alpha_i,\beta\right)$$
  \end{proposition}
  \begin{corollary}
    Let $n\in\NN$ and $Z_1,\ldots,Z_n$ be \iid random variable with standard normal distribution. Then: $${Z_1}^2+\cdots+{Z_n}^2\sim\text{Gamma}\left(\frac{n}{2},\frac{1}{2}\right)$$
  \end{corollary}
  \begin{definition}
    We define the \emph{chi-squared distribution with $n$ degrees of freedom}, denoted as ${\chi_n}^2$, as the distribution $${\chi_n}^2:=\text{Gamma}\left(\frac{n}{2},\frac{1}{2}\right)$$ which is the distribution of ${Z_1}^2+\cdots+{Z_n}^2$, where $Z_1,\ldots,Z_n\sim N(0,1)$ are \iid random variables. Its pdf is:
    $$f_{{\chi_n}^2}(x)=\frac{1}{2^\frac{n}{2}\Gamma\left(\frac{n}{2}\right)}x^{\frac{n}{2}-1}\exp{-\frac{x}{2}}\vf{1}_{(0,\infty)}(x)$$
    We will denote by ${\chi_{n;p}}^2:=Q_{{\chi_n}^2}(p)$ the quantile of the ${\chi_n}^2$
  \end{definition}
  \begin{proposition}
    Let $X\sim\text{Gamma}(\alpha,\beta)$ and $c\in\RR_{>0}$. Then, $cX\sim\text{Gamma}(\alpha,\beta/c)$. In particular, if $X\sim\text{Gamma}(n,1)$, then $2X\sim {\chi_{2n}}^2$.
  \end{proposition}
  \subsubsection{Student's \texorpdfstring{$t$}{t}-distribution}
  \begin{definition}
    Let $n\in\NN$ and $Z\sim N(0,1)$ and $Y\sim{\chi_n}^2$ be independent random variables. We define the \emph{Student's $t$-distribution with $n$ degrees of freedom} as the distribution of: $$\frac{Z}{\sqrt{Y/n}}$$
    We will denote by $t_{n;p}:=Q_{t_n}(p)$ the quantile of the $t_n$
  \end{definition}
  \begin{proposition}
    Let $n\in\NN$. Then, the pdf of $t_n$ is: $$f_{t_n}(x)=\frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{\pi n}\Gamma\left(\frac{n}{2}\right)}{\left(1+\frac{x^2}{n}\right)}^{-\frac{n+1}{2}}\footnote{It makes sense if we remplace the value $n\in\NN$ for a value $\nu\in\RR_{>0}$. However the original definition of $t_n$ from the ${\chi_n}^2$ fails.}$$
  \end{proposition}
  \subsubsection{Fisher's theorem}
  \begin{theorem}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and suppose $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ are \iid random variables. Then:
    \begin{enumerate}
      \item $\overline{X}_n\sim N\left(\mu,\frac{\sigma^2}{n}\right)$
      \item $\tilde{s}_n{}^2\sim\frac{\sigma^2}{n-1}{\chi_{n-1}}^2$
      \item $\overline{X}_n$ and $\tilde{s}_n{}^2$ are independent.
    \end{enumerate}
  \end{theorem}
  \begin{corollary}
    Let $n\in\NN$ and $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ be \iid random variables. Then: $$\frac{\overline{X}_n-\mu}{\frac{\tilde{s}_n}{\sqrt{n}}}\sim t_{n-1}$$
  \end{corollary}
  \begin{corollary}
    Let $n\in\NN$ and $X\sim t_n$ be a random variable. Then: $$X\overset{\text{d}}{\longrightarrow }N\left(0,1\right)$$
    Hence, $N(0,1)=t_\infty$.
  \end{corollary}
  \begin{corollary}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a parametric statistical model and suppose $X_1,\ldots,X_n\sim N(\mu,\sigma^2)$ are \iid random variables. Then, the estimators $\overline{X}_n$ of $\mu$ and $\tilde{s}_n{}^2$ of $\sigma^2$ are unbiased and consistent.
  \end{corollary}
  \begin{center}
    \begin{minipage}{\linewidth}
      \centering
      \includestandalone[mode=image|tex,width=\linewidth]{Images/student-normal}
      \captionof{figure}{Probability density function of 4 Student's $t$-distribution together with a standard normal $N(0,1)=t_{\infty}$.}
    \end{minipage}
  \end{center}
  \subsection{Confidence intervals}
  \subsubsection{Confidence regions}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a parametric statistical model and $\vf{g}:\Theta\rightarrow\RR^m$, $m\leq d$, be a function. A \emph{confidence region} for $\vf{g}(\vf\theta)$ with \emph{confidence level} $\gamma\in[0,1]$ is a random region $$C(\vf{X}_n)$$ such that $$\Prob(\vf{g}(\vf\theta)\in C(\vf{X}_n))\geq \gamma\quad\forall \theta\in\Theta$$ If $d=1$, we talk about \emph{confidence intervals}. The value $\alpha:=1-\gamma$ is called \emph{significance level}.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a parametric statistical model and $\vf{g}:\Theta\rightarrow\RR^m$, $m\leq d$, be a function. A \emph{pivot} for $\vf{g}(\vf\theta)$ is a measurable function
    $$\function{\pi}{\mathcal{X}\times \vf{g}(\Theta)}{\RR^m}{(\vf{x},\vf{g}(\vf\theta))}{\vf\pi(\vf{x},\vf{g}(\vf\theta))}$$
    such that the distribution of $\vf\pi(\vf{x},\vf{g}(\vf\theta))$ does not depend on $\vf\theta$.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_{\vf\theta}^{\vf{X}_n}:\vf\theta\in\Theta\subseteq\RR^d\})$ be a parametric statistical model, $\gamma\in[0,1]$, $\vf{g}:\Theta\rightarrow\RR^m$, $m\leq d$, be a function, $\vf\pi(\vf{x},\vf{g}(\vf\theta))$ be a pivot for $\vf{g}(\vf\theta)$ and $B\in\mathcal{B}(\RR^m)$ such that: $$\Prob(\vf\pi(\vf{X}_n,\vf{g}(\vf\theta))\in B)\geq \gamma\quad\forall \vf\theta\in\Theta$$
    Then: $$C(\vf{X})=\{\vf{g}(\vf\theta):\vf\pi(\vf{X}_n,\vf{g}(\vf\theta))\in B\}\subseteq \vf{g}(\vf\Theta)$$
    is a confidence region with confidence level $\gamma$.
  \end{proposition}
  \subsubsection{Confidence intervals for the relative frequency}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_n\sim\text{Bern}(p)\ \text{i.i.d.}:p\in(0,1)\})$ be a parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_n)$ and $\alpha\in[0,1]$. Let $\hat{p}=\overline{x}_n$. Then, an asymptotic confidence interval for $p$ of confidence level $1-\alpha$ is:
    $$p\in\left(\hat{p}-z_{1-\frac{\alpha}{2}}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}},\hat{p}+z_{1-\frac{\alpha}{2}}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\right)$$
  \end{proposition}
  \subsubsection{Confidence intervals for \texorpdfstring{$N(\mu,\sigma^2)$}{N(mu,sigma2)}}
  \begin{proposition}[Interval for $\mu$ with $\sigma$ known]
    Let $\sigma\in\RR_{\geq 0}$ be a known parameter, $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_n\sim N(\mu,\sigma^2)\text{ i.i.d.}:\mu\in\RR\})$ be a parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_n)$ and $\alpha\in[0,1]$. Then, a confidence interval for $\mu$ of confidence level $1-\alpha$ is:
    $$\mu\in\left(\overline{x}_n-z_{1-\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}},\overline{x}_n+z_{1-\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}\right)$$
  \end{proposition}
  \begin{proposition}[Intervals for $\mu$ and $\sigma^2$]
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_n\sim N(\mu,\sigma^2)\text{ i.i.d.}:(\mu,\sigma^2)\in\RR\times\RR_{\geq 0}\})$ be a parametric statistical model, $\vf{x}_n\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_n)$ and $\alpha\in[0,1]$. Then, a confidence interval for $\mu$ of confidence level $1-\alpha$ is:
    $$\mu\in\left(\overline{x}_n-t_{n-1;1-\frac{\alpha}{2}}\frac{\tilde{s}_n}{\sqrt{n}},\overline{x}_n+t_{n-1;1-\frac{\alpha}{2}}\frac{\tilde{s}_n}{\sqrt{n}}\right)$$
    A confidence interval for $\sigma^2$ of confidence level $1-\alpha$ is:
    $$\sigma^2\in\left(\frac{(n-1)\tilde{s}_n{}^2}{{\chi_{n;1-\frac{\alpha}{2}}}},\frac{(n-1)\tilde{s}_n{}^2}{{\chi_{n;\frac{\alpha}{2}}}}\right)$$
  \end{proposition}
  \subsubsection{Confidence intervals for two-samples problems}
  \begin{proposition}[Independent samples with known variances]
    Let $\sigma_x,\sigma_y\in\RR_{\geq 0}$ be known parameters, $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_{n_x}\sim N(\mu_x,{\sigma_x}^2)\text{ i.i.d.},Y_1,\ldots,Y_{n_y}\sim N(\mu_y,{\sigma_y}^2)\text{ i.i.d.}:(\mu_x,\mu_y,)\in\RR^2\})$ be a parametric statistical model such that each $X_i$ is independent of $Y_j$ $\forall (i,j)\in\{1,\ldots,n_x\}\times\{1,\ldots,n_y\}$, $\vf{x}_{n_x}\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_{n_x})$, $\vf{y}_{n_y}\in\mathcal{X}$ be a realization of $(Y_1,\ldots,Y_{n_y})$ and $\alpha\in[0,1]$. Then, an asymptotic confidence interval for $\mu_x-\mu_y$ of confidence level $1-\alpha$ is:
    $$\mu_x-\mu_y\in\left(\overline{x}_{n_x}-\overline{y}_{n_y}-z_{1-\frac{\alpha}{2}}s,\overline{x}_{n_x}-\overline{y}_{n_y}+z_{1-\frac{\alpha}{2}}s\right)$$ where $s=\sqrt{\frac{{\sigma_x}^2}{n_x}+\frac{{\sigma_y}^2}{n_y}}$.
  \end{proposition}
  \begin{proposition}[Independent samples with unknown equal variances]
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_{n_x}\sim N(\mu_x,\sigma^2)\text{ i.i.d.},Y_1,\ldots,Y_{n_y}\sim N(\mu_y,\sigma^2)\text{ i.i.d.}:(\mu_x,\mu_y,)\in\RR^2\times{\RR_{\geq 0}}^2\})$ be a parametric statistical model such that each $X_i$ is independent of $Y_j$ $\forall (i,j)\in\{1,\ldots,n_x\}\times\{1,\ldots,n_y\}$, $(x_1,\ldots,x_n)\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_{n})$, $(y_1,\ldots,y_n)\in\mathcal{X}$ be a realization of $(Y_1,\ldots,Y_{n})$ and $\alpha\in[0,1]$. Let $\tilde{s}_{n_x}{}^2:=\frac{1}{n_x-1}\sum_{i=1}^n{(x_i-\overline{x})}^2$ and $\tilde{s}_{n_y}{}^2:=\frac{1}{n_y-1}\sum_{i=1}^n{(y_i-\overline{y})}^2$. Then, an asymptotic confidence interval for $\mu_x-\mu_y$ of confidence level $1-\alpha$ is:
    \begin{multline*}
      \mu_x-\mu_y\in\left(\overline{x}_{n_x}-\overline{y}_{n_y}-t_{\nu;1-\frac{\alpha}{2}}s_p\sqrt{\frac{1}{n_x}+\frac{1}{n_y}},\right.\\\left.\overline{x}_{n_x}-\overline{y}_{n_y}+t_{\nu;1-\frac{\alpha}{2}}s_p\sqrt{\frac{1}{n_x}+\frac{1}{n_y}}\right)
    \end{multline*}
    where ${s_p}^2=\frac{(n_x-1)\tilde{s}_{n_x}{}^2+(n_y-1)\tilde{s}_{n_y}{}^2}{n_x+n_y-2}$ and $\nu=n_x+n_y-2$.
  \end{proposition}
  \begin{proposition}[Independent samples with unknown variances]
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_{n_x}\sim N(\mu_x,{\sigma_x}^2)\text{ i.i.d.},Y_1,\ldots,Y_{n_y}\sim N(\mu_y,{\sigma_y}^2)\text{ i.i.d.}:(\mu_x,\mu_y,)\in\RR^2\times{\RR_{\geq 0}}^2\})$ be a parametric statistical model such that each $X_i$ is independent of $Y_j$ $\forall (i,j)\in\{1,\ldots,n_x\}\times\{1,\ldots,n_y\}$, $\vf{x}_{n_x}\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_{n_x})$, $\vf{y}_{n_y}\in\mathcal{X}$ be a realization of $(Y_1,\ldots,Y_{n_y})$ and $\alpha\in[0,1]$. Let $\tilde{s}_{n_x}{}^2:=\frac{1}{n_x-1}\sum_{i=1}^n{(x_i-\overline{x})}^2$ and $\tilde{s}_{n_y}{}^2:=\frac{1}{n_y-1}\sum_{i=1}^n{(y_i-\overline{y})}^2$. Then, an asymptotic confidence interval for $\mu_x-\mu_y$ of confidence level $1-\alpha$ is:
    \begin{multline*}
      \mu_x-\mu_y\in\left(\overline{x}_{n_x}-\overline{y}_{n_y}-t_{\nu;1-\frac{\alpha}{2}}\sqrt{\frac{\tilde{s}_{n_x}{}^2}{n_x}+\frac{\tilde{s}_{n_y}{}^2}{n_y}},\right.\\\left.\overline{x}_{n_x}-\overline{y}_{n_y}+t_{\nu;1-\frac{\alpha}{2}}\sqrt{\frac{\tilde{s}_{n_x}{}^2}{n_x}+\frac{\tilde{s}_{n_y}{}^2}{n_y}}\right)
    \end{multline*}
    where $$\nu=\frac{\displaystyle{\left(\frac{\tilde{s}_{n_x}{}^2}{n_x}+\frac{\tilde{s}_{n_y}{}^2}{n_y}\right)}^2}{\displaystyle\frac{{\left(\frac{\tilde{s}_{n_x}{}^2}{n_x}\right)}^2}{n_x-1}+\frac{{\left(\frac{\tilde{s}_{n_y}{}^2}{n_y}\right)}^2}{n_y-1}}$$
  \end{proposition}
  \begin{proposition}[Related samples with unknown variances]
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_{n}\sim N(\mu_x,{\sigma_x}^2)\text{ i.i.d.},Y_1,\ldots,Y_{n}\sim N(\mu_y,{\sigma_y}^2)\text{ i.i.d.}:(\mu_x,{\sigma_x}^2,\mu_y,{\sigma_y}^2)\in\RR^2\times{\RR_{\geq 0}}^2\})$ be a parametric statistical model such that each $W_i:=X_i-Y_i\sim N(\mu_x-\mu_y,{\sigma_x}^2-{\sigma_y}^2)$ are i.i.d., $(x_1,\ldots,x_n)\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_{n})$, $(y_1,\ldots,y_n)\in\mathcal{X}$ be a realization of $(Y_1,\ldots,Y_{n})$ and $\alpha\in[0,1]$. Then, we can proceed as if we only had the sample $(W_1,\ldots,W_n)$. In particular, a confidence interval for $\mu_x-\mu_y$ of confidence level $1-\alpha$ is:
    \begin{multline*}
      \mu_x-\mu_y\in\left(\overline{x}_{n}-\overline{y}_{n}-t_{n-1;1-\frac{\alpha}{2}}\frac{\hat{s}_n}{\sqrt{n}},\right.\\\left.\overline{x}_{n}-\overline{y}_{n}+t_{n-1;1-\frac{\alpha}{2}}\frac{\hat{s}_n}{\sqrt{n}}\right)
    \end{multline*}
    where ${\hat{s}_n}^2=\frac{1}{n-1}\sum_{i=1}^n{(x_i-y_i-(\overline{x}-\overline{y}))}^2$.
  \end{proposition}
  \subsection{Hypothesis testing}
  \subsubsection{Hypothesis test}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $\Theta_0,\Theta_1\subset\Theta$ be disjoint subsets. Our goal is to know whether $\theta\in\Theta_0$ or $\theta\in\Theta_1$ (even if it isn't neither of them) and we will use a sample $\vf{x}\in\mathcal{X}$ to conclude our objective. We define the following two propositions which we will call \emph{hypothesis}:
    $$\mathcal{H}_0:\theta\in\Theta_0\qquad\mathcal{H}_1:\theta\in\Theta_1$$
    $\mathcal{H}_0$ is called \emph{null hypothesis} and $\mathcal{H}_1$ is called \emph{alternative hypothesis}. We say that the hypothesis $\mathcal{H}_i$ is \emph{simple} if $\Theta_i=\{\theta_0\}$ for some $\theta_0\in\Theta$. Otherwise we say that the hypothesis $\mathcal{H}_i$ is compound.
  \end{definition}
  \begin{definition}[Hypothesis test]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. A \emph{hypothesis test} is a function $$\function{\delta}{\mathcal{X}}{\{\mathcal{H}_0,\mathcal{H}_1\}}{\vf{x}}{\delta(\vf{x})}$$
    The set $A_0:=\delta^{-1}(\mathcal{H}_0)\subseteq \mathcal{X}$, which is the set of samples that will lead us to accept $\mathcal{H}_0$, is called \emph{acceptation region}. The set $A_1:=\delta^{-1}(\mathcal{H}_1)\subseteq \mathcal{X}$, which is the set of samples that will lead us to accept $\mathcal{H}_1$ (and therefore reject $\mathcal{H}_0$), is called \emph{critical region}\footnote{In order to denote these concepts more compactly, we will write $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ to denote the hypothesis test whose acceptation and critical regions are $A_0$ and $A_1$, respectively.}.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model and $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test. An \emph{error of type I} is the rejection of $\mathcal{H}_0$ when it is true. An \emph{error of type II} is the acceptation of $\mathcal{H}_0$ when it is false. We define the probabilities $\alpha$ and $\beta$ as:
    \begin{align*}
      \alpha & :=\Prob(\text{Error of type I})=\Prob(\text{Reject $\mathcal{H}_0$}\mid\text{$\mathcal{H}_0$ is true})   \\
      \beta  & :=\Prob(\text{Error of type II})=\Prob(\text{Accept $\mathcal{H}_0$}\mid\text{$\mathcal{H}_0$ is false})
    \end{align*}
    More precisely, if $\vf{x}\in\mathcal{X}$ is a realization of $\vf{X}_n$, then: $$\alpha:=\sup\{\Prob(\vf{x}\in A_1\mid \theta):\theta\in\Theta_0\}$$
    The value $1-\beta$ is called \emph{power} of the test and the value $\alpha$, \emph{size} of the test. Moreover we say that the test has \emph{significance level} $\alpha\in[0,1]$ if its size is less than or equal to $\alpha$. In many cases, the size of the test and the significance level are equal\footnote{In particular, for simple hypostesis they are the same thing.}\footnote{In practise, we fix a significance level $\alpha\in[0,1]$ small enough ($\approx 0.05$ but may vary depending on the problem) with which we accept making mistakes and from here we try to minimize the $\beta$ (or maximize the power $1-\beta$). Moreover having fixed $\alpha$, we obtain a confidence level $1-\alpha$. And if we impose $\Prob(\vf{x}\in A_0\mid \mathcal{H}_0)=1-\alpha$, we are able to determine $A_0$.}.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model, $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test and $\vf{x}\in\mathcal{X}$ be a realization of $\vf{X}_n$. We define the \emph{power function} as: $$\Pi(\theta)=\Prob(\text{Reject $\mathcal{H}_0$})=\Prob(\vf{x}\in A_1)$$
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model, $\vf{x}\in\mathcal{X}$ be a realization of $\vf{X}_n$ and $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test. Then:
    $$
      \Pi(\theta)=
      \begin{cases}
        \alpha  & \text{if }\theta\in\Theta_0 \\
        1-\beta & \text{if }\theta\in\Theta_1
      \end{cases}
    $$
  \end{proposition}
  \subsubsection{Test statistic and \texorpdfstring{$p$}{p}-value}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a statistical model, $\vf{x}\in\mathcal{X}$ be a realization of $\vf{X}_n$ and $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test. A statistic $T$ used to decide whether or not reject the null hypothesis is called a \emph{test statistic}.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a statistical model, $\vf{x}\in\mathcal{X}$ be a realization of $\vf{X}_n$, $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test such that $\Theta_0=\{\theta_0\}$, and $T$ be a test statistic. Suppose that we have observed the value $t:=T(\vf{x})$. We define the \emph{$p$-value} as the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.
  \end{definition}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a statistical model, $\vf{x}\in\mathcal{X}$ be a realization of $\vf{X}_n$, $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test such that $\Theta_0=\{\theta_0\}$. We say that the test is a
    \begin{itemize}
      \item \emph{one-sided right tail test} if $\Theta_1=(\theta_0,\infty)$.
      \item \emph{one-sided left tail test} if $\Theta_1=(-\infty,theta_0)$.
      \item \emph{two-sided test} if $\Theta_1=\RR\setminus\{\theta_0\}$.
    \end{itemize}
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\subseteq\RR\})$ be a statistical model, $\vf{x}\in\mathcal{X}$ be a realization of $\vf{X}_n$, $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test such that $\Theta_0=\{\theta_0\}$, and $T$ be a test statistic. Suppose that we have observed the value $t:=T(\vf{x})$ and let $p$ be the $p$-value of the test. Then:
    \begin{enumerate}
      \item One-sided right tail test: $$p=\Prob(T\geq t\mid \mathcal{H}_0)$$
      \item One-sided left tail test: $$p=\Prob(T\leq t\mid \mathcal{H}_0)$$
      \item Two-sided test: $$p=2\min\{\Prob(T\geq t\mid \mathcal{H}_0),\Prob(T\leq t\mid \mathcal{H}_0)\}\footnote{If the statistic $T$ is symmetric with respect to the origin, then $p=\Prob(|T|\geq |t|\mid \mathcal{H}_0)$.}$$
    \end{enumerate}
    And given a significance level $\alpha\in(0,1)$ we will reject $\mathcal{H}_0$ if $p<\alpha$ and accept $\mathcal{H}_0$ if $p\geq\alpha$.
  \end{proposition}
  \begin{center}
    \begin{minipage}{\linewidth}
      \centering
      \includestandalone[mode=image|tex,width=\linewidth]{Images/p-value}
      \captionof{figure}{Probability density function of a test statistic (assuming the null hypothesis) together with a observed value and the $p$-value of the one-sided right tail test.}
    \end{minipage}
  \end{center}
  \subsubsection{Neymann-Pearson test}
  \begin{definition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We say that a test $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ of significance level $\alpha$ is a \emph{uniformly most powerful} (\emph{UMP}) \emph{test} if it has the greatest power among all the tests with significance level $\alpha$.
  \end{definition}
  \begin{lemma}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model. We say that a test $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ of significance level $\alpha$. If $A_1$ does not depend on the parameter $\theta\in\Theta_1$, then $\delta$ is a UMP test.
  \end{lemma}
  \begin{definition}[Neymann-Pearson test]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model, $\vf{x}\in\mathcal{X}$ be a realization of $\vf{X}_n$ and $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test such that $\Theta_0=\{\theta_0\}$ and $\Theta_1=\{\theta_1\}$. We say that $\delta$ is a \emph{Neyman-Pearson test} of significance level $\alpha\in[0,1]$ if $\exists C>0$ such that:
    \begin{gather*}
      \left\{\vf{x}\in\mathcal{X}:\frac{L(\theta_0;\vf{x})}{L(\theta_1;\vf{x})}> C\right\}= A_0\\
      \left\{\vf{x}\in\mathcal{X}:\frac{L(\theta_0;\vf{x})}{L(\theta_1;\vf{x})}\leq C\right\}= A_1
    \end{gather*}
    and $\Prob(\vf{x}\in A_1\mid \mathcal{H}_0)=\alpha$.
  \end{definition}
  \begin{lemma}[Neyman-Pearson lemma]
    Any Neyman-Pearson test is a UMP test.
  \end{lemma}
  \begin{theorem}
    Any UMP test is a Neyman-Pearson test.
  \end{theorem}
  \subsubsection{Likelihood-ratio test}
  \begin{definition}[Likelihood-ratio test]
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model, $\vf{x}\in\mathcal{X}$ be a realization of $\vf{X}_n$, $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test of compound hypothesis $\mathcal{H}_0$ and $\mathcal{H}_1$.
    Then, the \emph{likelihood ratio test} (\emph{LRT}) is given by the critical region:
    $$\left\{\vf{x}\in\mathcal{X}:\frac{\sup\{L(\theta;\vf{x}):\theta\in\Theta_0\}}{\sup\{L(\theta;\vf{x}):\theta\in\Theta\}}\leq C\right\}= A_1$$
    for some constant $C>0$\footnote{Note that $L(\hat\theta,\vf{x})=\sup\{L(\theta;\vf{x}):\theta\in\Theta\}$, where $\hat\theta$ is the MLE estimator. Similarly $L(\hat\theta_0,\vf{x})=\sup\{L(\theta;\vf{x}):\theta\in\Theta\}$, where $\hat\theta_0$ is the MLE estimator restricted to $\Theta_0$.}.
  \end{definition}
  \begin{proposition}
    Let $(\mathcal{X},\mathcal{F},\{\Prob_\theta^{\vf{X}_n}:\theta\in\Theta\})$ be a statistical model, $\vf{x}\in\mathcal{X}$ be a realization of $\vf{X}_n$, $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$ be a hypothesis test of simple hypothesis $\mathcal{H}_0$ and $\mathcal{H}_1$. Then, the LRT is a Neyman-Pearson test.
  \end{proposition}
  \subsubsection{\texorpdfstring{$t$}{t}-test}
  \begin{definition}[$t$-test]
    Let $(\mathcal{X},\mathcal{F},\{X_1,\ldots,X_n\sim N(\mu,\sigma^2)\ \text{\iid} : \mu\in\RR\})$ be a statistical model, $\vf{x}\in\mathcal{X}$ be a realization of $(X_1,\ldots,X_n)$. The \emph{$t$-test} is the test $\delta:\mathcal{X}=A_1\sqcup A_2\rightarrow\{\mathcal{H}_0,\mathcal{H}_1\}$, where $\mathcal{H}_0:\mu=\mu_0$ for some $\mu_0\in\RR$ and $\mathcal{H}_1$ can be either of $\{\mu>\mu_0,\mu<\mu_0,\mu\ne\mu_0\}$. In this case the test statistic that is taken is: $$\frac{\overline{X}_n-\mu}{\frac{\tilde{s}_n}{\sqrt{n}}}\sim t_{n-1}$$
  \end{definition}
  \subsection{Analyzing data}
  \subsubsection{Comparizing distributions}
  \begin{definition}[Q-Q plots]
    Consider a sample $x_1,\ldots,x_n$ of data and the ordered sample $x_{(1)},\ldots,x_{(n)}$. We would like to know whether the data come from a distribution $F$ or not. To do conclude something, we define $$y_{(k)}=y_k:=F^{-1}\left(\frac{k}{n+1}\right)\quad k=1,\ldots,n$$
    Then, we plot the pairs $(y_{(k)},x_{(k)})$ (or equivalently the pairs $(F(y_{(k)}),F(x_{(k)}))$). The more similar is the plot to a line, the more possible is for the data to come from the distribution $F$. This plot is called \emph{Quantile-Quantile plot} (or \emph{Q-Q plots}), $y_{(k)}$ are called the \emph{theoretical quantiles} and $x_{(k)}$ the \emph{sample quantiles}\footnote{Sometimes the theoretical quantiles taken are $F^{-1}\left(\frac{k-0.5}{n}\right)$ instead of $F^{-1}\left(\frac{k}{n+1}\right)$.}.
  \end{definition}
  \begin{proposition}[Normal Q-Q plots]
    Consider a sample $x_1,\ldots,x_n$ of data and the ordered sample $x_{(1)},\ldots,x_{(n)}$. We would like to know whether the data come from a normal distribution $N(\mu,\sigma^2)$ or not. We define $$z_{(k)}=z_{\frac{k}{n+1}}=\Phi^{-1}\left(\frac{k}{n+1}\right)\quad k=1,\ldots,n$$
    If the data are reasonably normal, the Q-Q plot of the pairs $(z_{(k)},x_{(k)})$ is approximately a line of slope $\sigma$ and $y$-intercept $\mu$.
  \end{proposition}
  \begin{center}
    \begin{minipage}{\linewidth}
      \centering
      \includestandalone[mode=image|tex,width=\linewidth]{Images/qq-plot}
      \captionof{figure}{Two normal Q-Q plots of samples of size 100. On the right-hand side the sample comes from a exponential distribution and so we can see that the data doesn't fit quite well in a line. On the left-hand side the data come from a normal distribution and so the data is approximately fitted in a line whose slope is $2.369\approx \sqrt{4}$ and whose $y$-intersect is $2.956\approx 3$.}
    \end{minipage}
  \end{center}
\end{multicols}
\end{document}